# rRNA removal

####Goal of this analysis

To remove ribosomal reads from the metatranscriptomes. We used an rRNA removal kit before sequencing, but 50-70% of the reads in the sample are still rRNA. We're not interested in these biased reads with little information about gene expression. Luckily the samples are big enough that 30-50% of reads is still plenty to look at non-rRNA expression.

##### Approach

Starting with the quality filtered fastq files provided by the JGI through the Genome Portal, I will use Sortmerna to separate rRNA and non-rRNA reads into two separate files. I'll save both, but use the non-rRNA reads for downstream analyses.

## Most recent workflow. 
####Use this if you want to replicate our protocol. Updated 2017-02-07

First, a word on where all of our files are located:

- zissou.bact.wisc.edu, our lab server, is where data is kept long term. However, it does not have the processing power to do the necessary computations.
- Instead, we run things through UW-Madison's Center for High Throughput Computing, specifically on submit-3.chtc.wisc.edu. I submit jobs and store short-term, small files on this submit node in /home/amlinz/
- BUT the actual data files are still way too large to be stored on the submit node. Instead, I have a gluster account where I keep the data files - from submit-3.chtc.wisc.edu, it is located at /mnt/gluster/amlinz. This means that instead of transferring data files with my executable in the submit node, I instead wrote a line in the executable that says "go to gluster and download this file." Therefore, I can only run my programs on computers with access to gluster. Also, please note that gluster is not for long-term storage - that is what Zissou is for!

All the code below is run from my submit node, and frequently references my gluster account. Keep this in mind if you are trying to replicate the workflow!

1. The first thing we need to do is split the giant fastq files into many smaller pieces. I'm starting with the reads that went through quality control at the JGI (extension .filter-MTF.fastq), located in /mnt/gluster/amlinz/GEODES_metaT.

```{r, eval = F}
# Make a new directory for the split files
mkdir /mnt/gluster/amlinz/GEODES_metaT_split/
# Alternatively, if you've already done this before, just make sure this directory is empty
# rm -r /mnt/gluster/amlinz/GEODES_metaT_split/*

# Make a list of samples to run
for file in /mnt/gluster/amlinz/GEODES_metaT/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > samplenames.txt
# Don't want to run all of your samples just yet? The line below will keep just the first 3 files in the list.
# head -3 samplenames.txt > test_samplenames.txt; mv test_samplenames.txt samplenames.txt
# Double check!
# cat samplenames.txt

# You should have both 00split_fastq.sub and 00split_fastq.sh in your home folder. The .sub file references the .sh file.
# I've set my executable to split files into 1,000,000 reads each, but you can change that.

# Check that your submit and executable files are referencing the right places, then submit your jobs:
condor_submit 00split_fastq.sub

# Check status with this command:
condor_q
# For a more detailed report of computers available while your job is idle:
# condor_q -analyze
# Something stuck or not going well? Remove jobs with:
# condor_rm -all, or condor_rm -8222164 (job id)

# All done! Time with 3 files = 23 minutes. Time with 85 files = 3 hours, but 2 got stuck.
# Check to see if there's anything in the error file
ls -ltr 00*.err

# Check to make sure the output is what you wanted:
ls -ltr /mnt/gluster/amlinz/GEODES_metaT_split/
find /mnt/gluster/amlinz/GEODES_metaT_split/ -type f
  
```

2. Sort that RNA! I'm using sortmerna-2.1-linux-64.tar.gz from http://bioinfo.lifl.fr/RNA/sortmerna/ . Make sure to have the program tarball in your home folder. I'm using all of the provided databases as my alignment references.

```{r, eval = F}
# Make some directories to store the output, or alternatively, empty these directories as above:
mkdir /mnt/gluster/amlinz/GEODES_nonrRNA/
mkdir /mnt/gluster/amlinz/GEODES_rRNA/

# Make a file that contains the paths to all of the file parts:
find /mnt/gluster/amlinz/GEODES_metaT_split/ -type f > path2splitfastqs.txt
# Double check!
cat path2splitfastqs.txt

# Submit the rRNA sorting jobs
condor_submit 01rRNA_removal.sub

# I've submit 177 jobs from 3 original fastq files. Time = 1 hour for most
# 3 files got stuck somewhere. After contacting CHTC, here's what they told me to do if something gets stuck running way longer than it should:

# Check to see where the jobs are:
condor_q -run -nobatch
# My stuck files were all on the same server.
# Move stuck jobs to hold and then restart:
condor_hold amlinz
condor_release amlinz

# Check the output when all jobs have finished:
ls -ltr /mnt/gluster/amlinz/GEODES_nonrRNA/
ls -ltr /mnt/gluster/amlinz/GEODES_rRNA/
  
# Are the error files empty?
ls -ltr 01*.err
```

3. Put everything back together. This is a pretty simple program - all it does it copy files generated from the same sample, concatenate them into a single file, count the number of lines, zip it up, and send it back to gluster.

```{r, eval = F}
# We'll be using samplenames.txt again as our reference file.

# Submit the jobs:
condor_submit 02cat_files.sub

# You'll get a file back in your home folder that has the number of lines in both the rRNA and nonrRNA files. Concatenate these into a single file:
cat *_rRNA_results.txt > GEODES_rRNA_ratios.txt
cat GEODES_rRNA_ratios.txt

# Check the output!
ls -ltr /mnt/gluster/amlinz/GEODES_nonrRNA_concat/
ls -ltr /mnt/gluster/amlinz/GEODES_rRNA_concat/
  
# Are the error files empty?
ls -ltr 02*.err
```

4. Clean up after yourself. Gluster is not meant for long term storage of files!

From Zissou:
```{r, eval = F}
# Download files to scratch space. These will then be copied to /lakes_data/ 
cd /scratch/amlinz/GEODES
mkdir GEODES_nonrRNA
mkdir GEODES_rRNA

rsync -av GEODES_nonrRNA amlinz@submit-3.chtc.wisc.edu:/mnt/gluster/amlinz/GEODES_nonrRNA_concat/
rsync -av GEODES_rRNA amlinz@submit-3.chtc.wisc.edu:/mnt/gluster/amlinz/GEODES_rRNA_concat/
```

On my computer:
Open up WinSCP and log into submit-3.chtc.wisc.edu. Download the GEODES_rRNA_ratios.txt file to my github repo, geodes/analyses/01rRNA_removal/. Download the most recent versions of the scripts used here while you're at it. Run the R script plot_rRNA_ratios.Rmd.

On submit-3.chtc.wisc.edu:

```{r, eval = F}
# The only files I need going forward are the nonrRNA files. Delete the rest!
# Note: I'll keep the directories, in case I need to run a few more files in the future:

rm /mnt/gluster/amlinz/GEODES_metaT/*
rm -r /mnt/gluster/amlinz/GEODES_metaT_split/*
rm /mnt/gluster/amlinz/GEODES_nonrRNA/*
rm /mnt/gluster/amlinz/GEODES_rRNA/*
rm /mnt/gluster/amlinz/GEODES_rRNA_concat/*
  
# Delete all the .log, .out, and .err files in your home directory
rm *.err
rm *.log
rm *.out

# Remove the sortmerna program and the rRNA ratios report
rm *.txt
rm sortmerna-2.1-linux-64.tar.gz

# Move old scripts into their own folder
mkdir scripts #if necessary
mv *.sh scripts/
mv *.sub scripts/

```

Congratulations! You now have files of just nonrRNA reads from your metatranscriptomes, and are ready to run the next step.

# Lab Notebook

#### 2017-02-02

I've downloaded the latest precompiled Linux binaries from here: http://bioinfo.lifl.fr/RNA/sortmerna/

Now I want to know where the databases are kept and which ones come preloaded with the install. On my home folder in submit-3.chtc.wisc.edu, I entered:

```{r, eval = F}
tar -xvf sortmerna-2.1-linux-64.tar.gz
```

They are located in sortmerna/rRNA_databases/ , and looks everything I need.

The steps my executable will need to perform are:

1. Get a fastq file from gluster (the place to hold giant files on our high throughput computing system)
2. Unzip the fastq file
3. Unzip the sortmerna program file
4. Index the databases
5. Run sortmerna
6. Transfer output files back to gluster
7. Remove all files transferred from gluster

Here's my first stab at the executable. Just running on one fastq file for now.
```{r, eval = F}
#!/bin/bash
#Sort metatranscriptomic reads into rRNA and non-rRNA files
#Starting with one fastq file for now

#Transfer the fasta file from gluster
cp /mnt/gluster/amlinz/GEODES_metaT/GEODES001.filter-MTF.fastq.gz ./

#Unzip files
gzip -d GEODES001.filter-MTF.fastq.gz
tar -xvf sortmerna-2.1-linux-64.tar.gz

cd sortmerna-2.1-linux-64

#Index the rRNA databases
./indexdb_rna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac$
./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:\
./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:\
./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:\
./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:\
./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:\
./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:\
./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db

#Run the sorting program
./sortmerna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-1$
./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:\
./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:\
./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:\
./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:\
./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:\
./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:\
./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db\
--reads ./GEODES001.filter-MTF.fastq  --fastx --aligned GEODES001_rRNA\
--other GEODES002_nonrRNA --log -v -m 3

#Let's unpack the parameters I've chosen. --ref refers to the databases I've just indexed. --reads says use the fastq file provided. --fastx means I would like a fastq file as output. --aligned is the name for rRNA reads, --other is the name for non-rRNA reads. --log says compute statistics about the run. -v means verbose. I've left the alignment setting at the default, --best. There's a faster setting, but I'm hoping I won't need it. And finally, -m tells sortmerna it can use 3GB of RAM to hold each piece of the fastq file as it processes. Hopefully that will give it a fighting chance

#Move the output files back to gluster
mv GEODES001_rRNA.fastq /mnt/gluster/amlinz/GEODES_rRNA
mv GEODES001_nonrRNA.fastq /mnt/gluster/amlinz/GEODES_nonrRNA

#Remove files
cd ..
rm GEODES001.filter-MTF.fastq
rm GEODES001.filter-MTF.fastq.gz
rm sortmerna-2.1-linux-64.tar.gz
rm -f sortmerna-2.1-linux-64

```

Saved as 01rRNA_removal.sh
Here's its associated submit file, 01rRNA_removal.sub:

```{r, eval = F}
# 01rRNA_removal.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 01rRNA_removal_$(Cluster).log
error = 01rRNA_removal_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 01rRNA_removal.sh
#arguments = $(fastqfile)
output = 01rRNA_removal_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
#when_to_transfer_output = ON_EXIT
transfer_input_files = sortmerna-2.1-linux-64.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 4GB
request_disk = 10GB
#
# Tell HTCondor to run every fasta file in the provided list:
#queue fastafile from metagenome_list.txt
queue 1

```
I've commented out the options I'll need later (arguments, queue)

Now the moment of truth. By the way, I'm running things on HTCondor using

```{r, eval = F}
condor_submit 01rRNA_removal.sub
#check status
condor_q
```

Error thrown: I got the help page for sortmerna and the output files weren't generated, which the flags for that command are wrong. I also got that I can't remove a directory.

I couldn't find anything wrong with sortmerna, so I move the commands for ./indexdb and ./sortmerna onto one line, instead of / at the end of each line (like what I copied off the manual). Maybe that doesn't work in bash scripts. I also changed the removal of the directory to "rm -rf".

Error again: No help file from sortmerna, but no results either. I realized I was missing a dot in calling the reads file - it should be "--reads ../GEODESetc" instead lf "--reads ./GEODESetc". I also realized that gzip -d removes the .gz file, which is why I got the error that it didn't exist when I tried to remove it.

Next run produced nothing - no errors, no writing to the screen, no output files. I think I need to run this on Zissou to do more testing. I need to get all my files up there anyway. Will come back to this later!

#### 2017-02-03

Well, the Zissou test worked great and ran for awhile (2 hours) before I killed it. It was 1/5 done. But everything seems to be in order in the bash script. I suspect it might have died because it was too greedy for RAM - on Zissou it used nearly 40GB, despite the "-m 3" setting, while I'd only requested 4. I'll also need double the hard drive space - 10GB is enough for the input file, but not the input file + output. I'll change my request settings and try again. I'm also going to add 10 threads in sortmerna using "-a 10"
Request: 10 threads, 25GB RAM, 20GB drive space. No idea how many computers in the network meet that requirement.

#### 2017-02-06

Sucess! (ish) The first run I started last Friday was stuck in the queue for an hour, so I restarted with 5 threads, 10GB RAM, and 18GB drive space. That started right away and finished in about 11 hours. No errors were produced, and the right output ended up on Gluster.

Fun fact: check how many computers can run your job with:

```{r, eval = F}
condor_q -analyze
```

HOWEVER. I'm going to need to run this on 108 metatranscriptomes, and there are not 108 servers that meet my requirements, and 11 hours is a long time. SortmeRNA broke up the files into nearly 1900 pieces, then ran them 5 at a time. In order to increase the number of computers meeting my requirements (and therefore increase speed), I need to split my fastq files into many smaller files. I'll do this on CHTC as well, and use Grace's script from https://github.com/dshrade1/Metagenomic-Time-Series-CHTC/blob/master/TE/preprocess/preprocess_fasta.sh
as a model. Just doing one file to test for right now.

```{r, eval = F}
#!/bin/bash

cp /mnt/gluster/amlinz/GEODES_metaT/GEODES001.filter-MTF.fastq.gz ./
gzip -d GEODES001.filter-MTF.fastq.gz

# make a folder for each metagenome
code=`echo "$1" | cut -d'.' -f1`
mkdir ${code}

# Split by number of lines
# Each takes up four lines - @HISEQ header, forward read, "+", reverse read. So make sure my line cutoff is a multiple of 4
split -l 1000000 $1 ${code}/${code}

# Grace includes a read file in the folder with the split files, but I don't think that makes sense for using with SortmeRNA
# I'm also not going to zip up the file, because I'd just need to unzip back on gluster
cp ${code} /mnt/amlinz/gluster/GEODES_metaT_split/${code}
rm -r ${code}
rm GEODES001.filter-MTF.fastq


```

And the submit file:

```{r, eval = F}
# 00split_fastq.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 00split_fastq_$(Cluster).log
error = 00split_fastq_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 00split_fastq.sh
#arguments = $(fastqfile)
output = 00split_fastq_$(Cluster).out
#
# No files to transfer, since it's going to interact with Gluster instead of the submit node
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files = 
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
# This gives me ~800 target computers
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2GB
request_disk = 12GB
#
# Just one job for testing
queue 1
```

I did a little tweaking of naming the output files. Here's the final version of the executable. I'm debating adding a line to add .fastq to the end of the split files, but will see if sortmerna can figure it out first.

```{r, eval = F}

#!/bin/bash

cp /mnt/gluster/amlinz/GEODES_metaT/GEODES001.filter-MTF.fastq.gz ./
gzip -d GEODES001.filter-MTF.fastq.gz

# make a folder for each metagenome
code=`echo "GEODES001.filter-MTF.fastq" | cut -d'.' -f1`
mkdir ${code}

# Split by number of lines
split -l 1000000 GEODES001.filter-MTF.fastq ${code}/${code}

# Grace includes a read file in the folder with the split files, but I don't think that makes sense for using with SortmeRNA
# I'm also not going to zip up the file, because I'd just need to unzip back on gluster
mv ${code} /mnt/gluster/amlinz/GEODES_metaT_split/
rm GEODES001.filter-MTF.fastq
```


Now to test sortmerna on one of the split files!

New executable:
```{r, eval = F}

#!/bin/bash
#Sort metatranscriptomic reads into rRNA and non-rRNA files
#Starting with one fastq file for now

#Transfer the fasta file from gluster
cp /mnt/gluster/amlinz/GEODES_metaT_split/GEODES001/GEODES001aa ./

#Unzip files
#gzip -d GEODES001.filter-MTF.fastq.gz
tar -xvf sortmerna-2.1-linux-64.tar.gz

cd sortmerna-2.1-linux-64

#Index the rRNA databases
./indexdb_rna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:\./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db

#Run the sorting program
./sortmerna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db --reads ../GEODES001aa  --fastx --aligned GEODES001_rRNA --other GEODES001_nonrRNA --log -v -m 3 -a 10

#Let's unpack the parameters I've chosen. --ref refers to the databases I've just indexed. --reads says use the fastq file provided. --fastx means I would like a fastq file as output. --aligned is the name for rRNA reads, --other is the name for non-rRNA reads. --log says compute statistics about the run. -v means verbose. I've left the alignment setting at the default, --best. There's a faster setting, but I'm hoping I won't need it. And finally, -m tells sortmerna it can use 3GB of RAM to hold each piece of the fastq file as it processes. Hopefully that will give it a fighting chance.
#Move the output files back to gluster
mv GEODES001aa_rRNA.fastq /mnt/gluster/amlinz/GEODES_rRNA/
mv GEODES001aa_nonrRNA.fastq /mnt/gluster/amlinz/GEODES_nonrRNA/

#Remove files
cd ..
rm GEODES001aa
rm sortmerna-2.1-linux-64.tar.gz
rm -r sortmerna-2.1-linux-64


```

Submit file is the same except changed requirements - 10 threads, 3GB RAM, 5GB drive space

Sortmerna has no problem running on the file without the fastq extension, but I'm having trouble moving the output to gluster. Moving to Zissou to find out what the output file names should be.

Turns out the error was because there was no .fastq extension.

I added:
```{r, eval = F}
# Rename files to include .fastq extension
for file in ${code}/*;do mv "$file" "$file.fastq";done
```

Note: code will break if there's already a folder named GEODES001 in the sample place. Delete old run outputs before rerunning.

Looks like everything worked! My next goal is to run the rRNA sorting script on all the pieces of GEODES001.

#### 2017-02-07

First things first, let's make a list of the files to run:
```{r, eval = F}
find /mnt/gluster/amlinz/GEODES_metaT_split/ -type f > path2splitfastqs.txt
```

Now modify the submit file to read from that list:
```{r, eval = F}

# 01rRNA_removal.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 01rRNA_removal_$(Cluster).log
error = 01rRNA_removal_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 01rRNA_removal.sh
arguments = $(fastqfile)
output = 01rRNA_removal_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = sortmerna-2.1-linux-64.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue fastqfile from path2splitfastqs.txt
```

And add the $1 argument variable to the executable:

```{r, eval = F}

#!/bin/bash
#Sort metatranscriptomic reads into rRNA and non-rRNA files

#Transfer the fasta file from gluster
cp $1 ./
name=$(basename '$1' |cut -d'.' -f1)

#Unzip files
tar -xvf sortmerna-2.1-linux-64.tar.gz

cd sortmerna-2.1-linux-64

#Index the rRNA databases
./indexdb_rna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:\./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db

#Run the sorting program
./sortmerna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db --reads ../${name}.fastq  --fastx --aligned ${name}_rRNA --other ${name}_nonrRNA --log -v -m 1 -a 1

#Let's unpack the parameters I've chosen. --ref refers to the databases I've just indexed. --reads says use the fastq file provided. --fastx means I would like a fastq file as output. --aligned is the name for rRNA reads, --other is the name for non-rRNA reads. --log says compute statistics about the run. -v means verbose. I've left the alignment setting at the default, --best. There's a faster setting, but I'm hoping I won't need it. And finally, -m tells sortmerna it can use 3GB of RAM to hold each piece of the fastq file as it processes. Hopefully that will give it a fighting chance.
#Move the output files back to gluster
mv ${name}_rRNA.fastq /mnt/gluster/amlinz/GEODES_rRNA/
mv ${name}_nonrRNA.fastq /mnt/gluster/amlinz/GEODES_nonrRNA/

#Remove files
cd ..
rm ${name}.fastq
rm sortmerna-2.1-linux-64.tar.gz
rm -r sortmerna-2.1-linux-64

```

I'm going to try running this on just three files to start:

```{r, eval = F}
head -3 path2splitfastqs.txt > test_path2splitfastqs.txt
```

Looking good! In fact, I can drop my memory requirements down to 1GB RAM and 2GB drive space.

Now one last script - putting the files back together. I'll need to copy all of the files from each GEODES number to a server, concatenate them, and send it back to gluster.

This line makes a file of all of the sample nams:

```{r, eval = F}
for file in /mnt/gluster/amlinz/GEODES_metaT/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > samplenames.txt

head -1 samplenames.txt > test_samplenames.txt
```

First stab at the executable:
```{r, eval = F}
#!/bin/bash
#Concatenate sortmerna output

cp /mnt/gluster/amlinz/GEODES_nonrRNA/$1??_nonrRNA.fastq ./
cat $1??_nonrRNA.fastq > $1_nonrRNA.fastq
mv $1_nonrRNA.fastq /mnt/gluster/amlinz/GEODES_nonrRNA_concat/
rm *_nonrRNA.fastq

cp /mnt/gluster/amlinz/GEODES_rRNA/$1*??_rRNA.fastq ./
cat $1??_rRNA.fastq > $1_rRNA.fastq
mv $1_rRNA.fastq /mnt/gluster/amlinz/GEODES_rRNA_concat/
rm *_rRNA.fastq

```

And the submit file:
```{r, eval = F}
# 02cat_files.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02cat_files_$(Cluster).log
error = 02cat_files_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 02cat_files.sh
arguments = $(sample)
output = 02cat_files_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files = 
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 10GB
request_disk = 10GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue sample from test_samplenames.txt
```

I got an error from cat saying "input file is output file". I added ?? characters to the selection of file names to specify that I want GEODES001ae_nonrRNA.fastq, not GEODES001_nonrRNA.fastq.

That worked! Now I know I said that was the last process, but there's something else I want. I want to know the ratio of nonrRNA to rRNA reads for each sample. I can add that to the above script, and also zip the files up when I'm done. I've add the output file $1_rRNA_results.txt to transfer_output in .sub.

New executable:
```{r, eval = F}
#!/bin/bash
#Concatenate sortmerna output

cp /mnt/gluster/amlinz/GEODES_nonrRNA/$1??_nonrRNA.fastq ./
cat $1??_nonrRNA.fastq > $1_nonrRNA.fastq
nonrRNAcount=$(wc -l $1_nonrRNA.fastq)
gzip $1_nonrRNA.fastq
mv $1_nonrRNA.fastq.gz /mnt/gluster/amlinz/GEODES_nonrRNA_concat/
rm *_nonrRNA.fastq

cp /mnt/gluster/amlinz/GEODES_rRNA/$1*??_rRNA.fastq ./
cat $1??_rRNA.fastq > $1_rRNA.fastq
rRNAcount=$(wc -l $1_rRNA.fastq)
gzip $1_rRNA.fastq
mv $1_rRNA.fastq.gz /mnt/gluster/amlinz/GEODES_rRNA_concat/
rm *_rRNA.fastq

echo ${nonrRNAcount},${rRNAcount} > $1_rRNA_results.txt
```

Alright! I'm going to clean up my directories, then rerun all scripts on 3 samples. I'll document everything at the top "most recent workflow" section so that I'll be ready to go when I have all of the data downloaded (hopefully tomorrow!)

####2017-02-08

Everything great on the three samples, except for 3 samples getting stuck in infinite run time. I asked Christina at CHTC about this and she said that just happens sometimes. In that case, I can stop and restart the runs with:

```{r, eval = F}
condor_q -run -nobatch # Check to see whate server they're on
condor_hold amlinz # Move all my jobs to holding
condor_release amlinz # Start all jobs that are in holding
```

After that it was all peachy, right up until I wiped my home folder clean with a bad rm command *sigh*. Luckily I'd downloaded all my scripts to my github repo, but I lost a couple minor changes. For posterity, here's the FINAL FINAL versions of the scripts. I'm testing these on one more sample while I'm downloading the remaining fastq files from the JGI portal.

New fun fact: Since I saved my files on a Windows machine, they got new line endings and won't run. Fix using:
```{r, eval = F}
dos2unix *.sh
dos2unix *.sub
```
I love Windows.

00split_fastq.sh:
```{r, eval = F}

#!/bin/bash

cp /mnt/gluster/amlinz/GEODES_metaT/$1.filter-MTF.fastq.gz ./
gzip -d $1.filter-MTF.fastq.gz

# make a folder for each metagenome
mkdir $1

# Split by number of lines
split -l 1000000 $1.filter-MTF.fastq $1/$1

# Rename files to include .fastq extension
for file in $1/*;do mv "$file" "$file.fastq";done

# Grace includes a read file in the folder with the split files, but I don't think that makes sense for using with SortmeRNA
# I'm also not going to zip up the file, because I'd just need to unzip back on gluster
mv $1 /mnt/gluster/amlinz/GEODES_metaT_split/
rm $1.filter-MTF.fastq

```

00split_fastq.sub:
```{r,eval = F}

# 00split_fastq.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 00split_fastq_$(Cluster).log
error = 00split_fastq_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 00split_fastq.sh
arguments = $(sample)
output = 00split_fastq_$(Cluster).out
#
# No files to transfer, since it's going to interact with Gluster instead of the submit node
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files =
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2 GB
request_disk = 12 GB
#
# Just one job for testing
queue sample from samplenames.txt
```

01rRNA_removal.sh:
```{r, eval = F}

#!/bin/bash
#Sort metatranscriptomic reads into rRNA and non-rRNA files

#Transfer the fasta file from gluster
cp $1 ./
name=$(basename $1 |cut -d'.' -f1)

#Unzip files
tar -xvf sortmerna-2.1-linux-64.tar.gz

cd sortmerna-2.1-linux-64

#Index the rRNA databases
./indexdb_rna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:\./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db

#Run the sorting program
./sortmerna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db --reads ../${name}.fastq  --fastx --aligned ${name}_rRNA --other ${name}_nonrRNA --log -v -m 1 -a 1

#Let's unpack the parameters I've chosen. --ref refers to the databases I've just indexed. --reads says use the fastq file provided. --fastx means I would like a fastq file as output. --aligned is the name for rRNA reads, --other is the name for non-rRNA reads. --log says compute statistics about the run. -v means verbose. I've left the alignment setting at the default, --best. There's a faster setting, but I'm hoping I won't need it. And finally, -m tells sortmerna it can use 3GB of RAM to hold each piece of the fastq file as it processes. Hopefully that will give it a fighting chance.
#Move the output files back to gluster
mv ${name}_rRNA.fastq /mnt/gluster/amlinz/GEODES_rRNA/
mv ${name}_nonrRNA.fastq /mnt/gluster/amlinz/GEODES_nonrRNA/

#Remove files
cd ..
rm ${name}.fastq
rm sortmerna-2.1-linux-64.tar.gz
rm -r sortmerna-2.1-linux-64

```

01rRNA_removal.sub
```{r, eval = F}

# 01rRNA_removal.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 01rRNA_removal_$(Cluster).log
error = 01rRNA_removal_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 01rRNA_removal.sh
arguments = $(fastqfile)
output = 01rRNA_removal_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = sortmerna-2.1-linux-64.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue fastqfile from path2splitfastqs.txt
```

02cat_files.sh:
```{r, eval = F}

#!/bin/bash
#Concatenate sortmerna output

cp /mnt/gluster/amlinz/GEODES_nonrRNA/$1??_nonrRNA.fastq ./
cat $1??_nonrRNA.fastq > $1_nonrRNA.fastq
nonrRNAcount=$(wc -l $1_nonrRNA.fastq)
gzip $1_nonrRNA.fastq
mv $1_nonrRNA.fastq.gz /mnt/gluster/amlinz/GEODES_nonrRNA_concat/
rm *_nonrRNA.fastq

cp /mnt/gluster/amlinz/GEODES_rRNA/$1*??_rRNA.fastq ./
cat $1??_rRNA.fastq > $1_rRNA.fastq
rRNAcount=$(wc -l $1_rRNA.fastq)
gzip $1_rRNA.fastq
mv $1_rRNA.fastq.gz /mnt/gluster/amlinz/GEODES_rRNA_concat/
rm *_rRNA.fastq

echo ${nonrRNAcount},${rRNAcount} > $1_rRNA_results.txt
```

02cat_files.sub
```{r, eval = F}

# 02cat_files.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02cat_files_$(Cluster).log
error = 02cat_files_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 02cat_files.sh
arguments = $(sample)
output = 02cat_files_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files =
#transfer_output_files = $(sample)_rRNA_results.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 10GB
request_disk = 10GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue sample from samplenames.txt
```

Everything looks ready for the full run now! I'm just waiting on the file upload to gluster. It'll take at least the rest of the night, so I'll plan on starting in the morning.

####2017-02-09

Off we go! Everything finished downloading around 10 this morning. Got the first step started at 10:35. I'm currently running on 85 samples. (The remaining 23 are still being sequenced or processed)

2 got stuck, and produced errors that there was their output already existed in gluster, but everything looks fine so I'll carry on. For reference, those two errors were from GEODES050 and GEODES154. Total time: 3 hours.

Moving on. 7897 jobs submitted - holy smokes!
