# Process mapped reads

####Goal of this analysis

The mapping in the last step produced a .sam file where each line is a read in the metatranscriptome, and information is included about whether or not it mapped, and where it mapped. Technically, this is the data I need to address my hypotheses, but each file is way to large and clunky to analyze. I'd like to analyze these using the DESEQ R package, which requires a table where rows are genes and columns are samples. 

##### Approach

I'm going to use htseq to count up to reads mapping to the same thing in each metaT. I'll need samtools again to reformat the mapping output. Once I get an htseq-count summary file for each sample, I'll combine them into a table for DESEQ. While I'm here, I'll also create metadata files from the gff file (reproducibly!) so I can redo it for any genomes in my database.

## Most recent workflow. 
####Use this if you want to replicate our protocol. Updated 2017-03-02

1. Setup
Set up folders in gluster for the outputs:
```{r, eval = F}
mkdir /mnt/gluster/amlinz/GEODES_mapping_summaries/
```

Build python install
interactive.sub:
```{r, eval = F}
universe = vanilla
# Name the log file:
log = interactive.log

# Name the files where standard output and error should be saved:
output = process.out
error = process.err

# If you wish to compile code, you'll need the below lines. 
#  Otherwise, LEAVE THEM OUT if you just want to interactively test!
+IsBuildJob = true
requirements = (OpSysAndVer =?= "SL6") && ( IsBuildSlot == true )

# Indicate all files that need to go into the interactive job session,
#  including any tar files that you prepared:
transfer_input_files = Python-2.7.13.tgz

# It's still important to request enough computing resources. The below 
#  values are a good starting point, but consider your file sizes for an
#  estimate of "disk" and use any other information you might have
#  for "memory" and/or "cpus".
request_cpus = 1
request_memory = 1GB
request_disk = 1GB

queue
```

Start the session with:
```{r, eval = F}
condor_submit -i interactive.sub
```

Here's what I'm typing in the interactive session:
```{r, eval = F}
mkdir python
tar -xvf Python-2.7.13.tgz
cd Python-2.7.13
./configure --prefix=$(pwd)/../python
make
make install
cd ..
ls python
ls python/bin
# Maybe I can install htseq right in here?
export PATH=$(pwd)/python/bin:$PATH
wget https://bootstrap.pypa.io/get-pip.py
python get-pip.py
pip install numpy
pip install matplotlib
pip install htseq
pip install pysam
# hey, that worked!
tar -czvf python.tar.gz python/
exit

# In your home folder, copy the install to gluster (it's too big to transfer via the submit file)
cp python.tar.gz /mnt/gluster/amlinz/
```

Make a list of files to run:
```{r, eval = F}
for file in /mnt/gluster/amlinz/GEODES_mapping_concat/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > samfiles.txt
cat samfiles.txt

```

2. Run 03processing:
```{r, eval = F}

# 03processing.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03processing_$(Cluster).log
error = 03processing_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 03processing.sh
arguments = $(samplename)
output = 03processing_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = samtools.tar.gz,mapping_database.gff.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 3GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from samplenames.txt
```

Here's its executable, 03processing.sh:
```{r, eval = F}
#!/bin/bash
#Make a count table of mapped reads

#Transfer data from gluster
cp /mnt/gluster/amlinz/GEODES_mapping_concat/$1.all.sam .
cp /mnt/gluster/amlinz/python.tar.gz .


#Unzip programs
tar xzf python.tar.gz
tar xvf samtools.tar.gz
gzip -d mapping_database.gff.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

#Manipulate the output
samtools view -b -S $1.all.sam > $1.bam 
samtools sort -o $1.sorted $1.bam 
samtools index $1.sorted.bam

#Count reads
python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t CDS -i locus_tag -m intersection-strict -o $1.CDS.sam $1.sorted.bam mapping_database.gff > $1.CDS.out

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.CDS.out /mnt/gluster/amlinz/GEODES_mapping_summaries/
rm $1.CDS.out

#Clean up
rm -rf python
rm -rf samtools
rm *.sam
rm *.bam
rm *.bai
rm mapping_database*
rm *.tar.gz
```


Check the results

```{r, eval = F}
ls -ltr /mnt/gluster/amlinz/GEODES_mapping_summaries/
```

3. Compile the result into a single table using the scripts 03maketable:

03maketable.sub:
```{r, eval = F}
# 03maketable.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03maketable_$(Cluster).log
error = 03maketable_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 03maketable.sh
#arguments = 
output = 03maketable_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files = 
transfer_output_files = GEODES_genes_2017-02-22.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 1GB
request_disk = 2GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue
```

03maketable.sh
```{r, eval = F}
#!/bin/bash
#Combine htseq-count results into one table
cp /mnt/gluster/amlinz/GEODES_mapping_summaries/* .

awk '{print $1}' GEODES001_nonrRNA.CDS.out > rownames.txt
for file in *.out;do awk '{print $2}' $file > temp.txt;sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - temp.txt > temp2.txt && mv temp2.txt $file;done
echo "\t" | cat - rownames.txt > temp3.txt && mv temp3.txt rownames.txt
files=$(echo *.out)
paste rownames.txt $files > GEODES_genes_2017-02-27.txt

rm *.out
rm rownames.txt
rm temp*.txt

```

4. Make a metadata file

You'll need a custom file with information about each locus tag in the table - what genome did it come from and what gene product does it encode? Make this with make_metadata_file.R from your mapping_database.gff file. Use the commented bash code to get contig names from your gff file first. Then change your file paths and go ahead and source it. I like to do this with Rstudio on my desktop, so I downloaded the file first. 

5. Cleanup

The only thing I want to save is the table itself, which I downloaded onto my computer with WinSCP. Once you've checked the table to make sure it works, delete all /*.err, /*.out, and /*.log files. Move onto the 05R_analyses directory, where you'll link those locus tags to genomes and gene products.

#Lab Notebook

####2017-02-21

Some of this is carried over from the 02mapping lab notebook, since I was originally planning to do that in one step. However, the script was large, slow and error prone, so here we are. I'll need my samtools install from the last script, and the python install with the htseq module.

Make a list of files to run:
```{r, eval = F}
ls /mnt/gluster/amlinz/GEODES_mapping_concat/ > samfiles.txt
cat samfiles.txt

```

Copy the python file over to gluster then write submit script - the only things it's sending over is the samtools install and the gff file.
03processing.sub:
```{r, eval = F}
# 03processing.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03processing_$(Cluster).log
error = 03processing_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 03processing.sh
arguments = $(samplename)
output = 03processing_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = samtools.tar.gz,mapping_database.gff.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 3GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from samfiles.txt
```

And the executable, based on my early mapping testing with htseq:
```{r, eval = F}
#!/bin/bash
#Make a count table of mapped reads

#Transfer data from gluster
cp /mnt/gluster/amlinz/GEODES_mapping_concat/$1.all.sam .
cp /mnt/gluster/amlinz/python.tar.gz .


#Unzip programs
tar xzf python.tar.gz
tar xvf samtools.tar.gz
gzip -d mapping_database.gff.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

#Manipulate the output
samtools view -b -S $1.all.sam > $1.bam 
samtools sort $1.bam $1.sorted 
samtools index $1.sorted.bam

#Count reads
python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t CDS -i locus_tag -m intersection-strict -o $1.CDS.sam $1.sorted.bam mapping_database.gff > $1.CDS.out

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.CDS.out /mnt/gluster/amlinz/GEODES_mapping_summaries/
rm $1.CDS.out

#Clean up
rm -rf python
rm -rf samtools
rm *.sam
rm *.bam
rm *.bai
rm mapping_database*
rm *.tar.gz
  
```

Something's wrong with the header in the all.sam file? That would be an issue with the merging script. Going into interactive mode to check. samtools is a pain and their documentation seems out of date.

Fixed the issue - left out the -h flag in samtools view that leaves the header in output. Rerun 02merge script and 03processing, and got the following error:

Error occured when processing SAM input (record #285895 in file GEODES002_nonrRNA.all.sam):
  object of type 'NoneType' has no len()
  [Exception type: TypeError, raised in _HTSeq.pyx:771]

It happened in all three files, although in different lines. The only thing I can find online is here? https://sourceforge.net/p/htseq/bugs/12/ Doesn't seem helpful because many of my lines have *. Will sleep on this and come back tomorrow.

####2017-02-22

Back to the googling. In regular old python, you can get this error when measuring the length of "None" because that is a protected variable. Does "None" appear in my sam files?

```{r, eval = F}
grep -n None /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam
```

Nothing. Replaced search term with \* and nearly every line has that symbol. I didn't get this error before I split and merged files - what is different now?
Not sure, but I realized I wasn't including header in the line count. So record #341922 is actually line 382861, which does have an asterix in a weird place - after the sequence. Is this my error? let's change that * to "None" and see what happens.

```{r, eval = F}
awk '{gsub(/\*/,"None",$11)}1' /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam > /mnt/gluster/amlinz/GEODES_mapping_concat/test.sam
```

Now I get an error that says "truncated file" right at that same line. There are only 18 lines with no Phred score out of 500,000+ , so I'll just remove these and address it later.

```{r, eval = F}
#Count phred scores with "*"
 awk '{print $11}' /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam | grep \* |wc -l
#Remove these lines
awk '{ if ( $11 != "\*" ) { print $0; } }' /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam > /mnt/gluster/amlinz/GEODES_mapping_concat/test.all.sam
```

Looks like that removed the right lines! I get an warning about the escape character, but I think that's what I want. Testing 03processing again.

It made a feature.out file! I'm just going to look up a couple "features" in the gff file to make sure I can use that as metadata.
```{r, eval = F}
gzip -d mapping_database.gff.gz
grep scaffold_01090 mapping_database.gff
grep YUCDRAFT_00363 mapping_database.gff
gzip mapping_database.gff
```

Looking good! Those both produce unique lines. I'm going to add the awk lines (including printing how many * lines were removed) and run both the merging and processing on all three files. (make sure to clean up dud files first)

I'm still getting this error occasionally in the merge step:
[sam_header_line_parse] expected '@XY', got [@BFFGGGGGGGGGGGDGGGGD>FGGCGGGGGGGGGGGGGGGGEGGGGGBGFGGGDGGGGGGGEAGGGGGGGGGGEGCGGGFGGGGCGGGGGGBGGGGG:CGGGGGGGGGEGBEG<BGGEG/DDEGGGGGDD=EGEGD>GGGGGGGGG     4       *       0       0       *       *       0   0*       AS:i:0  XS:i:0]
Hint: The header tags must be tab-separated.
[samopen] no @SQ lines in the header.
[sam_read1] missing header? Abort!

Ran the merging and processing, and everything looks great! Now I need a script to combine all of the output files into a single table. I'm going to start an interactive session for testing this.

```{r, eval = F}
condor_submit -i testinteractive.sub

cp /mnt/gluster/amlinz/GEODES_mapping_summaries/* .

#Are all files the same length?
for file in *.out;do wc -l $file;done

#Yes - presumably the rownames are the same.
#Add a header of sample name to each of the files
for file in *.out;do sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - $file > temp && mv temp $file;done

#That worked!
#Now try join to combine

join GEODES002_nonrRNA.CDS.out GEODES001_nonrRNA.CDS.out 
# Worked, but gave me an warning about sorting and didn't keep the headers.

#Let's try paste instead:
paste -d' ' GEODES001_nonrRNA.CDS.out GEODES002_nonrRNA.CDS.out

#Keeps the headers but prints every rowname (with the header on the rowname)
#Can I 1. make a separeate file of rownames 2. remove rownames from the out files 3. add the header 4. paste everything together?

awk '{print $1}' GEODES001_nonrRNA.CDS.out > rownames.txt
for file in *.out;do awk '{print $2}' $file > temp.txt;sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - temp.txt > temp2.txt && mv temp2.txt $file;done
echo "/t" | cat - rownames.txt > temp3.txt && mv temp3.txt rownames.txt
files=$(echo *.out)
paste rownames.txt $files > GEODES_genes_all.txt

#Looking good! exit and download to file to make sure I can open it in Rstudio
exit
```

Write this up as a script and test again:
03maketable.sub:
```{r, eval = F}
# 03maketable.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03maketable_$(Cluster).log
error = 03maketable_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 03maketable.sh
#arguments = 
output = 03maketable_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files = 
transfer_output_files = GEODES_genes_2017-02-22.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 1GB
request_disk = 2GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue
```

03maketable.sh
```{r, eval = F}
#!/bin/bash
#Combine htseq-count results into one table
cp /mnt/gluster/amlinz/GEODES_mapping_summaries/* .

awk '{print $1}' GEODES001_nonrRNA.CDS.out > rownames.txt
for file in *.out;do awk '{print $2}' $file > temp.txt;sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - temp.txt > temp2.txt && mv temp2.txt $file;done
echo "\t" | cat - rownames.txt > temp3.txt && mv temp3.txt rownames.txt
files=$(echo *.out)
paste rownames.txt $files > GEODES_genes_2017-02-22.txt

rm *.out
rm rownames.txt
rm temp*.txt

```

Beautiful! Everything seems to be in working order. Time to put this aside for now and move over to the kraken classification workflow.

####2017-02-23

I got some weird errors about truncated files when trying to get a fastq file out of samtools, so I'm going back to all scripts, testing samtools in interactive mode, and then running. I'll do the same here, while also updating the "current workflow" section.

####2017-02-27

Ran the processing and didn't get any errors but about 10 samples produced empty output files. I'll come back and fix this if I have time before lab meeting. Moving on to making the metadata file now. 