# Process mapped reads

####Goal of this analysis

The mapping in the last step produced a .sam file where each line is a read in the metatranscriptome, and information is included about whether or not it mapped, and where it mapped. Technically, this is the data I need to address my hypotheses, but each file is way to large and clunky to analyze. I'd like to analyze these using the DESEQ R package, which requires a table where rows are genes and columns are samples. 

##### Approach

I'm using a program called FeatureCounts to count the number of reads mapped, then compiling these into a table. I'm also making metadata files to tell me which contig each gene locus_id belongs to and what it is annotated as.

## Most recent workflow. 
####Use this if you want to replicate our protocol. Updated 2017-04-25

1. Set up folders in gluster for the outputs, and a list of files to run.
```{r, eval = F}
mkdir /mnt/gluster/amlinz/GEODES_mapping_summaries/

for file in /mnt/gluster/amlinz/GEODES_mapping_results/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > bamfiles.txt
cat bamfiles.txt
```

Build the gff file in an interactive session. Note that every gff format is different, and I'll likely need to modify this for every type of reference genome I (you) want to use. I'm using the program genometools to fix up gff files - better than wrangling around in awk - but it can't fix everything. In my case, the gff files for the metagenome assemblies produced by the JGI don't have an initial comment line and use a different strand annotation (-1, 1, instead of -, +), both of which crash genometools' gff tidy function.

build_gff.sub:
```{bash, eval = F}

#build_gff.sub
#
universe = vanilla
# Name the log file:
log = interactive.log

# Name the files where standard output and error should be saved:
output = process.out
error = process.err

# If you wish to compile code, you'll need the below lines.
#  Otherwise, LEAVE THEM OUT if you just want to interactively test!
+IsBuildJob = true
requirements = (OpSysAndVer =?= "SL6") && ( IsBuildSlot == true )

# Indicate all files that need to go into the interactive job session,
#  including any tar files that you prepared:
transfer_input_files = zipped/genometools-1.5.9.tar.gz

# It's still important to request enough computing resources. The below
#  values are a good starting point, but consider your file sizes for an
#  estimate of "disk" and use any other information you might have
#  for "memory" and/or "cpus".
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 20GB
request_disk = 25GB

queue
```

Running the interactive session:
```{bash, eval = F}
condor_submit -i submits/build_gff.sub
# Wait

tar -xvzf genometools-1.5.9.tar.gz
cd genometools-1.5.9
make cairo=no
make prefix=$(pwd)/../genometools/ cairo=no install
cd ..
export PATH=$(pwd)/genometools/bin:$PATH

#Copy the gff files to this directory
cp /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff .

#Fix the strand issue
for file in *.gff; do awk -F'\t' -vOFS='\t' '{gsub("-1", "-", $7); gsub("1", "+", $7); print}' $file > f1_$file; done

#Add first comment line
for file in f1*gff; do echo '##gff-version 3' | cat - $file > temp && mv temp $file; done
echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff

#Sort and tidy
for file in f1*gff; do gt gff3 -sort yes -tidy -force -retainids -o tidy_$file $file; done

#Run the merge
gt merge -o mapping_database.gff tidy*gff

gzip mapping_database.gff

cp mapping_database.gff.gz /mnt/gluster/amlinz/

rm *.gz
rm *.gff
rm -r genome*
rm temp
exit

```

Last thing before running the scripts is to build an install for the FeatureCounts program. I installed it from source code and am using the Linux instructions here: http://bioinf.wehi.edu.au/subread-package/

install_featurecounts.sub
```{bash, eval = F}

#install_featurecounts.sub
#
universe = vanilla
# Name the log file:
log = interactive.log

# Name the files where standard output and error should be saved:
output = process.out
error = process.err

# If you wish to compile code, you'll need the below lines.
#  Otherwise, LEAVE THEM OUT if you just want to interactively test!
+IsBuildJob = true
requirements = (OpSysAndVer =?= "SL6") && ( IsBuildSlot == true )

# Indicate all files that need to go into the interactive job session,
#  including any tar files that you prepared:
transfer_input_files = zipped/subread-1.5.2-source.tar.gz

# It's still important to request enough computing resources. The below
#  values are a good starting point, but consider your file sizes for an
#  estimate of "disk" and use any other information you might have
#  for "memory" and/or "cpus".
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 8GB
request_disk = 4GB

queue

```

In the interactive session:
```{bash, eval = F}
condor_submit -i install_featurecounts.sub
# Wait

tar zxvf subread-1.5.2-source.tar.gz
cd subread-1.5.2-source/src
make -f Makefile.Linux
tar czvf subread.tar.gz subread-1.5.2-source

# Move tarball back to gluster
cp subread.tar.gz /mnt/gluster/amlinz

# Clean up
rm subread.tar.gz
rm -r subread-1.5.2.source
exit

# Move tarball into zipped folder
mv /mnt/gluster/amlinz/subread.tar.gz zipped/subread.tar.gz

```

2. Peform the counting. Like Jimmy Johns, FeatureCounts is freaky fast (and seems to be freaky good as well, at least it gives the same results as HTSeq-count) so this should only take a day or so. I'm using minOverlap and fracOverlap == 1, equivalent to HTSeq's mode intersection_strict (read must be contained in a gene to be mapped to that gene). 

FeatureCounts produces two files. The txt.summary has mapped and unmapped read proportions, while the mapped.txt has counts per feature. I remove unmapped reads from the file to save on space, otherwise I can't open the files.

06featurecounts.sub:
```{bash, eval = F}

# 06featurecounts.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03featurecounts_$(Cluster).log
error = 03featurecounts_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/03featurecounts.sh
arguments = $(samplename)
output = 03featurecounts_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/subreads.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 20GB
request_disk = 12GB
#
# Tell HTCondor to run every file in the provided list:
queue samplename from bamfiles.txt

```

06featurecounts.sh:
```{bash, eval = F}

#!/bin/bash
#Make a count table of mapped reads

#Transfer data from gluster
cp /mnt/gluster/amlinz/GEODES_mapping_results/$1.mapped.bam .
cp /mnt/gluster/amlinz/mapping_database.gff.gz .

#Unzip programs
tar xvzf subreads.tar.gz
gzip -d mapping_database.gff.gz

#Count reads
./subread-1.5.2-source/bin/featureCounts -t CDS -g locus_tag --minOverlap 1 --fracOverlap 1 -a mapping_database.gff -o $1.CDS.txt $1.mapped.bam

# Keep only mapped reads
awk '(NR==1) || ($7 > 0 ) ' $1.CDS.txt > $1.CDS.mapped.txt

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.CDS.mapped.txt /mnt/gluster/amlinz/GEODES_mapping_summaries/
cp $1.CDS.txt.summary /mnt/gluster/amlinz/GEODES_mapping_summaries/
rm $1.CDS*

#Clean up
rm *.bam
rm mapping_database*
rm *.tar.gz
rm -r subread-1.5.2-source

```


Run and check the results

```{r, eval = F}
condor_submit submits/06feature_counts.sub

ls -ltr 06*err
ls -ltr /mnt/gluster/amlinz/GEODES_mapping_summaries/
```

3. Compile the result into a single table using the scripts 03maketable:

03maketable.sub:
```{r, eval = F}
# 03maketable.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03maketable_$(Cluster).log
error = 03maketable_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 03maketable.sh
#arguments = 
output = 03maketable_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files = 
transfer_output_files = GEODES_genes_2017-02-22.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 1GB
request_disk = 2GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue
```

03maketable.sh
```{r, eval = F}
#!/bin/bash
#Combine htseq-count results into one table
cp /mnt/gluster/amlinz/GEODES_mapping_summaries/* .

awk '{print $1}' GEODES001_nonrRNA.CDS.out > rownames.txt
for file in *.out;do awk '{print $2}' $file > temp.txt;sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - temp.txt > temp2.txt && mv temp2.txt $file;done
echo "\t" | cat - rownames.txt > temp3.txt && mv temp3.txt rownames.txt
files=$(echo *.out)
paste rownames.txt $files > GEODES_genes_2017-02-27.txt

rm *.out
rm rownames.txt
rm temp*.txt

```

4. Make a metadata file

You'll need a custom file with information about each locus tag in the table - what genome did it come from and what gene product does it encode? Make this with make_metadata_file.R from your mapping_database.gff file. Use the commented bash code to get contig names from your gff file first. Then change your file paths and go ahead and source it. I like to do this with Rstudio on my desktop, so I downloaded the file first. 

5. Cleanup

The only thing I want to save is the table itself, which I downloaded onto my computer with WinSCP. Once you've checked the table to make sure it works, delete all /*.err, /*.out, and /*.log files. Move onto the 05R_analyses directory, where you'll link those locus tags to genomes and gene products.

#Lab Notebook

####2017-02-21

Some of this is carried over from the 02mapping lab notebook, since I was originally planning to do that in one step. However, the script was large, slow and error prone, so here we are. I'll need my samtools install from the last script, and the python install with the htseq module.

Make a list of files to run:
```{r, eval = F}
ls /mnt/gluster/amlinz/GEODES_mapping_concat/ > samfiles.txt
cat samfiles.txt

```

Copy the python file over to gluster then write submit script - the only things it's sending over is the samtools install and the gff file.
03processing.sub:
```{r, eval = F}
# 03processing.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03processing_$(Cluster).log
error = 03processing_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 03processing.sh
arguments = $(samplename)
output = 03processing_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = samtools.tar.gz,mapping_database.gff.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 3GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from samfiles.txt
```

And the executable, based on my early mapping testing with htseq:
```{r, eval = F}
#!/bin/bash
#Make a count table of mapped reads

#Transfer data from gluster
cp /mnt/gluster/amlinz/GEODES_mapping_concat/$1.all.sam .
cp /mnt/gluster/amlinz/python.tar.gz .


#Unzip programs
tar xzf python.tar.gz
tar xvf samtools.tar.gz
gzip -d mapping_database.gff.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

#Manipulate the output
samtools view -b -S $1.all.sam > $1.bam 
samtools sort $1.bam $1.sorted 
samtools index $1.sorted.bam

#Count reads
python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t CDS -i locus_tag -m intersection-strict -o $1.CDS.sam $1.sorted.bam mapping_database.gff > $1.CDS.out

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.CDS.out /mnt/gluster/amlinz/GEODES_mapping_summaries/
rm $1.CDS.out

#Clean up
rm -rf python
rm -rf samtools
rm *.sam
rm *.bam
rm *.bai
rm mapping_database*
rm *.tar.gz
  
```

Something's wrong with the header in the all.sam file? That would be an issue with the merging script. Going into interactive mode to check. samtools is a pain and their documentation seems out of date.

Fixed the issue - left out the -h flag in samtools view that leaves the header in output. Rerun 02merge script and 03processing, and got the following error:

Error occured when processing SAM input (record #285895 in file GEODES002_nonrRNA.all.sam):
  object of type 'NoneType' has no len()
  [Exception type: TypeError, raised in _HTSeq.pyx:771]

It happened in all three files, although in different lines. The only thing I can find online is here? https://sourceforge.net/p/htseq/bugs/12/ Doesn't seem helpful because many of my lines have *. Will sleep on this and come back tomorrow.

####2017-02-22

Back to the googling. In regular old python, you can get this error when measuring the length of "None" because that is a protected variable. Does "None" appear in my sam files?

```{r, eval = F}
grep -n None /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam
```

Nothing. Replaced search term with \* and nearly every line has that symbol. I didn't get this error before I split and merged files - what is different now?
Not sure, but I realized I wasn't including header in the line count. So record #341922 is actually line 382861, which does have an asterix in a weird place - after the sequence. Is this my error? let's change that * to "None" and see what happens.

```{r, eval = F}
awk '{gsub(/\*/,"None",$11)}1' /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam > /mnt/gluster/amlinz/GEODES_mapping_concat/test.sam
```

Now I get an error that says "truncated file" right at that same line. There are only 18 lines with no Phred score out of 500,000+ , so I'll just remove these and address it later.

```{r, eval = F}
#Count phred scores with "*"
 awk '{print $11}' /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam | grep \* |wc -l
#Remove these lines
awk '{ if ( $11 != "\*" ) { print $0; } }' /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam > /mnt/gluster/amlinz/GEODES_mapping_concat/test.all.sam
```

Looks like that removed the right lines! I get an warning about the escape character, but I think that's what I want. Testing 03processing again.

It made a feature.out file! I'm just going to look up a couple "features" in the gff file to make sure I can use that as metadata.
```{r, eval = F}
gzip -d mapping_database.gff.gz
grep scaffold_01090 mapping_database.gff
grep YUCDRAFT_00363 mapping_database.gff
gzip mapping_database.gff
```

Looking good! Those both produce unique lines. I'm going to add the awk lines (including printing how many * lines were removed) and run both the merging and processing on all three files. (make sure to clean up dud files first)

I'm still getting this error occasionally in the merge step:
[sam_header_line_parse] expected '@XY', got [@BFFGGGGGGGGGGGDGGGGD>FGGCGGGGGGGGGGGGGGGGEGGGGGBGFGGGDGGGGGGGEAGGGGGGGGGGEGCGGGFGGGGCGGGGGGBGGGGG:CGGGGGGGGGEGBEG<BGGEG/DDEGGGGGDD=EGEGD>GGGGGGGGG     4       *       0       0       *       *       0   0*       AS:i:0  XS:i:0]
Hint: The header tags must be tab-separated.
[samopen] no @SQ lines in the header.
[sam_read1] missing header? Abort!

Ran the merging and processing, and everything looks great! Now I need a script to combine all of the output files into a single table. I'm going to start an interactive session for testing this.

```{r, eval = F}
condor_submit -i testinteractive.sub

cp /mnt/gluster/amlinz/GEODES_mapping_summaries/* .

#Are all files the same length?
for file in *.out;do wc -l $file;done

#Yes - presumably the rownames are the same.
#Add a header of sample name to each of the files
for file in *.out;do sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - $file > temp && mv temp $file;done

#That worked!
#Now try join to combine

join GEODES002_nonrRNA.CDS.out GEODES001_nonrRNA.CDS.out 
# Worked, but gave me an warning about sorting and didn't keep the headers.

#Let's try paste instead:
paste -d' ' GEODES001_nonrRNA.CDS.out GEODES002_nonrRNA.CDS.out

#Keeps the headers but prints every rowname (with the header on the rowname)
#Can I 1. make a separeate file of rownames 2. remove rownames from the out files 3. add the header 4. paste everything together?

awk '{print $1}' GEODES001_nonrRNA.CDS.out > rownames.txt
for file in *.out;do awk '{print $2}' $file > temp.txt;sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - temp.txt > temp2.txt && mv temp2.txt $file;done
echo "/t" | cat - rownames.txt > temp3.txt && mv temp3.txt rownames.txt
files=$(echo *.out)
paste rownames.txt $files > GEODES_genes_all.txt

#Looking good! exit and download to file to make sure I can open it in Rstudio
exit
```

Write this up as a script and test again:
03maketable.sub:
```{r, eval = F}
# 03maketable.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03maketable_$(Cluster).log
error = 03maketable_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 03maketable.sh
#arguments = 
output = 03maketable_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files = 
transfer_output_files = GEODES_genes_2017-02-22.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 1GB
request_disk = 2GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue
```

03maketable.sh
```{r, eval = F}
#!/bin/bash
#Combine htseq-count results into one table
cp /mnt/gluster/amlinz/GEODES_mapping_summaries/* .

awk '{print $1}' GEODES001_nonrRNA.CDS.out > rownames.txt
for file in *.out;do awk '{print $2}' $file > temp.txt;sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - temp.txt > temp2.txt && mv temp2.txt $file;done
echo "\t" | cat - rownames.txt > temp3.txt && mv temp3.txt rownames.txt
files=$(echo *.out)
paste rownames.txt $files > GEODES_genes_2017-02-22.txt

rm *.out
rm rownames.txt
rm temp*.txt

```

Beautiful! Everything seems to be in working order. Time to put this aside for now and move over to the kraken classification workflow.

####2017-02-23

I got some weird errors about truncated files when trying to get a fastq file out of samtools, so I'm going back to all scripts, testing samtools in interactive mode, and then running. I'll do the same here, while also updating the "current workflow" section.

####2017-02-27

Ran the processing and didn't get any errors but about 10 samples produced empty output files. I'll come back and fix this if I have time before lab meeting. Moving on to making the metadata file now. 

####2017-04-11

It's been awhile for this step! I've got 10 files mapped accurately and am ready to revisit read counting. My issues last time were:
- need a better way to parse the gff file
- got errors because some SAM files had * as a Phred score
- some files were just empty

I'll start with the gff file. Can I find a tool for merging gff files? The tools at genometools.org/pub look good. I downloaded verstion 1.5.9. Uploading to CHTC.

WHY JGI WHY??? GFF formats are the bane of my existance right now. After getting genometools installed (not easy) I finally run merge and get the error: strand '-1' not one character long on line 1 in GEODES005.assembled.gff

Running head on my gff file piece indeed confirms that strand is indicated with -1 or 1. The standard gff format is - or +, hence the "one character" error. I'll need to change -1 to - and 1 to +, but only for that specific column. A job for awk maybe? Will also need to be conservative so as not to screw up my MAG gff files.

First I should right down how I ultimately installed genometools. This is in an interactive session.
```{bash, eval = F}
tar -xvzf genometools-1.5.9.tar.gz
cd genometools-1.5.9
make
make prefix=$(pwd)/../genometools/ install
cd ..
export PATH=$(pwd)/genometools/bin:$PATH

#Now I can run genometools
gt merge -retainids -tidy -o mapping_database.gff /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff

#This is what gave me that error

#Copy the gff files to this directory
cp /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff .

#Fix the strand issue
for file in *.gff; do awk -F'\t' -vOFS='\t' '{gsub("-1", "-", $7); gsub("1", "+", $7); print}' $file > f1_$file; done

#Try again
gt merge -retainids -tidy -o mapping_database.gff -force f1*.gff


```

error:gff file not sorted

####2017-04-12

Busy day of meetings but I'll be working on the gff issue in between. According to the genometools manual, the command gt gff3 will sort gff files if that's added as a parameter. It will also tidy them up here instead of at the merge step. Starting a new interactive session to test:

```{bash, eval = F}
tar -xvzf genometools-1.5.9.tar.gz
cd genometools-1.5.9
make cairo=no
make prefix=$(pwd)/../genometools/ cairo=no install
cd ..
export PATH=$(pwd)/genometools/bin:$PATH

#Copy the gff files to this directory
cp /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff .

#Fix the strand issue
for file in *.gff; do awk -F'\t' -vOFS='\t' '{gsub("-1", "-", $7); gsub("1", "+", $7); print}' $file > f1_$file; done

#Add first comment line
for file in f1*gff; do echo '##gff-version 3' | cat - $file > temp && mv temp $file; done
echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff

#Sort and tidy
for file in f1*gff; do gt gff3 -sort yes -tidy -force -retainids -o tidy_$file $file; done

#Run the merge
gt merge -o mapping_database.gff tidy*gff

gzip mapping_database.gff

cp mapping_database.gff.gz /mnt/gluster/amlinz/

rm *.gz
rm *.gff
rm -r genome*
rm temp
exit

```

I think that's working! Needed cairo=no, apparently not all nodes have cairo. Also needed to add a first comment line into the gff files. The tidy function added a gff header and corrected items to lowercase capitalization. After that the merge ran with no errors!
This will work for now, but unfortunately I think I will need a separate script for each "type" of gff file I want to include in my database due to formatting issues... yuck.

####2017-04-13

Next up, edit the htseq-couns script:
```{bash, eval = F}
#!/bin/bash
#Make a count table of mapped reads

#Transfer data from gluster
cp /mnt/gluster/amlinz/GEODES_mapping_results/$1.mapped.bam .
cp /mnt/gluster/amlinz/python.tar.gz .
cp /mnt/gluster/amlinz/mapping_database.gff.gz .

#Unzip programs
tar xzf python.tar.gz
tar xvf samtools.tar.gz
gzip -d mapping_database.gff.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$PATH
export HOME=$(pwd)/home

#Manipulate the output
samtools sort -o $1.sorted $1.mapped.bam
samtools index $1.sorted.bam

#Count reads
python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t CDS -i locus_tag -m intersection-strict -o $1.CDS.sam $1.sorted.bam mapping_database.gff > $1.CDS.out

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.CDS.out /mnt/gluster/amlinz/GEODES_mapping_summaries/
rm $1.CDS.out

#Clean up
rm -r python
rm -r samtools
rm *.bam
rm *.bai
rm mapping_database*
rm *.tar.gz
```

Make a list of files to run:
```{bash, eval = F}
for file in /mnt/gluster/amlinz/GEODES_mapping_results/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > bamfiles.txt
cat bamfiles.txt

```

And the submit file:
```{bash, eval = F}

# 03processing.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03processing_$(Cluster).log
error = 03processing_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 03processing.sh
arguments = $(samplename)
output = 03processing_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/samtools.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 3GB
request_disk = 12GB
#
# Tell HTCondor to run every file in the provided list:
queue samplename from bamfiles.txt

```

Looks like CHTC is still down for repairs from last night. Will try again later today.

####2017-04-17

I did get the script started, and it ran for a few hours before running out of RAM. So I bumped the RAM up to 12GB, and it ran for 2.5 days, and then ran out of RAM. Clearly that's not feasible. I think I need to split up the input BAM files, run them individually, and then compile the results of HTseq count afterwards.

Here's my first attempt at that script. After looking online, splitting by line should be sufficient.

05splitbams.sh
```{bash, eval = F}
#!/bin/bash
#Split bam files into smaller pieces based on line count
tar xvf samtools.tar.gz

cp /mnt/gluster/amlinz/GEODES_mapping_results/$1.mapped.bam .

samtools view $1.mapped.bam > $1.sam
split -l 1000000 $1.sam part_$1

# Rename files to include .sam extension
for file in part_$1*;do mv $file $file.sam;done

# Convert to bam and add the header back in
for file in part_$1*.sam; do name=$(basename $file | cut -d'.' -f1);  samtools reheader $1.sam $file > $name.sam; samtools view -b $name.sam > $name.bam; done

# Send files back to gluster
cp part_$1*.bam /mnt/gluster/amlinz/GEODES_split_bams/

# Clean up
rm -r $1
rm *sam
rm *bam
rm -r samtools
rm samtools.tar.gz

```

Run this in interactive mode to test

SO MANY PROBLEMS. The internet lies. I can't reheader because I have no EOF lines and I can't get the EOF lines by converting to BAM because I have no header. At this point, I think it would be easier to install another program.  I'm looking at NGSutils bamutils.

Downloaded to my submit node via git, then intalling in interactive mode here: http://ngsutils.org/installation/

####2017-04-18

Couldn't install ngsutils yesterday - it needs Python, and I don't think my custom Python install had what it need. It kept giving an error about not finding a bin file during make and when I tried to run bamutils, I got an error that pysam was not installed even though I can call pysam just fine in a python session.

Josh didn't have any more ideas for splitting bam files, so I'm going to try featureCounts instead of HTSeq. This one is supposed to be much faster, and I'm not seeing anything saying it's more error prone. Some say that it is more liberal in calling reads than HTSeq but that HTSeq may be too conservative overall.

I'm going to build a custom install of featureCounts in an interactive session. I installed it from source code and am using the Linux instructions here: http://bioinf.wehi.edu.au/subread-package/

```{bash, eval = F}
tar zxvf subread-1.5.2-source.tar.gz
cd subread-1.5.2-source/src
make -f Makefile.Linux
tar czvf subreads.tar.gz subread-1.5.2-source
cp subreads.tar.gz /mnt/gluster/amlinz
#Move to zipped/ back in home directory
```

New counting script - lines just commented out for now
03featurecounts.sh
```{bash, eval = F}

#!/bin/bash
#Make a count table of mapped reads

#Transfer data from gluster
cp /mnt/gluster/amlinz/GEODES_mapping_results/$1.mapped.bam .
#cp /mnt/gluster/amlinz/python.tar.gz .
cp /mnt/gluster/amlinz/mapping_database.gff.gz .

#Unzip programs
#tar xzf python.tar.gz
#tar xvf samtools.tar.gz
tar xvzf subreads.tar.gz
gzip -d mapping_database.gff.gz

#Update the path variable
#mkdir home
#export PATH=$(pwd)/python/bin:$PATH
#export HOME=$(pwd)/home

#Manipulate the output
#samtools sort -O bam $1.mapped.bam > $1.sorted.bam
#samtools index $1.sorted.bam

#Count reads
#python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t CDS -i locus_tag -m intersection-strict -o $1.CDS.sam $1.sorted.bam mapping_database.gff > $1.CDS.out
./subread-1.5.2-source/bin/featureCounts -t CDS -g locus_tag -a mapping_database.gff -o $1.CDS.txt $1.mapped.bam


#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.CDS.txt /mnt/gluster/amlinz/GEODES_mapping_summaries/
cp $1.CDS.txt.summary /mnt/gluster/amlinz/GEODES_mapping_summaries/
rm $1.CDS.txt


#Clean up
#rm -r python
#rm -r samtools
rm *.bam
#rm *.bai
rm mapping_database.gff
rm *.tar.gz
rm -r subread-1.5.2-source
```

03featurecounts.sub
```{bash, eval = F}
# 03featurecounts.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03featurecounts_$(Cluster).log
error = 03featurecounts_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/03featurecounts.sh
arguments = $(samplename)
output = 03featurecounts_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/subreads.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 20GB
request_disk = 12GB
#
# Tell HTCondor to run every file in the provided list:
queue samplename from bamfiles.txt

```

Test on one first


