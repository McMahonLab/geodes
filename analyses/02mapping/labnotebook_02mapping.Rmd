# Mapping

####Goal of this analysis

Who's active in our samples? What genes are being expressed? We could try to classify and annotate each read in the metatranscriptomes, but that wouldn't be very accurate. Instead we're going to map the reads to a database of freshwater genomes to get the classification and  annotation of every read we can.

##### Approach

The input files are the output from the workflow in 01rRNA_removal. In that workflow, I took quality filter reads produced by the JGI and ran them through sortmerna to get only nonrRNA reads to map. The database I'm using is genomes assembled from metagenomes (MAGs) and single amplified genomes (SAGs) from the same lakes as my metatranscriptomes. It's been curated by both the JGI and our lab, and contains additional genomes generated from the metagenomes and single cell preservations we collected during the GEODES field campaign. We're hoping that we'll get the best mapping possible by using genomes from the same study sites. We're also mapping to an internal standard that we added during extraction to approximate the absolute read counts in our metatranscriptomes. I'm going to perform this mapping competitively using the program bwa, then summarize the results using samtools and htseq.

## Most recent workflow. 
####Use this if you want to replicate our protocol. Updated 2017-04-25

So far, we've removed rRNA reads from the metatranscriptomes (see ../01rRNA_removal/). What I need to do next is map those reads to our database of reference genomes. The issue is that the database is enormous. Splitting the files doesn't work as we want to perform competitive mapping (report best hit from entire database only). To solve this issue, I'm building the database index once, storing it in gluster, and then referencing all other mapping jobs to this index. 

1. First things first, build that database. Right now I'm just using the metagenome assemblies, but soon I hope to add the reference MAGs and SAGs from previous projects and the new SAGs we're sequencing.

Right now the reference files I'm using are located in /mnt/gluster/amlinz/metagenome_assemblies/.

Set up system and then run the build index script :
```{bash, eval = F}
mkdir /mnt/gluster/amlinz/GEODES_mapping_results/
condor_submit submits/04build_index.sub

#Check for errors
ls -ltr 04*.err

#Check that the output is in gluster
ls -lh /mnt/gluster/amlinz
```


Building the mapping database and its index. Download BBMap here: https://sourceforge.net/projects/bbmap/

BBMap doesn't have a complicated install - just unzip it and go. Note that I'm using the flags -Xmx30g (ups Java's RAM limit from 20 to 30) and -usemodulo=T (increases speed at the cost of some sensitivity). FYI, this is a really slow program. Mine took about 2 days to complete.

04build_index.sh:
```{bash, eval = F}
#!/bin/bash
#Build a re-usable mapping index
cat /mnt/gluster/amlinz/metagenome_assemblies/fastas/*.fna > mapping_database.fna

tar -xvzf BBMap_36.99.tar.gz

#Unzip bbmap and build the index
bbmap/bbmap.sh ref=mapping_database.fna usemodulo=T -Xmx30g

# make ref/ a tarball and move to gluster
tar czvf ref.tar.gz ref/

cp ref.tar.gz /mnt/gluster/amlinz/
rm ref.tar.gz
gzip mapping_database.fna
cp mapping_database.fna.gz /mnt/gluster/amlinz
rm mapping_database.fna.gz

```

04build_index.sub:
```{bash, eval = F}
# 04build_index.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04build_index_$(Cluster).log
error = 04build_index_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/04build_index.sh
output = 04build_index_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 30GB
request_disk = 60GB
#
#
queue
```

2. Run the mapping step. I'll need samtools to convert from SAM to BAM format (BAM is compressed and better for storage), so first order of business is to build the samtools installation tarball in an interactive session.

install_samtools.sub:
```{bash, eval = F}

#install_samtools.sub
#
universe = vanilla
# Name the log file:
log = interactive.log

# Name the files where standard output and error should be saved:
output = process.out
error = process.err

# If you wish to compile code, you'll need the below lines.
#  Otherwise, LEAVE THEM OUT if you just want to interactively test!
+IsBuildJob = true
requirements = (OpSysAndVer =?= "SL6") && ( IsBuildSlot == true )

# Indicate all files that need to go into the interactive job session,
#  including any tar files that you prepared:
transfer_input_files = zipped/samtools-1.3.1.tar.bz2

# It's still important to request enough computing resources. The below
#  values are a good starting point, but consider your file sizes for an
#  estimate of "disk" and use any other information you might have
#  for "memory" and/or "cpus".
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 8GB
request_disk = 10GB

queue

```


```{bash, eval = F}
condor_submit -i install_samtools.sub
# Wait for job to start
tar xvfj samtools-1.3.1.tar.bz2
cd samtools-1.3.1
make
make prefix=../samtools install
cd ..
ls samtools
#I've got a nice bin file in there now!
tar czvf samtools.tar.gz samtools/
ls
exit

#Move samtools to the zipped/ folder
mv samtools.tar.gz zipped/samtools.tar.gz
```

Make a list of files to run, then start the jobs, then check the output.

```{bash, eval = F}
ls /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ > path2mappingfastqs.txt
condor_submit submits/05mapping.sub

ls -ltr /mnt/gluster/amlinz/GEODES_mapping_results
ls -ltr 05*err
```

05mapping.sh:
```{bash, eval = F}

#!/bin/bash
#Map metatranscriptome reads to my pre-indexed database of reference genomes
#Transfer metaT from gluster
#Not splitting the metaTs anymore
cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/$1 .
cp /mnt/gluster/amlinz/ref.tar.gz .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar -xvf samtools.tar.gz
tar -xvzf ref.tar.gz
gzip -d $1
name=$(basename $1 | cut -d'.' -f1)
sed -i '/^$/d' $name.fastq

#Run the mapping step
bbmap/bbmap.sh in=$name.fastq out=$name.mapped.sam minid=0.8 sam=1.3 threads=1 build=1 usemodulo=T -Xmx30g

# I want to store the output as bam. Use samtools to convert.
samtools view -b -S -o $name.mapped.bam $name.mapped.sam

#Copy bam file back to gluster
cp $name.mapped.bam /mnt/gluster/amlinz/GEODES_mapping_results/

#Clean up
rm -r bbmap
rm -r ref
rm *.bam
rm *.sam
rm *.fastq
rm *.gz

```

05mapping.sub:
```{bash, eval = F}

# 04mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04mapping_$(Cluster).log
error = 04mapping_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/04mapping.sh
arguments = $(samplename)
output = 04mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz,zipped/samtools.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory =30GB
request_disk = 25GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from path2mappingfastqs.txt


```

3. Clean up.

On my computer:
Open up WinSCP and log into submit-3.chtc.wisc.edu. Download the most recent versions of the scripts, programs, and text files

On submit-3.chtc.wisc.edu:

```{bash, eval = F}
# Delete all the .log, .out, and .err files in your home directory
rm *.err
rm *.log
rm *.out
```

Congratulations! You now have BAM files containing the best mapping hit for every metatranscriptomic read we have

#Lazy Run

As in the previous step's lazy run workflow, THIS IS FOR AFTER YOU'VE DONE THE FULL WORKFLOW ONCE AND KNOW EVERYTHING WORKS. NOT FOR ACTUALLY BEING LAZY.

```{bash, eval = F}
mkdir /mnt/gluster/amlinz/GEODES_mapping_results/
condor_submit submits/04build_index.sub

condor_submit -i install_samtools.sub
# Wait for job to start
tar xvfj samtools-1.3.1.tar.bz2
cd samtools-1.3.1
make
make prefix=../samtools install
cd ..
ls samtools
tar czvf samtools.tar.gz samtools/
ls
exit
mv samtools.tar.gz zipped/samtools.tar.gz

ls /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ > path2mappingfastqs.txt
condor_submit submits/05mapping.sub

rm *.err
rm *.log
rm *.out
```


#Lab Notebook

####2017-02-13

There's quite a few programs involved in this step. I'm using:

- bwa (Burrows-Wheeler Aligner, v0.7.12), https://sourceforge.net/projects/bio-bwa/files/ 
- samtools (v1.3.1), https://sourceforge.net/projects/samtools/files/
- htseq (v0.6.1), http://www-huber.embl.de/users/anders/HTSeq/doc/install.html#install
- python (gzipped source tarball v2.7.13), https://www.python.org/downloads/release/python-2713/

The general plan is to:
- copy in the nonrRNA file of interest
- run bwa
- convert the output from .sam to .bam and index
- count reads by feature with htseq
- calculate RPKM and summarize results with a python script
- save files back to gluster

I'm basing this on Josh's script from OMD-TOIL found here https://github.com/alexlinz/OMD-TOILv2/tree/master/scripts/10_mapping/10c_competitive

But the installation on CHTC is going to be a little tricky. First I need to build my own python tarball in interactive mode. I'm following the instructions here http://chtc.cs.wisc.edu/python-jobs.shtml

First I need to start an interactive CHTC session.
interactive.sub:
```{bash, eval = F}
universe = vanilla
# Name the log file:
log = interactive.log

# Name the files where standard output and error should be saved:
output = process.out
error = process.err

# If you wish to compile code, you'll need the below lines. 
#  Otherwise, LEAVE THEM OUT if you just want to interactively test!
+IsBuildJob = true
requirements = (OpSysAndVer =?= "SL6") && ( IsBuildSlot == true )

# Indicate all files that need to go into the interactive job session,
#  including any tar files that you prepared:
transfer_input_files = Python-2.7.13.tgz

# It's still important to request enough computing resources. The below 
#  values are a good starting point, but consider your file sizes for an
#  estimate of "disk" and use any other information you might have
#  for "memory" and/or "cpus".
request_cpus = 1
request_memory = 1GB
request_disk = 1GB

queue
```

Start the session with:
```{bash, eval = F}
condor_submit -i interactive.sub
```

Here's what I'm typing in the interactive session:
```{bash, eval = F}
mkdir python
tar -xvf Python-2.7.13.tgz
cd Python-2.7.13
./configure --prefix=$(pwd)/../python
make
make install
cd ..
ls python
ls python/bin
# Maybe I can install htseq right in here?
export PATH=$(pwd)/python/bin:$PATH
wget https://bootstrap.pypa.io/get-pip.py
python get-pip.py
pip install numpy
pip install matplotlib
pip install htseq
pip install pysam
# hey, that worked!
tar -czvf python.tar.gz python/
exit

```

Wow, I feel like a real bioinformatician! +1 to CHTC for awesome tutorials. Now I have a tarball that I can transfer around with my jobs that includes both python and HTseq.

CHTC is closing down my submit node for maintenance tomorrow, so I'm going to stop here for now. I'll possibly do some testing on Zissou and will download my installation packages for that, but I'm not sure how two different versions of python will agree on Zissou.


####2017-02-15

We're back up and running! Today I'm going to write a script to map one metatranscriptome. I'm going to transfer all of the installs over, but examine the output and slowly add lines to the bash script as I go.

First, get sample names and shorten to one metatranscriptome.

```{bash, eval = F}
for file in /mnt/gluster/amlinz/GEODES_nonrRNA_concat/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > samplenames.txt
head -1 samplenames.txt > test_samplenames.txt; mv test_samplenames.txt samplenames.txt
```

Here's my first stab at the submit file 02mapping.sub:
```{bash, eval = F}
# 02mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02mapping_$(Cluster).log
error = 02mapping_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 02mapping.sh
arguments = $(samplename)
output = 02mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = python.tar.gz,bwa.tar.gz,samtools.tar.gz, mapping_database.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from samplenames.txt
```

I also need to build my reference genome database. Which means I need to go find and upload the internal standard fasta and gff. It also looks like the size is pretty small (< 1 GB) so I'll zip up this database after building and send it via the submit file

Here's my database building:
```{bash, eval = F}
cat /mnt/gluster/amlinz/ref_genomes/fasta_files/*.fna > mapping_database.fna
cat /mnt/gluster/amlinz/ref_genomes/gff_files/*.gff > mapping_database.gff
tar -czvf mapping_database.tar.gz mapping_database.fna mapping_database.gff
rm mapping_database.gff
rm mapping_database.fna
```

Cool! I'm going to retroactively add that to the submit file above. Now for the executable. I'm going to be testing unzipping things so I know what to add to my path variable outside of this code.

Gah. Looks like bwa and samtools also need to be installed via interactive session.

####2017-02-16

Yep, more interactive installs time. Let's do this for bwa first. I'm modifying my interactive.sub to tranfer bwa instead of python.

Here's what I'm typing to install. Following instructions from the top of the README.md file in the bwa tarball, since I can't find installation instructions anywhere on their website...
```{bash, eval = F}
condor_submit -i interactive.sub
# Wait for job to start
tar xvfj bwa-0.7.12.tar.bz2
cd bwa-0.7.12
make
cd ..
ls bwa-0.7.12
# bwa executable is in there!
tar czvf bwa.tar.gz bwa-0.7.12/
ls
exit

```

Now do the same thing for samtools. Installation instructions here: http://www.htslib.org/download/
I'm modifying my interactive.sub to transfer the samtools tarball this time.

```{bash, eval = F}
condor_submit -i interactive.sub
# Wait for job to start
tar xvfj samtools-1.3.1.tar.bz2
cd samtools-1.3.1
make
make prefix=../samtools install
cd ..
ls samtools
#I've got a nice bin file in there now!
tar czvf samtools.tar.gz samtools/
ls
exit

```

Time to make my first executable. This is the homerun attempt -  I'm expecting lots and lots of errors.

02mapping.sh:
```{bash, eval = F}
##!/bin/bash
#Map metatranscriptome reads to my database of reference genomes

#Unzip programs
tar xzf python.tar.gz
tar xvf bwa.tar.gz
tar xvf samtools.tar.gz
tar xvf mapping_database.gz

#Transfer metaT from gluster
cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/$1.fastq.gz
tar xzf $1.fastq.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$(pwd)/bwa:$PATH
export HOME=$(pwd)/home

#Index the reference database
bwa index mapping_database.fna

#Run the mapping step - using 5 processors
bwa mem -t 5 mapping_database $1.fastq > $1.sam

#Manipulate the output
samtools view -b -S -o $1.bam $1.sam
samtools sort -o $1.sorted.bam -O bam -T /temp/$1 $1.bam
samtools index $1.sorted.bam
rm $1.sam
rm $1.bam

#Count reads
htseq-count -f bam -r pos -s no -a 0 -t feature -i locus_tag -m intersection-strict -o $1.feature.sam $1.sorted.bam mapping_database.gff > $1.feature.out

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.feature.out /mnt/gluster/amlinz/GEODES_mapping_results/
rm $1.feature.out

#Clean up
rm -rf python
rm -rf bwa
rm -rf samtools
rm $1.fastq
rm *.sam
rm *.bam
rm mapping_database*
rm *.tar.gz
  
```

and the submit file:
02mapping.sub
```{bash, eval = F}
# 02mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02mapping_$(Cluster).log
error = 02mapping_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 02mapping.sh
arguments = $(samplename)
output = 02mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = python.tar.gz,bwa.tar.gz,samtools.tar.gz, mapping_database.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 5
request_memory = 5GB
request_disk = 8GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from samplenames.txt
```

Here goes nothing! I'm going to list errors here and revise the scripts below.

1. Job held. Events 007 and 012 - condor_shadow rejected my job (Exec format error), and the job was placed in holding. Google suggests a problem with line endings - will use dos2unix on both the sub and sh and try again.
2. Still didn't work. The other suggested issue could be with the !/bin/bash statement. Oh look, I've got an extra #. Removing and trying again. THAT WORKED!
3. Several errors with a couple main causes. One, can't find GEODES001_nonrRNA.fastq.gz. Two, can't find bwa in the path. Three, can't find python. Four, can't find mapping_database.gz. Yikes. Solution 1, added ./ to the cp command so it knows where to put that. Solution 2, call bwa directly instead of via PATH. Solution 4, fix the typo in mapping_database.gz to mapping_database.tar.gz. Solution 3, add the requirements line to the sub script from CHTC's python instructions. Sidenote 1, the python tarball is > 50MB so transferring from gluster instead of via the submit file. Sidenote 2, increasing my disk usage space to 10GB, reducing memory to 3 GB and processors to 3. Still, only 12 computers meet these requirements, so I may need to split the job up into several steps to cut down on disk space.
4. "This does not look like a tar archive" and "cannot find GEODES001_nonrRNA.fastq.gz". Changing tar extraction to gzip extraction. Still can't find python -adding "python" before I call htseq-count.
5. Progress! the bwa index worked, but then bwa mem couldn't find the index. Python got found this time but then couldn't find htseq-count. But hey, at least things are running? And the diskspace is more reasonable - dropping down to 6. I'll try calling the database with the .fna extension, and calling htseq-count directly.
6. Well, that took an hour and a half but made it to the samtools step. Error was "failed to open file /temp/GEODES001_nonrRNA.0000.bam". I'll take the temp folder designation out. I'm not sure what that's doing anyway.Also going to set samtools to use three threads, and going to save a copy of the mapping output to my home folder so I can rerun from after the mapping step next time. BTW, only 27 computers match my current job.
7. Ooh, that's interesting. Error occured due to failure to parse line 18699 of mapping_database.gff, specifically in the attribute column. Investigating with:
```{bash, eval = F}
sed -n '18699{p;q}' mapping_database.gff
```
Only thing different is that it has parantheses and a semicolon

####2017-02-20

I tried the code to below to remove the semicolon, since that's also a column separator
```{bash, eval = F}
sed -i 's|(IF-2; GTPase)|(IF-2 GTPase)|g' mapping_database.gff
```

But that didn't work. Still get the error at line 18699. I'm going to try downloading the gff file (gzipped) and running it through an online validator http://genometools.org/cgi-bin/gff3validator.cgi . 

Well it doesn't like comment lines. Let's remove those.
```{bash, eval = F}
sed -i '/^#/d' mapping_database.gff
```

Ah. Apparently the first line needs to start with ##gff-version 3. Let's add that back in.

```{bash, eval = F}
echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff
```

Now I get a parse error on line 22835. Looks like a CRISPR? Let's remove that line. Actually let's remove all lines that say CRISPR in them.

```{bash, eval = F}
sed '/CRISPR/d' mapping_database.gff > temp && mv temp mapping_database.gff
```

Oh good. Now I get the original error at line 18679. Josh's github says according to the GFFv3 standard, semi-colons have a reserved meaning and must be escaped. The standard recommends using the URL escape %3B. How can I replace only product names with semicolons? It has a space - can I use that?

```{bash, eval = F}
sed -i 's|; |%3B |g' mapping_database.gff
```

Technically it was validated, but got 40,000+ warnings about missing "##sequence-region" lines. I'm going to try coming back to this later, but will run as is for now since Josh didn't seem to run into this problem.

It read the gff file! Warning was "no features of type 'feature'" produced. Odd. Now it says "no module named pysam found" Looks like I need to go back to the python installation and add that module. Going now.

Got that module added to the install and tried again. It's taking forever, is that a good sign?
2 hours later - still not done. It was on one of the spalding servers I've been having trouble with, so restarted... and it finished five minutes later. Darn spalding. And whatever that feature.out output is, it's not what I want to use for RPKM counts. Although the mapped to unmapped ratio is nice.

__no_feature    898225
__ambiguous     0
__too_low_aQual 0
__not_aligned   5279253
__alignment_not_unique  0

What do I save to find out which genes/genomes it mapped to? Or are there supposed to be features in this file? I'm going to rerun and keep all the output this time.

Actually the raw bwa output looks like what I need. Can I process this in R? It's kind of large, but I could write my own script instead of the samtools/htseq combo to get exactly what I need out. Can I use samtools to parse down the size of that file?

Let's back up the train. Here's the results I need out of the mapping:

- How many reads are mapped vs unmapped
- What genome and gene (name please) did each read map to? (could be condensed to a table of counts per gene/genome)
- A fasta file of the unmapped read sequences

What exactly is in this output sam file from bwa? According to their manual:

- metaT read name
- flag about the mapping (includes unmapped)
- name of reference sequence it hit
- position of sequence
- mapping quality in Phred
- CIGAR?
- mate reference (for paired only)
- mate position
- inferred insert size
- which strant
- quailty of query seq in Phred
- optional fields

Really I only need the read name, did it map or not, and what did it hit. But the bwa file is way too big. I'm running again using samtool idxstats, which should give me a tab-delimited file of reference sequence name, sequence length, # mapped reads and # unmapped reads.

Next run technically worked, but produced all of the output to the screen and follows the format: 

TH03520DRAFT_TH03520_TBL_comb47_HYPODRAFT_10000124.2    95289   0       0
TH03520DRAFT_TH03520_TBL_comb47_HYPODRAFT_10000199.3    76284   8       0
TH03520DRAFT_TH03520_TBL_comb47_HYPODRAFT_10000209.4    75264   1       0
TH03520DRAFT_TH03520_TBL_comb47_HYPODRAFT_10000247.5    71296   0       0

So I assume that's genome/contig name, length of contig, number of reads and number of unmapped reads? Not so useful. HOw to I get the genes?


####2017-02-21

I did some reading on HTseq, and it should be able to count reads that hit genes. At the very least, it could at least open my gff file and show me if there's something wrong with the features. I'm going start an interactive session sending over python and samtools, and try to follow the instructions here: http://www-huber.embl.de/users/anders/HTSeq/doc/tour.html#tour

Here's what I'm typing in the interactive session:
```{bash, eval = F}
condor_submit -i interactive.sub
#Wait
cp /mnt/gluster/amlinz/GEODES001_nonrRNA.sam .
tar xzf python.tar.gz
tar xvf samtools.tar.gz
tar xvf mapping_database.tar.gz

samtools view -b -S -o GEODES001_nonrRNA.bam GEODES001_nonrRNA.sam
#Had some trouble with the command below - command flags have changed since Josh wrote his script. This is the correct format. Could this be why I have no features at the end?
samtools sort GEODES001_nonrRNA.bam -o GEODES001_nonrRNA.sorted.bam
samtools index GEODES001_nonrRNA.sorted.bam
samtools idxstats GEODES001_nonrRNA.sorted.bam
#That command works, but still goes by contig rather than by gene.
# Update the python path

mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

#Check the original htseq count command to see if fixing the sort line results in feature:
python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t CDS -i locus_tag -m intersection-strict -o GEODES001_nonrRNA.feature.sam GEODES001_nonrRNA.sorted.bam mapping_database.gff > GEODES001_nonrRNA.feature.out

# I think CDS and locus tag should work! I'll need to paste each feature.out as a column in a dataframe for DESeq, and I'll use the gff file as a metadata file
# How can I get a fast file of unmapped reads?
samtools view -f 4 GEODES001_nonrRNA.sam > GEODES001_nonrRNA.unaligned.sam 
samtools view -b -S -o GEODES001_nonrRNA.unaligned.bam GEODES001_nonrRNA.unaligned.sam
samtools bam2fq -nO GEODES001_nonrRNA.unaligned.bam > GEODES001_nonrRNA.unaligned.fastq 


```

That should do it! I've got unique identifiers and counts of reads that mapped, and I'll just need a metadata file to process those. I've also got a fastq file of unmapped reads, which should work for kraken. I've also realized that since both of these steps stem from the bwa output, I should split this into three separate scripts. This will cut down on space needed and hopefully speed up run time.  I could probably split up the input files for mapping, too, then put them back together with samtools merge.

Keeping the scripts below, but going to the current workflow section for this final pass! Should be pretty simple - I'll modify splitfastqs from the rRNA removal step, then run just the mapping on this step, then put it together. The processing and kraken results will be in a different folder.

02mapping.sh:
```{bash, eval = F}
#!/bin/bash
#Map metatranscriptome reads to my database of reference genomes

#Transfer metaT from gluster
cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/$1.fastq.gz .
cp /mnt/gluster/amlinz/python.tar.gz .
gzip -d $1.fastq.gz

#Unzip programs
tar xzf python.tar.gz
tar xvf bwa.tar.gz
tar xvf samtools.tar.gz
tar xvf mapping_database.tar.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

#Index the reference database
./bwa-0.7.12/bwa index mapping_database.fna

#Run the mapping step - using 5 processors
./bwa-0.7.12/bwa mem -t 3 mapping_database.fna $1.fastq > $1.sam
cp $1.sam /mnt/gluster/amlinz/

#Manipulate the output
samtools view -b -S -o $1.bam $1.sam
samtools sort -o $1.sorted.bam -O bam -T $1 -@ 3 $1.bam
samtools index $1.sorted.bam
rm $1.sam
rm $1.bam

#Count reads
python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t feature -i locus_tag -m intersection-strict -o $1.feature.sam $1.sorted.bam mapping_database.gff > $1.feature.out

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.feature.out /mnt/gluster/amlinz/GEODES_mapping_results/
rm $1.feature.out

#Clean up
rm -rf python
rm -rf bwa
rm -rf samtools
rm $1.fastq
rm *.sam
rm *.bam
rm mapping_database*
rm *.tar.gz
  
```

02mapping.sub
```{bash, eval = F}
# 02mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02mapping_$(Cluster).log
error = 02mapping_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 02mapping.sh
arguments = $(samplename)
output = 02mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = bwa.tar.gz,samtools.tar.gz, mapping_database.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 3
request_memory = 3GB
request_disk = 10GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from samplenames.txt
```


I've got everything copied up top, split fastqs modified, and a new script for merging at the end. I'll probably run that last one in interactive mode to check it first. Cleaning up my folder and leaving notes here as I test 3 metaTs.

Split fastqs started at 10:23, finished at 10:27. No errors. modified requests in submit file.
Mapping started at 10:39. At 11:13, most jobs were done but 13 were held. Why? Some jobs are going above the memory limit. Adjust and try again. Restarted at 11:17, finished at 12:09.

Did some futzing with samtools on the interactive sub and got something working. Started at 1:10, finished at 1:30, no errors. Looking good! I'll hold off running all of the mapping until I get the downsteam stuff working.

####2017-02-23

Coming back to the merging of files since I think I figured out my samtools issues. There was a major update in the past year, and this is the current documentation: http://www.htslib.org/doc/samtools.html . Josh is using the older version in his scripts.

I've rewritten the the samtools portion of merging and will try running it interactive mode first, then in script mode. Hopefully this will solve my issues with "truncated files" downstream.

After much fiddling and crashing an execute node once, I've got it working. On to redo-ing 03processing.

####2017-03-09

Welcome back! Since I last worked on the mapping step, I've written code for the downstream steps - counting the mapped reads, classifying the unmapped reads, and compiling/plotting the results in R. I finished my preliminary pass for lab meeting on the 6th and got some feedback on how to improve my workflow. 

There's a few things to change to generally improve the programming:

- Use a fastq splitting program instead of dividing total number of lines by 4. Sarah recommends getreads.sh from bbtools.
- Use a gff validation program instead of my awk/sed commands to get the mapping_database.gff file in the proper format.
- Switch bwa out for bbmap (strongly recommended by multiple lab members)
- Store output files as bam instead of sam (saves space and has more info)

There's also some scientific changes to make. I'm going to try several different versions of the mapping database on a subset of the data to determine what works best. This mean I need to a) define what makes one database better than another and b) come up with a way to run several mapping databases at once and compare the results.

I'm not quite sure how to approach that yet, so let's start with the easy stuff first - splitting fastq files. Before that, I should clean up my CHTC home folder and gluster account.

```{bash, eval = F}
#Make separate folders for sub and sh files
mkdir submits
mkdir executables
mv *.sub submits
mv *.sh executables
mv scripts/*.sub submits
mv scripts/*.sh executables
rm -r scripts

#Make a separate folder for programs and tarballs
mkdir zipped
mv *gz zipped
mv *bz2 zipped

#Make a folder for old output files that I want to save
mkdir output
rm temp.txt
rm test.txt
mv *.txt output
rm splitfastqsa*

#Now clean up in gluster. Remove intermediate kraken files
cd /mnt/gluster/amlinz/
rm -r GEODES_kraken_split
rm -r GEODES_kraken_results
rm -r GEODES_mapping_split/*
rm -r GEODES_mapping_results
```

I've downloaded BBMap here: https://sourceforge.net/projects/bbmap/?source=typ_redirect
The user guide is here:
http://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbmap-guide/


The file is BBMap_36.99.tar.gz. I've uploaded into to my "zipped" folder on CHTC. I'm going to start off with an interactive session to make sure the installation works and to figure out where the scripts I need are located.

```{bash, eval = F}
#Change my go-to interactive sub to tranfer BBMap
nano submits/testinteractive.sub

condor_submit -i submits/testinteractive.sub

#Wait for session to start

#Unzip to install
tar -xvzf BBMap_36.99.tar.gz
#Run the installation test from the user guide
bbmap/stats.sh in=bbmap/resources/phix174_ill.ref.fa.gz
#Works! Now where is my getreads.sh?
bbmap/getreads.sh
#The help file pops up, so that must be the location.

#Copy an input file in from gluster to test.

cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/GEODES001_nonrRNA.fastq.gz .

#How can I count how many sequences are in this file?
#Do my lines wrap?

# I got this code from biostars
zcat GEODES001_nonrRNA.fastq.gz | ruby -e'
skip_line = false
last_char = "+"
while gets
 if skip_line
  skip_line = false
  next
end
if $_ =~ /^@/ and last_char == "+"
 last_char = "@"
elsif $_ =~ /&@/ and last_char == "@"
 last_char = "+"
else
 STDERR.puts "WARNING: fastq lines wrap at line #{$.}"
 exit(1)
end
skip_line = true
end
STDERR.puts "fastq lines do not wrap"
'

#"fastq lines wrap at line 3"

#I wonder what happens if you give getreads.sh an output that's out of range.

bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=0-3 #prints output to screen nicely
wc -l GEODES001_nonrRNA.fastq
#24675059 lines, so no fewer than 6143765 reads (# if no wrapping)
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=7000000-7000003 out=out_of_range.fastq overwrite=T
#Produces an empty output file.
#Trying several ranges to try and find the end of the file

#Didn't find the exact end, but when you're out of it, you get an empty output file.

#Can I make a bunch of predefined files, then delete ones that are empty?

bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=0-999999 out=GEODES001_nonrRNA1.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1000000-1999999 out=GEODES001_nonrRNA2.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=2000000-2999999 out=GEODES001_nonrRNA3.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=3000000-3999999 out=GEODES001_nonrRNA4.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=4000000-4999999 out=GEODES001_nonrRNA5.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=5000000-5999999 out=GEODES001_nonrRNA6.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=6000000-6999999 out=GEODES001_nonrRNA7.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=7000000-7999999 out=GEODES001_nonrRNA8.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=8000000-8999999 out=GEODES001_nonrRNA9.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=9000000-9999999 out=GEODES001_nonrRNA10.fastq

#Error at line 10138059 in the second command - is this this end of file?
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=0-99999 out=GEODES001_nonrRNA1.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=100000-199999 out=GEODES001_nonrRNA2.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=200000-299999 out=GEODES001_nonrRNA3.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=300000-399999 out=GEODES001_nonrRNA4.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=400000-499999 out=GEODES001_nonrRNA5.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=500000-599999 out=GEODES001_nonrRNA6.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=600000-699999 out=GEODES001_nonrRNA7.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=700000-799999 out=GEODES001_nonrRNA8.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=800000-899999 out=GEODES001_nonrRNA9.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=900000-999999 out=GEODES001_nonrRNA10.fastq

bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1000000-1099999 out=GEODES001_nonrRNA11.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1100000-1199999 out=GEODES001_nonrRNA12.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1200000-1299999 out=GEODES001_nonrRNA13.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1300000-1399999 out=GEODES001_nonrRNA14.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1400000-1499999 out=GEODES001_nonrRNA15.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1500000-1599999 out=GEODES001_nonrRNA16.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1600000-1699999 out=GEODES001_nonrRNA17.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1700000-1799999 out=GEODES001_nonrRNA18.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1800000-1899999 out=GEODES001_nonrRNA19.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1900000-1999999 out=GEODES001_nonrRNA20.fastq

#Very weird - 11 is empty, 12 is normal, 13 gives the error. IDs 1300000 to 1399999?
#11 was missing a digit. My bad.
#Line 10138059 must be in ids 1300000-1399999

#Can I count unique line headers somehow? I know I can't use "@" because it's also a quality score, but maybe something longer?

grep @HISEQ GEODES001_nonrRNA.fastq | wc -l
#6168763 - that's right in my lines/4 range
grep + GEODES001_nonrRNA.fastq | wc -l
#Same number - but that doesn't match with where my error pops up.

#Let's look at that line specifically
sed '1{10138055;10138060}' GEODES001_nonrRNA.fastq

```

I looked at the code in getreads.sh, and it's using "@" to split sequences. THAT'S WHAT I WAS TRYING TO AVOID IN THE FIRST PLACE. It breaks on that particular line because there's a "@" in the quality sequence.

I wonder if I can manually change that in the script?
Nope. Still breaks. @ seems to have another meeting for java.

What about removing @ from the quality scores?

```{bash, eval = F}
 awk '!/HISEQ/{gsub(/@/, "?")}; 1' GEODES001_nonrRNA.fastq > mod.fastq

```

Nope, error at same line. Definitely does not have @.

What about converting to fasta and then splitting?
Would you believe it, there's the same error at the same line. WTF.
I'll come back to this later.

####2017-03-10

I've downloaded a fastq validator from here: http://genome.sph.umich.edu/wiki/FastQValidator
Running this in interactive mode on GEODES001 to see if it can diagnose the problem.

```{bash, eval = F}
#In interactive mode
tar -xvzf FastQValidatorLibStatGen.0.1.1a.tgz
cd fastQValidatorLibStatGen.0.1.1a
make all

cd ..
cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/GEODES001_nonrRNA.fastq.gz .
gzip -d GEODES001_nonrRNA.fastq.gz
./fastQValidator_0.1.1a/fastQValidator/bin/fastQValidator --file GEODES001_nonrRNA.fastq.gz

#FASTQ INVALID. Lots of repeated sequence identifiers, but not much I can do about that and I don't think that's causing the problem in one particular line. There are 6168721 sequences - a few less than I found with grep. I'd trust this program more. There are 2947696 errors. I'll turn off sequence ID matching and try again.
./fastQValidator_0.1.1a/fastQValidator/bin/fastQValidator --file GEODES001_nonrRNA.fastq.gz --disableSeqIDCheck

#Alright! Line 10138057, the sequence identifier lin was too short. Everything after that reads as an invalid character. What can I do about this
sed -n 10138057p GEODES001_nonrRNA.fastq

```

I've found the problem. Line 10138057 is a blank line. THANKS JGI. How do I remove blank lines from a file?

```{bash, eval = F}
sed '/^$/d' GEODES001_nonrRNA.fastq > mod.fastq
./fastQValidator_0.1.1a/fastQValidator/bin/fastQValidator --file mod.fastq --disableSeqIDCheck

#FASTQ SUCCESS. WOOHOO!
#Can I now accurately calculate the number of fastq seqs in my file from the number of lines? 
#Yes! 24675052/4 = 6168763, which is what the fastq validator reports
```

No errors reported in the fastq after I removed blank links, so let's try getreads.sh again.

In an interactive session:
```{bash, eval = F}
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/GEODES001_nonrRNA.fastq.gz .
gzip -d GEODES001_nonrRNA.fastq.gz

sed -i '/^$/d' GEODES001_nonrRNA.fastq

maxreads=$((`wc -l < GEODES001_nonrRNA.fastq` / 8 - 1))
startpoints=$(seq 0 500000 $maxreads)

#That's some funky math up there. The first command counts the number of lines, then divides by 4, giving 6168763. But each sequence is paired - when I give getreads.sh IDS from 0-100000, it reads 2000000 reads and quits between 3000000 and 4000000. So I want to divide this by 2 again, or in other words, divide the line count by 8 to get the max reads. Also, getreads.sh starts counting at 0, so I subtract by 1.
#The next line counts in increments from 500000 to the number of reads in the file. This should give me one million reads (paired) per file.

for num in $startpoints;
  do endpoint=$(($num + 499999));
  bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=$num-$endpoint out=GEODES001_nonrRNA_$endpoint.fastq overwrite=T;
  done

#That worked! I even have exactly 168762 reads left in the last file.
```

####2017-03-15

I went back to the rRNA removal step and rewrote the fastq splitting executable based on the work above. I reran everything, including the new samples that have arrived since February. Now I'll use that same script as a base here and then change my bwa mapping to bbmap.

I'm also going to change the numbering scheme of the scripts so that it's clear what sequence you run them in and there are no duplicates.
```{bash, eval = F}
for file in /mnt/gluster/amlinz/GEODES_nonrRNA_concat/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > samplenames.txt

head -3 samplenames.txt > test_samplenames.txt; mv test_samplenames.txt samplenames.txt
```


04splitfastqs.sub
```{bash, eval = F}

# 04split_fastq.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04split_fastq_$(Cluster).log
error = 04split_fastq_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/04split_fastq.sh
arguments = $(sample)
output = 04split_fastq_$(Cluster).out
#

should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2 GB
request_disk = 4 GB
#
# Just one job for testing
queue sample from samplenames.txt

```

04splitfastqs.sh
```{bash, eval = F}
#!/bin/bash
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/$1.fastq.gz .
gzip -d $1.fastq.gz

sed -i '/^$/d' $1_nonrRNA.fastq

maxreads=$((`wc -l < $1.fastq` / 8 - a))
startpoints=$(seq 0 500000 $maxreads)

for num in $startpoints;
  do endpoint=$(($num + 499999));
  bbmap/getreads.sh in=$1.fastq id=$num-$endpoint out=$1$endpoint.fastq overwrite=T;
  done

rm $1.fastq
gzip $1*
cp $1* /mnt/gluster/amlinz/GEODES_mapping_split
rm $1*
rm BBMap_36.99.tar.gz
rm -r bbmap


```

####2017-03-16

I started the scripts above and they ran for nearly 24 hours, producing no errors or output, until I killed it. It didn't take that long to run all 85 metatranscriptomes for the rRNA removal, and I'm using nearly the same code. I'll boot into interactive mode and see what's going on. I should at least see the getreads.sh output, so presumably it never made it to that step.

Think I found the problem - I left the .gz tag on the fastq file after I unzipped it. Removed that from the executable and restarted.

Actually the problem was that my variable $1 included the "_nonrRNA" tag and I included it in the script - so it was trying to process GEODES001_nonrRNA_nonrRNA.fastq.  Also there's something funky about variables and "_", will look into this further. Anyway, it's all working now.


####2017-03-17

Happy St. Patrick's Day! My goal for today is to get the bbmap mapping program working. I've already got the installation figured out for BBtools, so hopefully this should be straightforward. I'll run it in interactive mode first and record my commands below.

First, make the database. I'll use the ref MAGS SAGs for now, but switch to the metagenomes later.
```{bash, eval = F}
cat /mnt/gluster/amlinz/ref_genomes/fasta_files/*.fna > mapping_database.fna
cat /mnt/gluster/amlinz/ref_genomes/gff_files/*.gff > mapping_database.gff

#We've got to fix some formatting issues in the gff file first
#Remove comment lines
sed -i '/^#/d' mapping_database.gff
#Add back the first comment line
echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff
#Remove CRISPR arrays
sed '/CRISPR/d' mapping_database.gff > temp && mv temp mapping_database.gff
#Change semicolons in product names to a URL escape code
sed -i 's|; |%3B |g' mapping_database.gff
gzip mapping_database.gff 
gzip mapping_database.fna 

mv mapping_database* /mnt/gluster/amlinz/

```

```{bash, eval = F}

#!/bin/bash
#Map metatranscriptome reads to my database of reference genomes

#Transfer metaT from gluster
cp $1 .
name=$(basename $1 |cut -d'.' -f1)
cp /mnt/gluster/amlinz/mapping_database.fna.gz .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar xvf samtools.tar.gz
gzip -d $name.fastq.gz

#Index the reference database
bbmap/bbmap.sh ref=mapping_database.fna.gz

#Run the mapping step
#I've added a threads=1 parameter to keep the program from getting greedy and crashing a execute node. The minid parameter lets me set a minimum quality score for mapped reads. I'm also outputting the statistics produced (including %reads mapped) and % reads mapped to each scaffold.
bbmap/bbmap.sh in=$name.fastsq out=$name.mapped.sam minid=0.8 threads=1 scafstats=$name.scaffolds.txt statsfile=$name.stats.txt

# I want to store the output as bam. Use samtools to convert.
samtools view -b -S -o $name.bam $name.mapped.sam

#Copy bam file back to gluster
cp $name.bam /mnt/gluster/amlinz/GEODES_mapping_results/
#Clean up
rm -r bbmap
rm $name*
rm mapping_database.fna.gz
rm *.tar.gz

```

Before I get ahead of myself, let's switch over to the metagenome assemblies from GEODES instead of the ref MAGS SAGS.

This is going to take awhile. In the meantime, I'll write up the submit file and test to make sure the script works in parallel.

```{bash, eval = F}
# 04mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04mapping_$(Cluster).log
error = 04mapping_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/04mapping.sh
arguments = $(samplename)
output = 04mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz,zipped/samtools.tar.gz
transfer_output_files = *.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 5GB
request_disk = 4GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from path2splitmappingfastqs.txt

```

```{bash, eval = F}
find /mnt/gluster/amlinz/GEODES_mapping_results/ -type f > path2splitmappingfastqs.txt
```


Not bad! Produced the bam output in the right place. I get an error that rm *.sam doesn't work because no sam file was found - does converting it to bam destroy it? and the stats out files didn't get returned - can I add these to the submit file?

Note to self, using wildcards in the submit file results in your jobs being held. I'll transfer these to a new directory in gluster instead.

Works! This is running really fast, and I'm not sure how I would put the pieces back together again... can I run the mapping on non-split fastq files? TEst on file and time.

Hits an error on the first blank line. Removing with the sed command and trying again.

That took 50 minutes! Not bad. +1 for BBmap's speed. The stats look good, too - assuming I can run the downstream ht-seq on the full file, I think I'll do that.


####2017-03-18

I finished uploading the metagenome assemblies and gff files overnight. I'll build a database out of those and run the mapping on that to start.

```{bash, eval = F}
cat /mnt/gluster/amlinz/metagenome_assemblies/fastas/*.fna > mapping_database.fna
cat /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff > mapping_database.gff

#We've got to fix some formatting issues in the gff file first
#Remove comment lines
sed -i '/^#/d' mapping_database.gff
#Add back the first comment line
echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff
#Remove CRISPR arrays
sed '/CRISPR/d' mapping_database.gff > temp && mv temp mapping_database.gff
#Change semicolons in product names to a URL escape code
sed -i 's|; |%3B |g' mapping_database.gff
gzip mapping_database.gff 
gzip mapping_database.fna 

```

Looking good, except that there are no product names in the gff file. Looking in the IMG data folder I downloaded from JGI, this info is in the ".product_name" file. Why this is necessary is beyond me, there's a perfectly good place for product names in the gff file. But hey, do your own thing, JGI.

Anyway, for right now I just need to know what scaffold reads mapped to, which is output by BBmap. I'll write something to get the product name information later. Right now, I want to run BBMap using the metagenome assembly databases on the nonsplit nonrRNA fastq files.

```{bash, eval = F}
find /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ -type f > path2splitmappingfastqs.txt
 head -3 path2splitmappingfastqs.txt > temp.txt; mv temp.txt path2splitmappingfastqs.txt

```


Started at 11:11AM. We'll time that to see if it takes ridiculously long with multiple genomes + the giant database. I may need to up the disk space requirements.

####2017-03-20

Yep, it takes ridiculously long. Then gets evicted from the execute nodes for taking up too much disk space (12GB). I may need to split this up afterall. My options:

- Split up the fastq files again
- Add more threads to bbmap
- Use the nodisk option to prevent bbmap from writing anything but output
- Split up the mapping database by lake
- Remove scaffolds that are too short to do much with

I'll try removing small scaffolds first. I found some code online to do this with awk, but need to check line wrapping first.

Definitely wrapped. That sucks.I'll try removing newline characters first. From Stack Overflow:
```{bash, eval = F}
 sed -i ':a;N;/^>/M!s/\n//;ta;P;D' /mnt/gluster/amlinz/mapping_database.fna
```

While that's running, I'll work on getting CLaMs running. I downloaded both the UI and the command line. They run on Java, so supposedly by unzipping the installation package and clicking the .jar file, it might work if the configs are right... hey that worked!

I'm uploading the .fna file from the GEODES005 assembly in the GUI to test. I don't think it likes it very much, it's kind of frozen. Maybe I should run the length filtering here, too.

I crashed CLaMs. Definitely going to look into give it some length filtered sequences.

Ran:
```{bash, eval = F}
awk '!/^>/ { next } { getline seq } length(seq) >= 200 { print $0 "\n" seq }' GEODES005.assembled.fna > filtered.fna
```

Didn't reduce the file size at all. Not sure if that would help - but I still need to get the data into clams. For now I'll worry about the mapping, though.

Alright, let's try the nodisk option.

Update: that exceeded the memory requirements of BBmap itself. Oops. I'll try putting the database back to disk and using split fastq files instead.

####2017-03-21

Splitting the fastq files didn't help - I still ran out of memory. After talking to Sarah, I think I need to split up the mapping database. This is non-competitve mapping, and I want competive, so I'll do "semi" competitve mapping by parsing the output files for the best hit out of all of the database sections.

I was running CLams on just the GEODES005 assembly yesterday. The file loaded and was running, but showed no progress after several hours. I'll try again on the smaller input file. 

I'll use an interactive session and getreads.sh to split up the assembly files.

```{bash, eval = F}
condor_submit -i submits/testinteractive.sub
#Wait
#My old mapping database was 1/2 GB, so I'll aim for that size in my split database files.
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/metagenome_assemblies/fastas/GEODES005.assembled.fna .

#sed -i '/^$/d' $1.filter-MTF.fastq

#Since this is a wrapped fasta file, I'm going to calculate the maxreads differently
maxreads=$(grep -c "^>" GEODES005.assembled.fna)
startpoints=$(seq 0 1000000 $maxreads)

for num in $startpoints;
  do endpoint=$(($num + 999999));
  bbmap/getreads.sh in=GEODES005.assembled.fna id=$num-$endpoint out=GEODES005.assembled.$endpoint.fna overwrite=T;
  done

rm GEODES005.assembled.fna
gzip *.fna
mkdir /mnt/gluster/amlinz/metagenome_assemblies/split_files/
cp *fna.gz /mnt/gluster/amlinz/metagenome_assemblies/split_files/
rm *.fna.gz
rm BBMap_36.99.tar.gz
rm -r bbmap
exit
```

I'll test on just the one assembled metagenome for now, and add more when I have everythign working. 

Looks like to run pairwise combinations of metaTs and databases, I'll need a list of every combination. Borrowing some code I found online to do this in bash.

```{bash, eval = F}
find /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ -type f > path2splitmappingfastqs.txt
 head -3 path2splitmappingfastqs.txt > temp.txt; mv temp.txt path2splitmappingfastqs.txt

ls /mnt/gluster/amlinz/metagenome_assemblies/split_files/ > split_databases.txt

for a in $(awk '{print $1}' path2splitmappingfastqs.txt) 
do 
    for b in $(awk '{print $1}' split_databases.txt) 
    do 
        echo $a $b 
    done 
done > metaT_db_combo.txt
  
head -2 metaT_db_combo.txt > temp.txt; mv temp.txt metaT_db_combo.txt

```

Now my argument $1 has both the query and database. I'll need to modify my executable to split this into two variables.

```{bash, eval = F}

#!/bin/bash
#Map metatranscriptome reads to my database of reference genomes
path=$(echo $1 | cut -d' ' -f1)
db=$(echo $1 | cut -d' ' -f2)
#Transfer metaT from gluster
cp $path .
name=$(basename $path |cut -d'.' -f1)
cp /mnt/gluster/amlinz/metagenome_assemblies/split_files/$db .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar xvf samtools.tar.gz
gzip -d $name.fastq.gz

#Index the reference database
bbmap/bbmap.sh ref=$db

#Run the mapping step
#I've added a threads=1 parameter to keep the program from getting greedy and crashing a execute node. The minid parameter lets me set a minimum quality score for mapped reads. I'm also outputting the statistics produced (including %reads mapped) and % reads mapped to each scaffold.
bbmap/bbmap.sh in=$name.fastq out=$name.mapped.sam minid=0.8 threads=1 scafstats=$name.scaffolds.txt statsfile=$name.stats.txt

# I want to store the output as bam. Use samtools to convert.
samtools view -b -S -o $name.bam $name.mapped.sam

#Copy bam file back to gluster
cp $name.bam /mnt/gluster/amlinz/GEODES_mapping_results/
#Clean up
rm -r bbmap
rm $name*
rm $db
rm *.tar.gz

```

Trying on two files to start. 

I had some issues with the variable naming - apparently the sh automatically recongizes the space as meaning there are two variables. Script changed to reflect that. After that it works, except for not copying the output stats files over to gluster. I'll fix that and then run on 3 metaTs and 4 db parts. I'll use those output files to write a script to keep only the best hits from all database parts.

####2017-03-22

3 of the files never finished - I suspect these are the first chunks of the database, and that they're too big. Next time I'll chop into smaller pieces. In the meantime, I'll use the output I have to write a parsing script. I think I'll do this in Python using HTseq. Will run in interactive mode first to start figure out how to code python...

```{bash, eval = F}
cp /mnt/gluster/amlinz/GEODES_mapping_results/GEODES071*.bam .
tar xvf samtools.tar.gz
tar xvf python.tar.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

for file in *.bam;do samtools sort $file sorted.$file;done

#samtools merge -n -u GEODES071_all.bam sorted.*.bam
#header will be wrong on the one below, not sure if this will be an issue or not
samtools cat -o GEODES071_all.bam sorted.*.bam

python
import sys,os,HTSeq

bam_file = HTSeq.BAM_Reader( "GEODES071_all.bam" )
bundles = HTSeq.bundle.mulitple.alignmnets(bam_file)

exit
```

The samtools bits take FOREVER and then printing bundles gave an error about an unknown CIGAR string "=". Google says this is because HTseq doesn't support SAM v1.4, only SAM v1.3. I can reformat the sam files with bbtools, or set my mapping to output v1.3 instead of 1.4. I need to redo my mapping with smaller database sizes anyway, so I'll do that option.

Here's the full commands to do this:

```{bash, eval = F}
condor_submit -i submits/testinteractive.sub
#Wait
#My old mapping database was 1/2 GB, so I'll aim for that size in my split database files.
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/metagenome_assemblies/fastas/GEODES005.assembled.fna .


#Since this is a wrapped fasta file, I'm going to calculate the maxreads differently
maxreads=$(grep -c "^>" GEODES005.assembled.fna)
startpoints=$(seq 0 250000 $maxreads)

for num in $startpoints;
  do endpoint=$(($num + 249999));
  bbmap/getreads.sh in=GEODES005.assembled.fna id=$num-$endpoint out=GEODES005.assembled.$endpoint.fna overwrite=T;
  done

rm GEODES005.assembled.fna
gzip *.fna
mkdir /mnt/gluster/amlinz/metagenome_assemblies/split_files/
cp *fna.gz /mnt/gluster/amlinz/metagenome_assemblies/split_files/
rm *.fna.gz
rm BBMap_36.99.tar.gz
rm -r bbmap
exit

find /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ -type f > path2splitmappingfastqs.txt
 head -3 path2splitmappingfastqs.txt > temp.txt; mv temp.txt path2splitmappingfastqs.txt

ls /mnt/gluster/amlinz/metagenome_assemblies/split_files/ > split_databases.txt

for a in $(awk '{print $1}' path2splitmappingfastqs.txt) 
do 
    for b in $(awk '{print $1}' split_databases.txt) 
    do 
        echo $a $b 
    done 
done > metaT_db_combo.txt
  
```

04mapping.sh
```{bash, eval = F}

#!/bin/bash
#Map metatranscriptome reads to my database of reference genomes
#Transfer metaT from gluster
cp $1 .
name=$(basename $1 |cut -d'.' -f1)
cp /mnt/gluster/amlinz/metagenome_assemblies/split_files/$2 .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar xvf samtools.tar.gz
gzip -d $name.fastq.gz
sed -i '/^$/d' $name.fastq

#Index the reference database
bbmap/bbmap.sh ref=$2

#Run the mapping step
bbmap/bbmap.sh in=$name.fastq out=$name.$2.mapped.sam minid=0.8 sam=1.3 threads=1 scafstats=$name.$2.scaffolds.txt statsfile=$name.$2.stats.txt

# I want to store the output as bam. Use samtools to convert.
samtools view -b -S -o $name.$2.bam $name.$2.mapped.sam

#Copy bam file back to gluster
cp $name.$2.bam /mnt/gluster/amlinz/GEODES_mapping_results/
cp *.txt /mnt/gluster/amlinz/GEODES_mapping_stats/

#Clean up
rm -r bbmap
rm *.bam
rm *.sam
rm *.fastq
rm *.gz


```

39 jobs submitted with the new, shorter database file.

####2017-03-27

Since last week, I've been chatting with people about how best to do these tasks. For BBMap, Trina put me in touch with its developer, Brian Bushnell. He hasn't responded to my email so I assume he doesn't have the answers. I could contact CHTC help, but even my split files from last week are taking between 6 - 10 hours to run (just BBMap, not the samtools portion). If I could get access to more RAM, it would take even longer, so that's not a great option. I still think the piecemeal version is the way to go.

For ClaMS, I've got it working on these smaller database chunks, but it's still quite slow. Really it's the same issue as BBMap - both run in Java, and have RAM limits which I far exceed. Also has issues with the NCBI database in that I have to specify a level to classify to ahead of time. Other recommendations I got from the lab were sourmash, LCA, and protein homology via blast. sourmash is a cool new matching algorithm, but I would need to write the rest of the classifier myself. LCA is supposedly fast and easy to use. A combination of LCA and protein homology would probably be the most accurate. I will discuss this with Trina on Wednesday.

In the meantime, I'll work on that python script to put the mapping results together. I ran the mapping with SAM1.3 so HTSeq should be able to read the bam files now. In interactive mode:

```{bash, eval = F}
cp /mnt/gluster/amlinz/GEODES_mapping_results/GEODES071*.bam .
tar xvf samtools.tar.gz
tar xvf python.tar.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

for file in *.bam;do samtools sort -o sorted.$file $file ;done

#samtools merge -n -u GEODES071_all.bam sorted.*.bam
#header will be wrong on the one below, not sure if this will be an issue or not
samtools cat -o GEODES071_all.bam sorted.*.bam

python
import sys,os,HTSeq

bam_file = HTSeq.BAM_Reader( "GEODES071_all.bam" )


exit


samtools view -f 4 -o GEODES071_all.sam GEODES071_all.bam


```

Still having problems with htseq - I don't understand how this BAM reader works, and I'm not seeing any examples online remotely close to what I want to do. Very tempted to write this in R, but the files are still huge. The full bam file for one sample is 18 GB, and I killed its sam file at 62 GB and it wasn't done writing yet. To cut down on this file size, I'm running the mapping again with mappedonly=t. This will save only mapped reads and not output a line for every read that didn't map. If I want to know how many didn't map, I can take the num entries in the sam file / num lines in the fasta file. 
I also told it to stop outputting the stats files. Doesn't really make sense now that I"m splitting everything.

####2017-03-30

The saga of mapping methods continues. I met with CHTC yesterday and they think that I CAN map without splitting if I just make the index once and store it in gluster. There are enough giant computers to do this, and they're willing to waive my time limits if necessary. I'll start this process by running the indexing on my small database, just to get the file names right.

```{bash, eval = F}
#Make the small database
cat /mnt/gluster/amlinz/ref_genomes/fasta_files/*.fna > mapping_database.fna
gzip mapping_database.fna 
cp mapping_database.fna.gz /mnt/gluster/amlinz/

#Test in interactive mode
condor_submit -i submits/testinteractive.sub

#In interactive mode
cp /mnt/gluster/amlinz/mapping_database.fna.gz .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz

#Index the reference database
bbmap/bbmap.sh ref=mapping_database.fna.gz

#clean up
rm mapping_database.fna.gz
rm -r ref
rm BBMap_36.99.tar.gz
rm -r bbmap

exit


```

Indexing creates a directory called "ref". I'm guessing that the mapping step looks for this directory and uses whatever's in it. Time to start the big index.

04build_index.sh:
```{bash, eval = F}
#!/bin/bash
#Build a re-usable mapping index
cat /mnt/gluster/amlinz/metagenome_assemblies/fastas/*.fna > mapping_database.fna
cat /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff > mapping_database.gff

#We've got to fix some formatting issues in the gff file
#Remove comment lines
sed -i '/^#/d' mapping_database.gff
#Add back the first comment line
echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff
#Remove CRISPR arrays
sed '/CRISPR/d' mapping_database.gff > temp && mv temp mapping_database.gff
#Change semicolons in product names to a URL escape code
sed -i 's|; |%3B |g' mapping_database.gff

#Remove short contigs in the mapping database fna file
awk '!/^>/ { next } { getline seq } length(seq) >= 1000 { print $0 "\n" seq }' mapping_database.fna > mapping_database1000.fna

gzip mapping_database.gff 
gzip mapping_database1000.fna

#No longer need the gff file or the unfiltered fna file - move to gluster
cp mapping_database.gff.gz /mnt/gluster/amlinz/
rm mapping_database.gff.gz
rm mapping_database.fna

#Unzip bbmap and build the index
tar -xvzf BBMap_36.99.tar.gz
bbmap/bbmap.sh ref=mapping_database1000.fna.gz

#make ref/ a tarball and move to gluster
tar czvf ref.tar.gz ref/

cp ref.tar.gz /mnt/gluster/amlinz/
rm ref.tar.gz
rm mapping_database1000.fna.gz
```

04build_index.sub:
```{bash, eval = F}
# 04build_index.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04build_index_$(Cluster).log
error = 04build_index_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/04build_index.sh
output = 04build_index_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz
#transfer_output_files = 
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 15GB
request_disk = 60GB
#
#
queue 

```

Here goes nothing...

After building the database for 4 hours, bbmap reports the reference is empty... I suspect the read trimming command. Will try a bbtools command instead.

```{bash, eval = F}
#!/bin/bash
#Build a re-usable mapping index
#cat /mnt/gluster/amlinz/metagenome_assemblies/fastas/*.fna > mapping_database.fna
#cat /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff > mapping_database.gff

#We've got to fix some formatting issues in the gff file
#Remove comment lines
#sed -i '/^#/d' mapping_database.gff
#Add back the first comment line
#echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff
#Remove CRISPR arrays
#sed '/CRISPR/d' mapping_database.gff > temp && mv temp mapping_database.gff
#Change semicolons in product names to a URL escape code
#sed -i 's|; |%3B |g' mapping_database.gff

cp /mnt/gluster/amlinz/mapping_database.fna.gz .
tar -xvzf BBMap_36.99.tar.gz
#Remove short contigs in the mapping database fna file
bbmap/reformat.sh in=mapping_database.fna.gz out=mapping_database1000.fna minlength=1000

#gzip mapping_database.gff 
gzip mapping_database1000.fna

#No longer need the gff file or the unfiltered fna file - move to gluster
#cp mapping_database.gff.gz /mnt/gluster/amlinz/
#rm mapping_database.gff.gz
rm mapping_database.fna

#Unzip bbmap and build the index
bbmap/bbmap.sh ref=mapping_database1000.fna.gz

#make ref/ a tarball and move to gluster
tar czvf ref.tar.gz ref/

cp ref.tar.gz /mnt/gluster/amlinz/
rm ref.tar.gz
rm mapping_database1000.fna.gz
```

####2017-04-03

Finished running on Saturday! Now I will modify my mapping script to refer to the database in gluster.

```{bash, eval = F}

#!/bin/bash
#Map metatranscriptome reads to my pre-indexed database of reference genomes
#Transfer metaT from gluster
#Not splitting the metaTs anymore
cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/$1 .
cp /mnt/gluster/amlinz/ref.tar.gz .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar -xvf samtools.tar.gz
tar -xvzf ref.tar.gz
gzip -d $1
name=$(basename $1 |cut -d'.' -f1)
sed -i '/^$/d' $name.fastq

#Run the mapping step
bbmap/bbmap.sh in=$name.fastq out=$name.mapped.sam minid=0.8 sam=1.3 threads=1 mappedonly=t

# I want to store the output as bam. Use samtools to convert.
samtools view -b -S -o $name.mapped.bam $name.mapped.sam

#Copy bam file back to gluster
cp $name.mapped.bam /mnt/gluster/amlinz/GEODES_mapping_results/

#Clean up
rm -r bbmap
rm -r ref
rm *.bam
rm *.sam
rm *.fastq
rm *.gz
```

Make a short list of metaTs to test

```{bash, eval = F}
ls /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ > path2mappingfastqs.txt
head -3 path2mappingfastqs.txt > temp.txt; mv temp.txt path2mappingfastqs.txt
```
 
Modify the submit file
 
```{bash, eval = F}

# 04mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04mapping_$(Cluster).log
error = 04mapping_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/04mapping.sh
arguments = $(samplename)
output = 04mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz,zipped/samtools.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory =15GB
request_disk = 60GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from path2mappingfastqs.txt


```

Error: can't find ref/1/summary.txt. Will go into interactive mode to test.
Forgot the current dir in the copy ref command, and had the $name variable wrong. Running again.

####2017-04-04
Ran for awhile then got booted off for exceeding RAM usage. After reading, it looks like I need to specify where the index is with build=1. If I specify no index, bbmap starts building a new one in RAM from the info in ref/. In theory, the build parameter says go into indexed folders ref/genome/1 and ref/index/1 and use that. 


Still no dice. Closer inspection showed that the index building is aborting early? Emailed Brian to find out why.

####2017-04-05

Brian says the indexing step ran out of memory, particularly the Java bit. I think I'm going to need to run this in low memory mode. Brian says that should take about 30GB RAM - Ill change the submit file to this amount, add the-Xmx flag so Java uses this amount, and turn on the lowmemory mode with usemodulo. 

Well that finished oddly quickly and doesn't seem to have aborted... Starting the mapping again to test.

Update as of 6:15PM: first job ran out of memory. Will up it tomorrow.

####2017-04-06

Well, good thing I left the other two jobs running - they ran just fine! One finished at 10PM and the other finished at 12:30AM. Sizes are both around 350MB, so not bad at all.

So how much memory did that first program try and use? What was the difference? GEODES001 is slightly bigger than GEODES002 and GEODES003, but not by much and there are certainly much bigger metatranscriptomes. 

Job usage: 21GB disk, 17GB RAM. 20 GB disk, 20GB RAM (limit). 20 GB disk, 20GB RAM (limit)

I'll try upping to 30GB RAM and running more jobs to see if it's only the really big files that are failing. If it is, I'll split up the fastq files and either run them each as a job or run them sequentially in the same script.

Also dropping disk space to 25, since 20 seems to be all that is needed.
```{bash, eval = F}
ls /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ > path2mappingfastqs.txt
head -10 path2mappingfastqs.txt > temp.txt; mv temp.txt path2mappingfastqs.txt
```

####2017-04-10

Success! Everything finished with no errors and the bam files look good. I did some poking around in the index and it looks like everything is there - same number of sequences as in the input fasta, so that's a good sign. I think I'm confident in saying the mapping worked. The last thing I want to do is write something to quickly check the .err files to make sure they worked,so that I don't have to go through them one by one.

```{bash, eval = F}
for file in 04mapping*.err; do test=$(grep "SAM header is present:" $file | wc -l); echo "$file $test"; done
```

I'm going to step away from mapping for a bit to get the HTSEQ count bit working with this output. I also need to figure out how to classify the contigs. Then I'll come back here, clean up the current workflow, and run the mapping on all files.

