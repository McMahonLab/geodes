# Mapping

####Goal of this analysis

Who's active in our samples? What genes are being expressed? We could try to classify and annotate each read in the metatranscriptomes, but that wouldn't be very accurate. Instead we're going to map the reads to a database of freshwater genomes to get the classification and  annotation of every read we can.

##### Approach

The input files are the output from the workflow in 01rRNA_removal. In that workflow, I took quality filter reads produced by the JGI and ran them through sortmerna to get only nonrRNA reads to map. The database I'm using is genomes assembled from metagenomes (MAGs) and single amplified genomes (SAGs) from the same lakes as my metatranscriptomes. It's been curated by both the JGI and our lab, and contains additional genomes generated from the metagenomes and single cell preservations we collected during the GEODES field campaign. We're hoping that we'll get the best mapping possible by using genomes from the same study sites. We're also mapping to an internal standard that we added during extraction to approximate the absolute read counts in our metatranscriptomes. I'm going to perform this mapping competitively using the program bwa, then summarize the results using samtools and htseq.

## Most recent workflow. 
####Use this if you want to replicate our protocol. Updated 2017-06-16

So far, we've removed rRNA reads from the metatranscriptomes (see ../01rRNA_removal/). What I need to do next is map those reads to our database of reference genomes. The issue is that the database is enormous. Splitting the files doesn't work as we want to perform competitive mapping (report best hit from entire database only). To solve this issue, I'm building the database index once, storing it in gluster, and then referencing all other mapping jobs to this index. 

1. First things first, build that database. Right now I'm just using the metagenome assemblies, but soon I hope to add the reference MAGs and SAGs from previous projects and the new SAGs we're sequencing.

Right now the reference files I'm using are located in /mnt/gluster/amlinz/metagenome_assemblies/.

Set up system and then run the build index script :
```{bash, eval = F}
mkdir /mnt/gluster/amlinz/GEODES_mapping_results/
condor_submit submits/04build_index.sub

#Check for errors
ls -ltr 04*.err

#Check that the output is in gluster
ls -lh /mnt/gluster/amlinz
```


Building the mapping database and its index. Download BBMap here: https://sourceforge.net/projects/bbmap/

BBMap doesn't have a complicated install - just unzip it and go. Note that I'm using the flags -Xmx30g (ups Java's RAM limit from 20 to 30) and -usemodulo=T (increases speed at the cost of some sensitivity). FYI, this is a really slow program. Mine took about 2 days to complete.

04build_index.sh:
```{bash, eval = F}
#!/bin/bash
#Build a re-usable mapping index
cat /mnt/gluster/amlinz/metagenome_assemblies/fastas/*.fna > mapping_database.fna

tar -xvzf BBMap_36.99.tar.gz

#Unzip bbmap and build the index
bbmap/bbmap.sh ref=mapping_database.fna usemodulo=T -Xmx30g

# make ref/ a tarball and move to gluster
tar czvf ref.tar.gz ref/

cp ref.tar.gz /mnt/gluster/amlinz/
rm ref.tar.gz
gzip mapping_database.fna
cp mapping_database.fna.gz /mnt/gluster/amlinz
rm mapping_database.fna.gz

```

04build_index.sub:
```{bash, eval = F}
# 04build_index.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04build_index_$(Cluster).log
error = 04build_index_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/04build_index.sh
output = 04build_index_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 30GB
request_disk = 60GB
#
#
queue
```

2. Run the mapping step. I'll need samtools to convert from SAM to BAM format (BAM is compressed and better for storage), so first order of business is to build the samtools installation tarball in an interactive session.

install_samtools.sub:
```{bash, eval = F}

#install_samtools.sub
#
universe = vanilla
# Name the log file:
log = interactive.log

# Name the files where standard output and error should be saved:
output = process.out
error = process.err

# If you wish to compile code, you'll need the below lines.
#  Otherwise, LEAVE THEM OUT if you just want to interactively test!
+IsBuildJob = true
requirements = (OpSysAndVer =?= "SL6") && ( IsBuildSlot == true )

# Indicate all files that need to go into the interactive job session,
#  including any tar files that you prepared:
transfer_input_files = zipped/samtools-1.3.1.tar.bz2

# It's still important to request enough computing resources. The below
#  values are a good starting point, but consider your file sizes for an
#  estimate of "disk" and use any other information you might have
#  for "memory" and/or "cpus".
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 8GB
request_disk = 10GB

queue

```


```{bash, eval = F}
condor_submit -i install_samtools.sub
# Wait for job to start
tar xvfj samtools-1.3.1.tar.bz2
cd samtools-1.3.1
make
make prefix=../samtools install
cd ..
ls samtools
#I've got a nice bin file in there now!
tar czvf samtools.tar.gz samtools/
ls
exit

#Move samtools to the zipped/ folder
mv samtools.tar.gz zipped/samtools.tar.gz
```

Make a list of files to run, then start the jobs, then check the output.

```{bash, eval = F}
ls /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ > path2mappingfastqs.txt
condor_submit submits/05mapping.sub

ls -ltr /mnt/gluster/amlinz/GEODES_mapping_results
ls -ltr 05*err
```

05mapping.sh:
```{bash, eval = F}

#!/bin/bash
#Map metatranscriptome reads to my pre-indexed database of reference genomes
#Transfer metaT from gluster
#Not splitting the metaTs anymore
cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/$1 .
cp /mnt/gluster/amlinz/ref.tar.gz .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar -xvf samtools.tar.gz
tar -xvzf ref.tar.gz
gzip -d $1
name=$(basename $1 | cut -d'.' -f1)
sed -i '/^$/d' $name.fastq

#Run the mapping step
bbmap/bbmap.sh in=$name.fastq out=$name.mapped.sam minid=0.8 trd=T sam=1.3 threads=1 build=1 usemodulo=T mappedonly=T -Xmx30g

# I want to store the output as bam. Use samtools to convert.
samtools view -b -S -o $name.mapped.bam $name.mapped.sam

#Copy bam file back to gluster
cp $name.mapped.bam /mnt/gluster/amlinz/GEODES_mapping_results/

#Clean up
rm -r bbmap
rm -r ref
rm *.bam
rm *.sam
rm *.fastq
rm *.gz


```

05mapping.sub:
```{bash, eval = F}

# 05mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 05mapping_$(Cluster).log
error = 05mapping_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/05mapping.sh
arguments = $(samplename)
output = 05mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz,zipped/samtools.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory =30GB
request_disk = 25GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from path2mappingfastqs.txt


```

3. Clean up.

On my computer:
Open up WinSCP and log into submit-3.chtc.wisc.edu. Download the most recent versions of the scripts, programs, and text files

On submit-3.chtc.wisc.edu:

```{bash, eval = F}
# Delete all the .log, .out, and .err files in your home directory
rm *.err
rm *.log
rm *.out
```

Congratulations! You now have BAM files containing the best mapping hit for every metatranscriptomic read we have

#Lazy Run

As in the previous step's lazy run workflow, THIS IS FOR AFTER YOU'VE DONE THE FULL WORKFLOW ONCE AND KNOW EVERYTHING WORKS. NOT FOR ACTUALLY BEING LAZY.

```{bash, eval = F}
mkdir /mnt/gluster/amlinz/GEODES_mapping_results/
condor_submit submits/04build_index.sub

condor_submit -i install_samtools.sub
# Wait for job to start
tar xvfj samtools-1.3.1.tar.bz2
cd samtools-1.3.1
make
make prefix=../samtools install
cd ..
ls samtools
tar czvf samtools.tar.gz samtools/
ls
exit
mv samtools.tar.gz zipped/samtools.tar.gz

ls /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ > path2mappingfastqs.txt
condor_submit submits/05mapping.sub

rm *.err
rm *.log
rm *.out
```


#Lab Notebook

####2017-02-13

There's quite a few programs involved in this step. I'm using:

- bwa (Burrows-Wheeler Aligner, v0.7.12), https://sourceforge.net/projects/bio-bwa/files/ 
- samtools (v1.3.1), https://sourceforge.net/projects/samtools/files/
- htseq (v0.6.1), http://www-huber.embl.de/users/anders/HTSeq/doc/install.html#install
- python (gzipped source tarball v2.7.13), https://www.python.org/downloads/release/python-2713/

The general plan is to:
- copy in the nonrRNA file of interest
- run bwa
- convert the output from .sam to .bam and index
- count reads by feature with htseq
- calculate RPKM and summarize results with a python script
- save files back to gluster

I'm basing this on Josh's script from OMD-TOIL found here https://github.com/alexlinz/OMD-TOILv2/tree/master/scripts/10_mapping/10c_competitive

But the installation on CHTC is going to be a little tricky. First I need to build my own python tarball in interactive mode. I'm following the instructions here http://chtc.cs.wisc.edu/python-jobs.shtml

First I need to start an interactive CHTC session.
interactive.sub:
```{bash, eval = F}
universe = vanilla
# Name the log file:
log = interactive.log

# Name the files where standard output and error should be saved:
output = process.out
error = process.err

# If you wish to compile code, you'll need the below lines. 
#  Otherwise, LEAVE THEM OUT if you just want to interactively test!
+IsBuildJob = true
requirements = (OpSysAndVer =?= "SL6") && ( IsBuildSlot == true )

# Indicate all files that need to go into the interactive job session,
#  including any tar files that you prepared:
transfer_input_files = Python-2.7.13.tgz

# It's still important to request enough computing resources. The below 
#  values are a good starting point, but consider your file sizes for an
#  estimate of "disk" and use any other information you might have
#  for "memory" and/or "cpus".
request_cpus = 1
request_memory = 1GB
request_disk = 1GB

queue
```

Start the session with:
```{bash, eval = F}
condor_submit -i interactive.sub
```

Here's what I'm typing in the interactive session:
```{bash, eval = F}
mkdir python
tar -xvf Python-2.7.13.tgz
cd Python-2.7.13
./configure --prefix=$(pwd)/../python
make
make install
cd ..
ls python
ls python/bin
# Maybe I can install htseq right in here?
export PATH=$(pwd)/python/bin:$PATH
wget https://bootstrap.pypa.io/get-pip.py
python get-pip.py
pip install numpy
pip install matplotlib
pip install htseq
pip install pysam
# hey, that worked!
tar -czvf python.tar.gz python/
exit

```

Wow, I feel like a real bioinformatician! +1 to CHTC for awesome tutorials. Now I have a tarball that I can transfer around with my jobs that includes both python and HTseq.

CHTC is closing down my submit node for maintenance tomorrow, so I'm going to stop here for now. I'll possibly do some testing on Zissou and will download my installation packages for that, but I'm not sure how two different versions of python will agree on Zissou.


####2017-02-15

We're back up and running! Today I'm going to write a script to map one metatranscriptome. I'm going to transfer all of the installs over, but examine the output and slowly add lines to the bash script as I go.

First, get sample names and shorten to one metatranscriptome.

```{bash, eval = F}
for file in /mnt/gluster/amlinz/GEODES_nonrRNA_concat/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > samplenames.txt
head -1 samplenames.txt > test_samplenames.txt; mv test_samplenames.txt samplenames.txt
```

Here's my first stab at the submit file 02mapping.sub:
```{bash, eval = F}
# 02mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02mapping_$(Cluster).log
error = 02mapping_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 02mapping.sh
arguments = $(samplename)
output = 02mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = python.tar.gz,bwa.tar.gz,samtools.tar.gz, mapping_database.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from samplenames.txt
```

I also need to build my reference genome database. Which means I need to go find and upload the internal standard fasta and gff. It also looks like the size is pretty small (< 1 GB) so I'll zip up this database after building and send it via the submit file

Here's my database building:
```{bash, eval = F}
cat /mnt/gluster/amlinz/ref_genomes/fasta_files/*.fna > mapping_database.fna
cat /mnt/gluster/amlinz/ref_genomes/gff_files/*.gff > mapping_database.gff
tar -czvf mapping_database.tar.gz mapping_database.fna mapping_database.gff
rm mapping_database.gff
rm mapping_database.fna
```

Cool! I'm going to retroactively add that to the submit file above. Now for the executable. I'm going to be testing unzipping things so I know what to add to my path variable outside of this code.

Gah. Looks like bwa and samtools also need to be installed via interactive session.

####2017-02-16

Yep, more interactive installs time. Let's do this for bwa first. I'm modifying my interactive.sub to tranfer bwa instead of python.

Here's what I'm typing to install. Following instructions from the top of the README.md file in the bwa tarball, since I can't find installation instructions anywhere on their website...
```{bash, eval = F}
condor_submit -i interactive.sub
# Wait for job to start
tar xvfj bwa-0.7.12.tar.bz2
cd bwa-0.7.12
make
cd ..
ls bwa-0.7.12
# bwa executable is in there!
tar czvf bwa.tar.gz bwa-0.7.12/
ls
exit

```

Now do the same thing for samtools. Installation instructions here: http://www.htslib.org/download/
I'm modifying my interactive.sub to transfer the samtools tarball this time.

```{bash, eval = F}
condor_submit -i interactive.sub
# Wait for job to start
tar xvfj samtools-1.3.1.tar.bz2
cd samtools-1.3.1
make
make prefix=../samtools install
cd ..
ls samtools
#I've got a nice bin file in there now!
tar czvf samtools.tar.gz samtools/
ls
exit

```

Time to make my first executable. This is the homerun attempt -  I'm expecting lots and lots of errors.

02mapping.sh:
```{bash, eval = F}
##!/bin/bash
#Map metatranscriptome reads to my database of reference genomes

#Unzip programs
tar xzf python.tar.gz
tar xvf bwa.tar.gz
tar xvf samtools.tar.gz
tar xvf mapping_database.gz

#Transfer metaT from gluster
cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/$1.fastq.gz
tar xzf $1.fastq.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$(pwd)/bwa:$PATH
export HOME=$(pwd)/home

#Index the reference database
bwa index mapping_database.fna

#Run the mapping step - using 5 processors
bwa mem -t 5 mapping_database $1.fastq > $1.sam

#Manipulate the output
samtools view -b -S -o $1.bam $1.sam
samtools sort -o $1.sorted.bam -O bam -T /temp/$1 $1.bam
samtools index $1.sorted.bam
rm $1.sam
rm $1.bam

#Count reads
htseq-count -f bam -r pos -s no -a 0 -t feature -i locus_tag -m intersection-strict -o $1.feature.sam $1.sorted.bam mapping_database.gff > $1.feature.out

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.feature.out /mnt/gluster/amlinz/GEODES_mapping_results/
rm $1.feature.out

#Clean up
rm -rf python
rm -rf bwa
rm -rf samtools
rm $1.fastq
rm *.sam
rm *.bam
rm mapping_database*
rm *.tar.gz
  
```

and the submit file:
02mapping.sub
```{bash, eval = F}
# 02mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02mapping_$(Cluster).log
error = 02mapping_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 02mapping.sh
arguments = $(samplename)
output = 02mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = python.tar.gz,bwa.tar.gz,samtools.tar.gz, mapping_database.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 5
request_memory = 5GB
request_disk = 8GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from samplenames.txt
```

Here goes nothing! I'm going to list errors here and revise the scripts below.

1. Job held. Events 007 and 012 - condor_shadow rejected my job (Exec format error), and the job was placed in holding. Google suggests a problem with line endings - will use dos2unix on both the sub and sh and try again.
2. Still didn't work. The other suggested issue could be with the !/bin/bash statement. Oh look, I've got an extra #. Removing and trying again. THAT WORKED!
3. Several errors with a couple main causes. One, can't find GEODES001_nonrRNA.fastq.gz. Two, can't find bwa in the path. Three, can't find python. Four, can't find mapping_database.gz. Yikes. Solution 1, added ./ to the cp command so it knows where to put that. Solution 2, call bwa directly instead of via PATH. Solution 4, fix the typo in mapping_database.gz to mapping_database.tar.gz. Solution 3, add the requirements line to the sub script from CHTC's python instructions. Sidenote 1, the python tarball is > 50MB so transferring from gluster instead of via the submit file. Sidenote 2, increasing my disk usage space to 10GB, reducing memory to 3 GB and processors to 3. Still, only 12 computers meet these requirements, so I may need to split the job up into several steps to cut down on disk space.
4. "This does not look like a tar archive" and "cannot find GEODES001_nonrRNA.fastq.gz". Changing tar extraction to gzip extraction. Still can't find python -adding "python" before I call htseq-count.
5. Progress! the bwa index worked, but then bwa mem couldn't find the index. Python got found this time but then couldn't find htseq-count. But hey, at least things are running? And the diskspace is more reasonable - dropping down to 6. I'll try calling the database with the .fna extension, and calling htseq-count directly.
6. Well, that took an hour and a half but made it to the samtools step. Error was "failed to open file /temp/GEODES001_nonrRNA.0000.bam". I'll take the temp folder designation out. I'm not sure what that's doing anyway.Also going to set samtools to use three threads, and going to save a copy of the mapping output to my home folder so I can rerun from after the mapping step next time. BTW, only 27 computers match my current job.
7. Ooh, that's interesting. Error occured due to failure to parse line 18699 of mapping_database.gff, specifically in the attribute column. Investigating with:
```{bash, eval = F}
sed -n '18699{p;q}' mapping_database.gff
```
Only thing different is that it has parantheses and a semicolon

####2017-02-20

I tried the code to below to remove the semicolon, since that's also a column separator
```{bash, eval = F}
sed -i 's|(IF-2; GTPase)|(IF-2 GTPase)|g' mapping_database.gff
```

But that didn't work. Still get the error at line 18699. I'm going to try downloading the gff file (gzipped) and running it through an online validator http://genometools.org/cgi-bin/gff3validator.cgi . 

Well it doesn't like comment lines. Let's remove those.
```{bash, eval = F}
sed -i '/^#/d' mapping_database.gff
```

Ah. Apparently the first line needs to start with ##gff-version 3. Let's add that back in.

```{bash, eval = F}
echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff
```

Now I get a parse error on line 22835. Looks like a CRISPR? Let's remove that line. Actually let's remove all lines that say CRISPR in them.

```{bash, eval = F}
sed '/CRISPR/d' mapping_database.gff > temp && mv temp mapping_database.gff
```

Oh good. Now I get the original error at line 18679. Josh's github says according to the GFFv3 standard, semi-colons have a reserved meaning and must be escaped. The standard recommends using the URL escape %3B. How can I replace only product names with semicolons? It has a space - can I use that?

```{bash, eval = F}
sed -i 's|; |%3B |g' mapping_database.gff
```

Technically it was validated, but got 40,000+ warnings about missing "##sequence-region" lines. I'm going to try coming back to this later, but will run as is for now since Josh didn't seem to run into this problem.

It read the gff file! Warning was "no features of type 'feature'" produced. Odd. Now it says "no module named pysam found" Looks like I need to go back to the python installation and add that module. Going now.

Got that module added to the install and tried again. It's taking forever, is that a good sign?
2 hours later - still not done. It was on one of the spalding servers I've been having trouble with, so restarted... and it finished five minutes later. Darn spalding. And whatever that feature.out output is, it's not what I want to use for RPKM counts. Although the mapped to unmapped ratio is nice.

__no_feature    898225
__ambiguous     0
__too_low_aQual 0
__not_aligned   5279253
__alignment_not_unique  0

What do I save to find out which genes/genomes it mapped to? Or are there supposed to be features in this file? I'm going to rerun and keep all the output this time.

Actually the raw bwa output looks like what I need. Can I process this in R? It's kind of large, but I could write my own script instead of the samtools/htseq combo to get exactly what I need out. Can I use samtools to parse down the size of that file?

Let's back up the train. Here's the results I need out of the mapping:

- How many reads are mapped vs unmapped
- What genome and gene (name please) did each read map to? (could be condensed to a table of counts per gene/genome)
- A fasta file of the unmapped read sequences

What exactly is in this output sam file from bwa? According to their manual:

- metaT read name
- flag about the mapping (includes unmapped)
- name of reference sequence it hit
- position of sequence
- mapping quality in Phred
- CIGAR?
- mate reference (for paired only)
- mate position
- inferred insert size
- which strant
- quailty of query seq in Phred
- optional fields

Really I only need the read name, did it map or not, and what did it hit. But the bwa file is way too big. I'm running again using samtool idxstats, which should give me a tab-delimited file of reference sequence name, sequence length, # mapped reads and # unmapped reads.

Next run technically worked, but produced all of the output to the screen and follows the format: 

TH03520DRAFT_TH03520_TBL_comb47_HYPODRAFT_10000124.2    95289   0       0
TH03520DRAFT_TH03520_TBL_comb47_HYPODRAFT_10000199.3    76284   8       0
TH03520DRAFT_TH03520_TBL_comb47_HYPODRAFT_10000209.4    75264   1       0
TH03520DRAFT_TH03520_TBL_comb47_HYPODRAFT_10000247.5    71296   0       0

So I assume that's genome/contig name, length of contig, number of reads and number of unmapped reads? Not so useful. HOw to I get the genes?


####2017-02-21

I did some reading on HTseq, and it should be able to count reads that hit genes. At the very least, it could at least open my gff file and show me if there's something wrong with the features. I'm going start an interactive session sending over python and samtools, and try to follow the instructions here: http://www-huber.embl.de/users/anders/HTSeq/doc/tour.html#tour

Here's what I'm typing in the interactive session:
```{bash, eval = F}
condor_submit -i interactive.sub
#Wait
cp /mnt/gluster/amlinz/GEODES001_nonrRNA.sam .
tar xzf python.tar.gz
tar xvf samtools.tar.gz
tar xvf mapping_database.tar.gz

samtools view -b -S -o GEODES001_nonrRNA.bam GEODES001_nonrRNA.sam
#Had some trouble with the command below - command flags have changed since Josh wrote his script. This is the correct format. Could this be why I have no features at the end?
samtools sort GEODES001_nonrRNA.bam -o GEODES001_nonrRNA.sorted.bam
samtools index GEODES001_nonrRNA.sorted.bam
samtools idxstats GEODES001_nonrRNA.sorted.bam
#That command works, but still goes by contig rather than by gene.
# Update the python path

mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

#Check the original htseq count command to see if fixing the sort line results in feature:
python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t CDS -i locus_tag -m intersection-strict -o GEODES001_nonrRNA.feature.sam GEODES001_nonrRNA.sorted.bam mapping_database.gff > GEODES001_nonrRNA.feature.out

# I think CDS and locus tag should work! I'll need to paste each feature.out as a column in a dataframe for DESeq, and I'll use the gff file as a metadata file
# How can I get a fast file of unmapped reads?
samtools view -f 4 GEODES001_nonrRNA.sam > GEODES001_nonrRNA.unaligned.sam 
samtools view -b -S -o GEODES001_nonrRNA.unaligned.bam GEODES001_nonrRNA.unaligned.sam
samtools bam2fq -nO GEODES001_nonrRNA.unaligned.bam > GEODES001_nonrRNA.unaligned.fastq 


```

That should do it! I've got unique identifiers and counts of reads that mapped, and I'll just need a metadata file to process those. I've also got a fastq file of unmapped reads, which should work for kraken. I've also realized that since both of these steps stem from the bwa output, I should split this into three separate scripts. This will cut down on space needed and hopefully speed up run time.  I could probably split up the input files for mapping, too, then put them back together with samtools merge.

Keeping the scripts below, but going to the current workflow section for this final pass! Should be pretty simple - I'll modify splitfastqs from the rRNA removal step, then run just the mapping on this step, then put it together. The processing and kraken results will be in a different folder.

02mapping.sh:
```{bash, eval = F}
#!/bin/bash
#Map metatranscriptome reads to my database of reference genomes

#Transfer metaT from gluster
cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/$1.fastq.gz .
cp /mnt/gluster/amlinz/python.tar.gz .
gzip -d $1.fastq.gz

#Unzip programs
tar xzf python.tar.gz
tar xvf bwa.tar.gz
tar xvf samtools.tar.gz
tar xvf mapping_database.tar.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

#Index the reference database
./bwa-0.7.12/bwa index mapping_database.fna

#Run the mapping step - using 5 processors
./bwa-0.7.12/bwa mem -t 3 mapping_database.fna $1.fastq > $1.sam
cp $1.sam /mnt/gluster/amlinz/

#Manipulate the output
samtools view -b -S -o $1.bam $1.sam
samtools sort -o $1.sorted.bam -O bam -T $1 -@ 3 $1.bam
samtools index $1.sorted.bam
rm $1.sam
rm $1.bam

#Count reads
python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t feature -i locus_tag -m intersection-strict -o $1.feature.sam $1.sorted.bam mapping_database.gff > $1.feature.out

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.feature.out /mnt/gluster/amlinz/GEODES_mapping_results/
rm $1.feature.out

#Clean up
rm -rf python
rm -rf bwa
rm -rf samtools
rm $1.fastq
rm *.sam
rm *.bam
rm mapping_database*
rm *.tar.gz
  
```

02mapping.sub
```{bash, eval = F}
# 02mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02mapping_$(Cluster).log
error = 02mapping_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 02mapping.sh
arguments = $(samplename)
output = 02mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = bwa.tar.gz,samtools.tar.gz, mapping_database.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 3
request_memory = 3GB
request_disk = 10GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from samplenames.txt
```


I've got everything copied up top, split fastqs modified, and a new script for merging at the end. I'll probably run that last one in interactive mode to check it first. Cleaning up my folder and leaving notes here as I test 3 metaTs.

Split fastqs started at 10:23, finished at 10:27. No errors. modified requests in submit file.
Mapping started at 10:39. At 11:13, most jobs were done but 13 were held. Why? Some jobs are going above the memory limit. Adjust and try again. Restarted at 11:17, finished at 12:09.

Did some futzing with samtools on the interactive sub and got something working. Started at 1:10, finished at 1:30, no errors. Looking good! I'll hold off running all of the mapping until I get the downsteam stuff working.

####2017-02-23

Coming back to the merging of files since I think I figured out my samtools issues. There was a major update in the past year, and this is the current documentation: http://www.htslib.org/doc/samtools.html . Josh is using the older version in his scripts.

I've rewritten the the samtools portion of merging and will try running it interactive mode first, then in script mode. Hopefully this will solve my issues with "truncated files" downstream.

After much fiddling and crashing an execute node once, I've got it working. On to redo-ing 03processing.

####2017-03-09

Welcome back! Since I last worked on the mapping step, I've written code for the downstream steps - counting the mapped reads, classifying the unmapped reads, and compiling/plotting the results in R. I finished my preliminary pass for lab meeting on the 6th and got some feedback on how to improve my workflow. 

There's a few things to change to generally improve the programming:

- Use a fastq splitting program instead of dividing total number of lines by 4. Sarah recommends getreads.sh from bbtools.
- Use a gff validation program instead of my awk/sed commands to get the mapping_database.gff file in the proper format.
- Switch bwa out for bbmap (strongly recommended by multiple lab members)
- Store output files as bam instead of sam (saves space and has more info)

There's also some scientific changes to make. I'm going to try several different versions of the mapping database on a subset of the data to determine what works best. This mean I need to a) define what makes one database better than another and b) come up with a way to run several mapping databases at once and compare the results.

I'm not quite sure how to approach that yet, so let's start with the easy stuff first - splitting fastq files. Before that, I should clean up my CHTC home folder and gluster account.

```{bash, eval = F}
#Make separate folders for sub and sh files
mkdir submits
mkdir executables
mv *.sub submits
mv *.sh executables
mv scripts/*.sub submits
mv scripts/*.sh executables
rm -r scripts

#Make a separate folder for programs and tarballs
mkdir zipped
mv *gz zipped
mv *bz2 zipped

#Make a folder for old output files that I want to save
mkdir output
rm temp.txt
rm test.txt
mv *.txt output
rm splitfastqsa*

#Now clean up in gluster. Remove intermediate kraken files
cd /mnt/gluster/amlinz/
rm -r GEODES_kraken_split
rm -r GEODES_kraken_results
rm -r GEODES_mapping_split/*
rm -r GEODES_mapping_results
```

I've downloaded BBMap here: https://sourceforge.net/projects/bbmap/?source=typ_redirect
The user guide is here:
http://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbmap-guide/


The file is BBMap_36.99.tar.gz. I've uploaded into to my "zipped" folder on CHTC. I'm going to start off with an interactive session to make sure the installation works and to figure out where the scripts I need are located.

```{bash, eval = F}
#Change my go-to interactive sub to tranfer BBMap
nano submits/testinteractive.sub

condor_submit -i submits/testinteractive.sub

#Wait for session to start

#Unzip to install
tar -xvzf BBMap_36.99.tar.gz
#Run the installation test from the user guide
bbmap/stats.sh in=bbmap/resources/phix174_ill.ref.fa.gz
#Works! Now where is my getreads.sh?
bbmap/getreads.sh
#The help file pops up, so that must be the location.

#Copy an input file in from gluster to test.

cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/GEODES001_nonrRNA.fastq.gz .

#How can I count how many sequences are in this file?
#Do my lines wrap?

# I got this code from biostars
zcat GEODES001_nonrRNA.fastq.gz | ruby -e'
skip_line = false
last_char = "+"
while gets
 if skip_line
  skip_line = false
  next
end
if $_ =~ /^@/ and last_char == "+"
 last_char = "@"
elsif $_ =~ /&@/ and last_char == "@"
 last_char = "+"
else
 STDERR.puts "WARNING: fastq lines wrap at line #{$.}"
 exit(1)
end
skip_line = true
end
STDERR.puts "fastq lines do not wrap"
'

#"fastq lines wrap at line 3"

#I wonder what happens if you give getreads.sh an output that's out of range.

bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=0-3 #prints output to screen nicely
wc -l GEODES001_nonrRNA.fastq
#24675059 lines, so no fewer than 6143765 reads (# if no wrapping)
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=7000000-7000003 out=out_of_range.fastq overwrite=T
#Produces an empty output file.
#Trying several ranges to try and find the end of the file

#Didn't find the exact end, but when you're out of it, you get an empty output file.

#Can I make a bunch of predefined files, then delete ones that are empty?

bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=0-999999 out=GEODES001_nonrRNA1.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1000000-1999999 out=GEODES001_nonrRNA2.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=2000000-2999999 out=GEODES001_nonrRNA3.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=3000000-3999999 out=GEODES001_nonrRNA4.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=4000000-4999999 out=GEODES001_nonrRNA5.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=5000000-5999999 out=GEODES001_nonrRNA6.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=6000000-6999999 out=GEODES001_nonrRNA7.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=7000000-7999999 out=GEODES001_nonrRNA8.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=8000000-8999999 out=GEODES001_nonrRNA9.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=9000000-9999999 out=GEODES001_nonrRNA10.fastq

#Error at line 10138059 in the second command - is this this end of file?
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=0-99999 out=GEODES001_nonrRNA1.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=100000-199999 out=GEODES001_nonrRNA2.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=200000-299999 out=GEODES001_nonrRNA3.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=300000-399999 out=GEODES001_nonrRNA4.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=400000-499999 out=GEODES001_nonrRNA5.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=500000-599999 out=GEODES001_nonrRNA6.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=600000-699999 out=GEODES001_nonrRNA7.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=700000-799999 out=GEODES001_nonrRNA8.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=800000-899999 out=GEODES001_nonrRNA9.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=900000-999999 out=GEODES001_nonrRNA10.fastq

bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1000000-1099999 out=GEODES001_nonrRNA11.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1100000-1199999 out=GEODES001_nonrRNA12.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1200000-1299999 out=GEODES001_nonrRNA13.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1300000-1399999 out=GEODES001_nonrRNA14.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1400000-1499999 out=GEODES001_nonrRNA15.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1500000-1599999 out=GEODES001_nonrRNA16.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1600000-1699999 out=GEODES001_nonrRNA17.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1700000-1799999 out=GEODES001_nonrRNA18.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1800000-1899999 out=GEODES001_nonrRNA19.fastq
bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=1900000-1999999 out=GEODES001_nonrRNA20.fastq

#Very weird - 11 is empty, 12 is normal, 13 gives the error. IDs 1300000 to 1399999?
#11 was missing a digit. My bad.
#Line 10138059 must be in ids 1300000-1399999

#Can I count unique line headers somehow? I know I can't use "@" because it's also a quality score, but maybe something longer?

grep @HISEQ GEODES001_nonrRNA.fastq | wc -l
#6168763 - that's right in my lines/4 range
grep + GEODES001_nonrRNA.fastq | wc -l
#Same number - but that doesn't match with where my error pops up.

#Let's look at that line specifically
sed '1{10138055;10138060}' GEODES001_nonrRNA.fastq

```

I looked at the code in getreads.sh, and it's using "@" to split sequences. THAT'S WHAT I WAS TRYING TO AVOID IN THE FIRST PLACE. It breaks on that particular line because there's a "@" in the quality sequence.

I wonder if I can manually change that in the script?
Nope. Still breaks. @ seems to have another meeting for java.

What about removing @ from the quality scores?

```{bash, eval = F}
 awk '!/HISEQ/{gsub(/@/, "?")}; 1' GEODES001_nonrRNA.fastq > mod.fastq

```

Nope, error at same line. Definitely does not have @.

What about converting to fasta and then splitting?
Would you believe it, there's the same error at the same line. WTF.
I'll come back to this later.

####2017-03-10

I've downloaded a fastq validator from here: http://genome.sph.umich.edu/wiki/FastQValidator
Running this in interactive mode on GEODES001 to see if it can diagnose the problem.

```{bash, eval = F}
#In interactive mode
tar -xvzf FastQValidatorLibStatGen.0.1.1a.tgz
cd fastQValidatorLibStatGen.0.1.1a
make all

cd ..
cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/GEODES001_nonrRNA.fastq.gz .
gzip -d GEODES001_nonrRNA.fastq.gz
./fastQValidator_0.1.1a/fastQValidator/bin/fastQValidator --file GEODES001_nonrRNA.fastq.gz

#FASTQ INVALID. Lots of repeated sequence identifiers, but not much I can do about that and I don't think that's causing the problem in one particular line. There are 6168721 sequences - a few less than I found with grep. I'd trust this program more. There are 2947696 errors. I'll turn off sequence ID matching and try again.
./fastQValidator_0.1.1a/fastQValidator/bin/fastQValidator --file GEODES001_nonrRNA.fastq.gz --disableSeqIDCheck

#Alright! Line 10138057, the sequence identifier lin was too short. Everything after that reads as an invalid character. What can I do about this
sed -n 10138057p GEODES001_nonrRNA.fastq

```

I've found the problem. Line 10138057 is a blank line. THANKS JGI. How do I remove blank lines from a file?

```{bash, eval = F}
sed '/^$/d' GEODES001_nonrRNA.fastq > mod.fastq
./fastQValidator_0.1.1a/fastQValidator/bin/fastQValidator --file mod.fastq --disableSeqIDCheck

#FASTQ SUCCESS. WOOHOO!
#Can I now accurately calculate the number of fastq seqs in my file from the number of lines? 
#Yes! 24675052/4 = 6168763, which is what the fastq validator reports
```

No errors reported in the fastq after I removed blank links, so let's try getreads.sh again.

In an interactive session:
```{bash, eval = F}
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/GEODES001_nonrRNA.fastq.gz .
gzip -d GEODES001_nonrRNA.fastq.gz

sed -i '/^$/d' GEODES001_nonrRNA.fastq

maxreads=$((`wc -l < GEODES001_nonrRNA.fastq` / 8 - 1))
startpoints=$(seq 0 500000 $maxreads)

#That's some funky math up there. The first command counts the number of lines, then divides by 4, giving 6168763. But each sequence is paired - when I give getreads.sh IDS from 0-100000, it reads 2000000 reads and quits between 3000000 and 4000000. So I want to divide this by 2 again, or in other words, divide the line count by 8 to get the max reads. Also, getreads.sh starts counting at 0, so I subtract by 1.
#The next line counts in increments from 500000 to the number of reads in the file. This should give me one million reads (paired) per file.

for num in $startpoints;
  do endpoint=$(($num + 499999));
  bbmap/getreads.sh in=GEODES001_nonrRNA.fastq id=$num-$endpoint out=GEODES001_nonrRNA_$endpoint.fastq overwrite=T;
  done

#That worked! I even have exactly 168762 reads left in the last file.
```

####2017-03-15

I went back to the rRNA removal step and rewrote the fastq splitting executable based on the work above. I reran everything, including the new samples that have arrived since February. Now I'll use that same script as a base here and then change my bwa mapping to bbmap.

I'm also going to change the numbering scheme of the scripts so that it's clear what sequence you run them in and there are no duplicates.
```{bash, eval = F}
for file in /mnt/gluster/amlinz/GEODES_nonrRNA_concat/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > samplenames.txt

head -3 samplenames.txt > test_samplenames.txt; mv test_samplenames.txt samplenames.txt
```


04splitfastqs.sub
```{bash, eval = F}

# 04split_fastq.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04split_fastq_$(Cluster).log
error = 04split_fastq_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/04split_fastq.sh
arguments = $(sample)
output = 04split_fastq_$(Cluster).out
#

should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2 GB
request_disk = 4 GB
#
# Just one job for testing
queue sample from samplenames.txt

```

04splitfastqs.sh
```{bash, eval = F}
#!/bin/bash
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/$1.fastq.gz .
gzip -d $1.fastq.gz

sed -i '/^$/d' $1_nonrRNA.fastq

maxreads=$((`wc -l < $1.fastq` / 8 - a))
startpoints=$(seq 0 500000 $maxreads)

for num in $startpoints;
  do endpoint=$(($num + 499999));
  bbmap/getreads.sh in=$1.fastq id=$num-$endpoint out=$1$endpoint.fastq overwrite=T;
  done

rm $1.fastq
gzip $1*
cp $1* /mnt/gluster/amlinz/GEODES_mapping_split
rm $1*
rm BBMap_36.99.tar.gz
rm -r bbmap


```

####2017-03-16

I started the scripts above and they ran for nearly 24 hours, producing no errors or output, until I killed it. It didn't take that long to run all 85 metatranscriptomes for the rRNA removal, and I'm using nearly the same code. I'll boot into interactive mode and see what's going on. I should at least see the getreads.sh output, so presumably it never made it to that step.

Think I found the problem - I left the .gz tag on the fastq file after I unzipped it. Removed that from the executable and restarted.

Actually the problem was that my variable $1 included the "_nonrRNA" tag and I included it in the script - so it was trying to process GEODES001_nonrRNA_nonrRNA.fastq.  Also there's something funky about variables and "_", will look into this further. Anyway, it's all working now.


####2017-03-17

Happy St. Patrick's Day! My goal for today is to get the bbmap mapping program working. I've already got the installation figured out for BBtools, so hopefully this should be straightforward. I'll run it in interactive mode first and record my commands below.

First, make the database. I'll use the ref MAGS SAGs for now, but switch to the metagenomes later.
```{bash, eval = F}
cat /mnt/gluster/amlinz/ref_genomes/fasta_files/*.fna > mapping_database.fna
cat /mnt/gluster/amlinz/ref_genomes/gff_files/*.gff > mapping_database.gff

#We've got to fix some formatting issues in the gff file first
#Remove comment lines
sed -i '/^#/d' mapping_database.gff
#Add back the first comment line
echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff
#Remove CRISPR arrays
sed '/CRISPR/d' mapping_database.gff > temp && mv temp mapping_database.gff
#Change semicolons in product names to a URL escape code
sed -i 's|; |%3B |g' mapping_database.gff
gzip mapping_database.gff 
gzip mapping_database.fna 

mv mapping_database* /mnt/gluster/amlinz/

```

```{bash, eval = F}

#!/bin/bash
#Map metatranscriptome reads to my database of reference genomes

#Transfer metaT from gluster
cp $1 .
name=$(basename $1 |cut -d'.' -f1)
cp /mnt/gluster/amlinz/mapping_database.fna.gz .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar xvf samtools.tar.gz
gzip -d $name.fastq.gz

#Index the reference database
bbmap/bbmap.sh ref=mapping_database.fna.gz

#Run the mapping step
#I've added a threads=1 parameter to keep the program from getting greedy and crashing a execute node. The minid parameter lets me set a minimum quality score for mapped reads. I'm also outputting the statistics produced (including %reads mapped) and % reads mapped to each scaffold.
bbmap/bbmap.sh in=$name.fastsq out=$name.mapped.sam minid=0.8 threads=1 scafstats=$name.scaffolds.txt statsfile=$name.stats.txt

# I want to store the output as bam. Use samtools to convert.
samtools view -b -S -o $name.bam $name.mapped.sam

#Copy bam file back to gluster
cp $name.bam /mnt/gluster/amlinz/GEODES_mapping_results/
#Clean up
rm -r bbmap
rm $name*
rm mapping_database.fna.gz
rm *.tar.gz

```

Before I get ahead of myself, let's switch over to the metagenome assemblies from GEODES instead of the ref MAGS SAGS.

This is going to take awhile. In the meantime, I'll write up the submit file and test to make sure the script works in parallel.

```{bash, eval = F}
# 04mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04mapping_$(Cluster).log
error = 04mapping_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/04mapping.sh
arguments = $(samplename)
output = 04mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz,zipped/samtools.tar.gz
transfer_output_files = *.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 5GB
request_disk = 4GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from path2splitmappingfastqs.txt

```

```{bash, eval = F}
find /mnt/gluster/amlinz/GEODES_mapping_results/ -type f > path2splitmappingfastqs.txt
```


Not bad! Produced the bam output in the right place. I get an error that rm *.sam doesn't work because no sam file was found - does converting it to bam destroy it? and the stats out files didn't get returned - can I add these to the submit file?

Note to self, using wildcards in the submit file results in your jobs being held. I'll transfer these to a new directory in gluster instead.

Works! This is running really fast, and I'm not sure how I would put the pieces back together again... can I run the mapping on non-split fastq files? TEst on file and time.

Hits an error on the first blank line. Removing with the sed command and trying again.

That took 50 minutes! Not bad. +1 for BBmap's speed. The stats look good, too - assuming I can run the downstream ht-seq on the full file, I think I'll do that.


####2017-03-18

I finished uploading the metagenome assemblies and gff files overnight. I'll build a database out of those and run the mapping on that to start.

```{bash, eval = F}
cat /mnt/gluster/amlinz/metagenome_assemblies/fastas/*.fna > mapping_database.fna
cat /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff > mapping_database.gff

#We've got to fix some formatting issues in the gff file first
#Remove comment lines
sed -i '/^#/d' mapping_database.gff
#Add back the first comment line
echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff
#Remove CRISPR arrays
sed '/CRISPR/d' mapping_database.gff > temp && mv temp mapping_database.gff
#Change semicolons in product names to a URL escape code
sed -i 's|; |%3B |g' mapping_database.gff
gzip mapping_database.gff 
gzip mapping_database.fna 

```

Looking good, except that there are no product names in the gff file. Looking in the IMG data folder I downloaded from JGI, this info is in the ".product_name" file. Why this is necessary is beyond me, there's a perfectly good place for product names in the gff file. But hey, do your own thing, JGI.

Anyway, for right now I just need to know what scaffold reads mapped to, which is output by BBmap. I'll write something to get the product name information later. Right now, I want to run BBMap using the metagenome assembly databases on the nonsplit nonrRNA fastq files.

```{bash, eval = F}
find /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ -type f > path2splitmappingfastqs.txt
 head -3 path2splitmappingfastqs.txt > temp.txt; mv temp.txt path2splitmappingfastqs.txt

```


Started at 11:11AM. We'll time that to see if it takes ridiculously long with multiple genomes + the giant database. I may need to up the disk space requirements.

####2017-03-20

Yep, it takes ridiculously long. Then gets evicted from the execute nodes for taking up too much disk space (12GB). I may need to split this up afterall. My options:

- Split up the fastq files again
- Add more threads to bbmap
- Use the nodisk option to prevent bbmap from writing anything but output
- Split up the mapping database by lake
- Remove scaffolds that are too short to do much with

I'll try removing small scaffolds first. I found some code online to do this with awk, but need to check line wrapping first.

Definitely wrapped. That sucks.I'll try removing newline characters first. From Stack Overflow:
```{bash, eval = F}
 sed -i ':a;N;/^>/M!s/\n//;ta;P;D' /mnt/gluster/amlinz/mapping_database.fna
```

While that's running, I'll work on getting CLaMs running. I downloaded both the UI and the command line. They run on Java, so supposedly by unzipping the installation package and clicking the .jar file, it might work if the configs are right... hey that worked!

I'm uploading the .fna file from the GEODES005 assembly in the GUI to test. I don't think it likes it very much, it's kind of frozen. Maybe I should run the length filtering here, too.

I crashed CLaMs. Definitely going to look into give it some length filtered sequences.

Ran:
```{bash, eval = F}
awk '!/^>/ { next } { getline seq } length(seq) >= 200 { print $0 "\n" seq }' GEODES005.assembled.fna > filtered.fna
```

Didn't reduce the file size at all. Not sure if that would help - but I still need to get the data into clams. For now I'll worry about the mapping, though.

Alright, let's try the nodisk option.

Update: that exceeded the memory requirements of BBmap itself. Oops. I'll try putting the database back to disk and using split fastq files instead.

####2017-03-21

Splitting the fastq files didn't help - I still ran out of memory. After talking to Sarah, I think I need to split up the mapping database. This is non-competitve mapping, and I want competive, so I'll do "semi" competitve mapping by parsing the output files for the best hit out of all of the database sections.

I was running CLams on just the GEODES005 assembly yesterday. The file loaded and was running, but showed no progress after several hours. I'll try again on the smaller input file. 

I'll use an interactive session and getreads.sh to split up the assembly files.

```{bash, eval = F}
condor_submit -i submits/testinteractive.sub
#Wait
#My old mapping database was 1/2 GB, so I'll aim for that size in my split database files.
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/metagenome_assemblies/fastas/GEODES005.assembled.fna .

#sed -i '/^$/d' $1.filter-MTF.fastq

#Since this is a wrapped fasta file, I'm going to calculate the maxreads differently
maxreads=$(grep -c "^>" GEODES005.assembled.fna)
startpoints=$(seq 0 1000000 $maxreads)

for num in $startpoints;
  do endpoint=$(($num + 999999));
  bbmap/getreads.sh in=GEODES005.assembled.fna id=$num-$endpoint out=GEODES005.assembled.$endpoint.fna overwrite=T;
  done

rm GEODES005.assembled.fna
gzip *.fna
mkdir /mnt/gluster/amlinz/metagenome_assemblies/split_files/
cp *fna.gz /mnt/gluster/amlinz/metagenome_assemblies/split_files/
rm *.fna.gz
rm BBMap_36.99.tar.gz
rm -r bbmap
exit
```

I'll test on just the one assembled metagenome for now, and add more when I have everythign working. 

Looks like to run pairwise combinations of metaTs and databases, I'll need a list of every combination. Borrowing some code I found online to do this in bash.

```{bash, eval = F}
find /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ -type f > path2splitmappingfastqs.txt
 head -3 path2splitmappingfastqs.txt > temp.txt; mv temp.txt path2splitmappingfastqs.txt

ls /mnt/gluster/amlinz/metagenome_assemblies/split_files/ > split_databases.txt

for a in $(awk '{print $1}' path2splitmappingfastqs.txt) 
do 
    for b in $(awk '{print $1}' split_databases.txt) 
    do 
        echo $a $b 
    done 
done > metaT_db_combo.txt
  
head -2 metaT_db_combo.txt > temp.txt; mv temp.txt metaT_db_combo.txt

```

Now my argument $1 has both the query and database. I'll need to modify my executable to split this into two variables.

```{bash, eval = F}

#!/bin/bash
#Map metatranscriptome reads to my database of reference genomes
path=$(echo $1 | cut -d' ' -f1)
db=$(echo $1 | cut -d' ' -f2)
#Transfer metaT from gluster
cp $path .
name=$(basename $path |cut -d'.' -f1)
cp /mnt/gluster/amlinz/metagenome_assemblies/split_files/$db .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar xvf samtools.tar.gz
gzip -d $name.fastq.gz

#Index the reference database
bbmap/bbmap.sh ref=$db

#Run the mapping step
#I've added a threads=1 parameter to keep the program from getting greedy and crashing a execute node. The minid parameter lets me set a minimum quality score for mapped reads. I'm also outputting the statistics produced (including %reads mapped) and % reads mapped to each scaffold.
bbmap/bbmap.sh in=$name.fastq out=$name.mapped.sam minid=0.8 threads=1 scafstats=$name.scaffolds.txt statsfile=$name.stats.txt

# I want to store the output as bam. Use samtools to convert.
samtools view -b -S -o $name.bam $name.mapped.sam

#Copy bam file back to gluster
cp $name.bam /mnt/gluster/amlinz/GEODES_mapping_results/
#Clean up
rm -r bbmap
rm $name*
rm $db
rm *.tar.gz

```

Trying on two files to start. 

I had some issues with the variable naming - apparently the sh automatically recongizes the space as meaning there are two variables. Script changed to reflect that. After that it works, except for not copying the output stats files over to gluster. I'll fix that and then run on 3 metaTs and 4 db parts. I'll use those output files to write a script to keep only the best hits from all database parts.

####2017-03-22

3 of the files never finished - I suspect these are the first chunks of the database, and that they're too big. Next time I'll chop into smaller pieces. In the meantime, I'll use the output I have to write a parsing script. I think I'll do this in Python using HTseq. Will run in interactive mode first to start figure out how to code python...

```{bash, eval = F}
cp /mnt/gluster/amlinz/GEODES_mapping_results/GEODES071*.bam .
tar xvf samtools.tar.gz
tar xvf python.tar.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

for file in *.bam;do samtools sort $file sorted.$file;done

#samtools merge -n -u GEODES071_all.bam sorted.*.bam
#header will be wrong on the one below, not sure if this will be an issue or not
samtools cat -o GEODES071_all.bam sorted.*.bam

python
import sys,os,HTSeq

bam_file = HTSeq.BAM_Reader( "GEODES071_all.bam" )
bundles = HTSeq.bundle.mulitple.alignmnets(bam_file)

exit
```

The samtools bits take FOREVER and then printing bundles gave an error about an unknown CIGAR string "=". Google says this is because HTseq doesn't support SAM v1.4, only SAM v1.3. I can reformat the sam files with bbtools, or set my mapping to output v1.3 instead of 1.4. I need to redo my mapping with smaller database sizes anyway, so I'll do that option.

Here's the full commands to do this:

```{bash, eval = F}
condor_submit -i submits/testinteractive.sub
#Wait
#My old mapping database was 1/2 GB, so I'll aim for that size in my split database files.
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/metagenome_assemblies/fastas/GEODES005.assembled.fna .


#Since this is a wrapped fasta file, I'm going to calculate the maxreads differently
maxreads=$(grep -c "^>" GEODES005.assembled.fna)
startpoints=$(seq 0 250000 $maxreads)

for num in $startpoints;
  do endpoint=$(($num + 249999));
  bbmap/getreads.sh in=GEODES005.assembled.fna id=$num-$endpoint out=GEODES005.assembled.$endpoint.fna overwrite=T;
  done

rm GEODES005.assembled.fna
gzip *.fna
mkdir /mnt/gluster/amlinz/metagenome_assemblies/split_files/
cp *fna.gz /mnt/gluster/amlinz/metagenome_assemblies/split_files/
rm *.fna.gz
rm BBMap_36.99.tar.gz
rm -r bbmap
exit

find /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ -type f > path2splitmappingfastqs.txt
 head -3 path2splitmappingfastqs.txt > temp.txt; mv temp.txt path2splitmappingfastqs.txt

ls /mnt/gluster/amlinz/metagenome_assemblies/split_files/ > split_databases.txt

for a in $(awk '{print $1}' path2splitmappingfastqs.txt) 
do 
    for b in $(awk '{print $1}' split_databases.txt) 
    do 
        echo $a $b 
    done 
done > metaT_db_combo.txt
  
```

04mapping.sh
```{bash, eval = F}

#!/bin/bash
#Map metatranscriptome reads to my database of reference genomes
#Transfer metaT from gluster
cp $1 .
name=$(basename $1 |cut -d'.' -f1)
cp /mnt/gluster/amlinz/metagenome_assemblies/split_files/$2 .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar xvf samtools.tar.gz
gzip -d $name.fastq.gz
sed -i '/^$/d' $name.fastq

#Index the reference database
bbmap/bbmap.sh ref=$2

#Run the mapping step
bbmap/bbmap.sh in=$name.fastq out=$name.$2.mapped.sam minid=0.8 sam=1.3 threads=1 scafstats=$name.$2.scaffolds.txt statsfile=$name.$2.stats.txt

# I want to store the output as bam. Use samtools to convert.
samtools view -b -S -o $name.$2.bam $name.$2.mapped.sam

#Copy bam file back to gluster
cp $name.$2.bam /mnt/gluster/amlinz/GEODES_mapping_results/
cp *.txt /mnt/gluster/amlinz/GEODES_mapping_stats/

#Clean up
rm -r bbmap
rm *.bam
rm *.sam
rm *.fastq
rm *.gz


```

39 jobs submitted with the new, shorter database file.

####2017-03-27

Since last week, I've been chatting with people about how best to do these tasks. For BBMap, Trina put me in touch with its developer, Brian Bushnell. He hasn't responded to my email so I assume he doesn't have the answers. I could contact CHTC help, but even my split files from last week are taking between 6 - 10 hours to run (just BBMap, not the samtools portion). If I could get access to more RAM, it would take even longer, so that's not a great option. I still think the piecemeal version is the way to go.

For ClaMS, I've got it working on these smaller database chunks, but it's still quite slow. Really it's the same issue as BBMap - both run in Java, and have RAM limits which I far exceed. Also has issues with the NCBI database in that I have to specify a level to classify to ahead of time. Other recommendations I got from the lab were sourmash, LCA, and protein homology via blast. sourmash is a cool new matching algorithm, but I would need to write the rest of the classifier myself. LCA is supposedly fast and easy to use. A combination of LCA and protein homology would probably be the most accurate. I will discuss this with Trina on Wednesday.

In the meantime, I'll work on that python script to put the mapping results together. I ran the mapping with SAM1.3 so HTSeq should be able to read the bam files now. In interactive mode:

```{bash, eval = F}
cp /mnt/gluster/amlinz/GEODES_mapping_results/GEODES071*.bam .
tar xvf samtools.tar.gz
tar xvf python.tar.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

for file in *.bam;do samtools sort -o sorted.$file $file ;done

#samtools merge -n -u GEODES071_all.bam sorted.*.bam
#header will be wrong on the one below, not sure if this will be an issue or not
samtools cat -o GEODES071_all.bam sorted.*.bam

python
import sys,os,HTSeq

bam_file = HTSeq.BAM_Reader( "GEODES071_all.bam" )


exit


samtools view -f 4 -o GEODES071_all.sam GEODES071_all.bam


```

Still having problems with htseq - I don't understand how this BAM reader works, and I'm not seeing any examples online remotely close to what I want to do. Very tempted to write this in R, but the files are still huge. The full bam file for one sample is 18 GB, and I killed its sam file at 62 GB and it wasn't done writing yet. To cut down on this file size, I'm running the mapping again with mappedonly=t. This will save only mapped reads and not output a line for every read that didn't map. If I want to know how many didn't map, I can take the num entries in the sam file / num lines in the fasta file. 
I also told it to stop outputting the stats files. Doesn't really make sense now that I"m splitting everything.

####2017-03-30

The saga of mapping methods continues. I met with CHTC yesterday and they think that I CAN map without splitting if I just make the index once and store it in gluster. There are enough giant computers to do this, and they're willing to waive my time limits if necessary. I'll start this process by running the indexing on my small database, just to get the file names right.

```{bash, eval = F}
#Make the small database
cat /mnt/gluster/amlinz/ref_genomes/fasta_files/*.fna > mapping_database.fna
gzip mapping_database.fna 
cp mapping_database.fna.gz /mnt/gluster/amlinz/

#Test in interactive mode
condor_submit -i submits/testinteractive.sub

#In interactive mode
cp /mnt/gluster/amlinz/mapping_database.fna.gz .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz

#Index the reference database
bbmap/bbmap.sh ref=mapping_database.fna.gz

#clean up
rm mapping_database.fna.gz
rm -r ref
rm BBMap_36.99.tar.gz
rm -r bbmap

exit


```

Indexing creates a directory called "ref". I'm guessing that the mapping step looks for this directory and uses whatever's in it. Time to start the big index.

04build_index.sh:
```{bash, eval = F}
#!/bin/bash
#Build a re-usable mapping index
cat /mnt/gluster/amlinz/metagenome_assemblies/fastas/*.fna > mapping_database.fna
cat /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff > mapping_database.gff

#We've got to fix some formatting issues in the gff file
#Remove comment lines
sed -i '/^#/d' mapping_database.gff
#Add back the first comment line
echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff
#Remove CRISPR arrays
sed '/CRISPR/d' mapping_database.gff > temp && mv temp mapping_database.gff
#Change semicolons in product names to a URL escape code
sed -i 's|; |%3B |g' mapping_database.gff

#Remove short contigs in the mapping database fna file
awk '!/^>/ { next } { getline seq } length(seq) >= 1000 { print $0 "\n" seq }' mapping_database.fna > mapping_database1000.fna

gzip mapping_database.gff 
gzip mapping_database1000.fna

#No longer need the gff file or the unfiltered fna file - move to gluster
cp mapping_database.gff.gz /mnt/gluster/amlinz/
rm mapping_database.gff.gz
rm mapping_database.fna

#Unzip bbmap and build the index
tar -xvzf BBMap_36.99.tar.gz
bbmap/bbmap.sh ref=mapping_database1000.fna.gz

#make ref/ a tarball and move to gluster
tar czvf ref.tar.gz ref/

cp ref.tar.gz /mnt/gluster/amlinz/
rm ref.tar.gz
rm mapping_database1000.fna.gz
```

04build_index.sub:
```{bash, eval = F}
# 04build_index.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04build_index_$(Cluster).log
error = 04build_index_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/04build_index.sh
output = 04build_index_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz
#transfer_output_files = 
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 15GB
request_disk = 60GB
#
#
queue 

```

Here goes nothing...

After building the database for 4 hours, bbmap reports the reference is empty... I suspect the read trimming command. Will try a bbtools command instead.

```{bash, eval = F}
#!/bin/bash
#Build a re-usable mapping index
#cat /mnt/gluster/amlinz/metagenome_assemblies/fastas/*.fna > mapping_database.fna
#cat /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff > mapping_database.gff

#We've got to fix some formatting issues in the gff file
#Remove comment lines
#sed -i '/^#/d' mapping_database.gff
#Add back the first comment line
#echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff
#Remove CRISPR arrays
#sed '/CRISPR/d' mapping_database.gff > temp && mv temp mapping_database.gff
#Change semicolons in product names to a URL escape code
#sed -i 's|; |%3B |g' mapping_database.gff

cp /mnt/gluster/amlinz/mapping_database.fna.gz .
tar -xvzf BBMap_36.99.tar.gz
#Remove short contigs in the mapping database fna file
bbmap/reformat.sh in=mapping_database.fna.gz out=mapping_database1000.fna minlength=1000

#gzip mapping_database.gff 
gzip mapping_database1000.fna

#No longer need the gff file or the unfiltered fna file - move to gluster
#cp mapping_database.gff.gz /mnt/gluster/amlinz/
#rm mapping_database.gff.gz
rm mapping_database.fna

#Unzip bbmap and build the index
bbmap/bbmap.sh ref=mapping_database1000.fna.gz

#make ref/ a tarball and move to gluster
tar czvf ref.tar.gz ref/

cp ref.tar.gz /mnt/gluster/amlinz/
rm ref.tar.gz
rm mapping_database1000.fna.gz
```

####2017-04-03

Finished running on Saturday! Now I will modify my mapping script to refer to the database in gluster.

```{bash, eval = F}

#!/bin/bash
#Map metatranscriptome reads to my pre-indexed database of reference genomes
#Transfer metaT from gluster
#Not splitting the metaTs anymore
cp /mnt/gluster/amlinz/GEODES_nonrRNA_concat/$1 .
cp /mnt/gluster/amlinz/ref.tar.gz .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar -xvf samtools.tar.gz
tar -xvzf ref.tar.gz
gzip -d $1
name=$(basename $1 |cut -d'.' -f1)
sed -i '/^$/d' $name.fastq

#Run the mapping step
bbmap/bbmap.sh in=$name.fastq out=$name.mapped.sam minid=0.8 sam=1.3 threads=1 mappedonly=t

# I want to store the output as bam. Use samtools to convert.
samtools view -b -S -o $name.mapped.bam $name.mapped.sam

#Copy bam file back to gluster
cp $name.mapped.bam /mnt/gluster/amlinz/GEODES_mapping_results/

#Clean up
rm -r bbmap
rm -r ref
rm *.bam
rm *.sam
rm *.fastq
rm *.gz
```

Make a short list of metaTs to test

```{bash, eval = F}
ls /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ > path2mappingfastqs.txt
head -3 path2mappingfastqs.txt > temp.txt; mv temp.txt path2mappingfastqs.txt
```
 
Modify the submit file
 
```{bash, eval = F}

# 04mapping.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04mapping_$(Cluster).log
error = 04mapping_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/04mapping.sh
arguments = $(samplename)
output = 04mapping_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz,zipped/samtools.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory =15GB
request_disk = 60GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from path2mappingfastqs.txt


```

Error: can't find ref/1/summary.txt. Will go into interactive mode to test.
Forgot the current dir in the copy ref command, and had the $name variable wrong. Running again.

####2017-04-04
Ran for awhile then got booted off for exceeding RAM usage. After reading, it looks like I need to specify where the index is with build=1. If I specify no index, bbmap starts building a new one in RAM from the info in ref/. In theory, the build parameter says go into indexed folders ref/genome/1 and ref/index/1 and use that. 


Still no dice. Closer inspection showed that the index building is aborting early? Emailed Brian to find out why.

####2017-04-05

Brian says the indexing step ran out of memory, particularly the Java bit. I think I'm going to need to run this in low memory mode. Brian says that should take about 30GB RAM - Ill change the submit file to this amount, add the-Xmx flag so Java uses this amount, and turn on the lowmemory mode with usemodulo. 

Well that finished oddly quickly and doesn't seem to have aborted... Starting the mapping again to test.

Update as of 6:15PM: first job ran out of memory. Will up it tomorrow.

####2017-04-06

Well, good thing I left the other two jobs running - they ran just fine! One finished at 10PM and the other finished at 12:30AM. Sizes are both around 350MB, so not bad at all.

So how much memory did that first program try and use? What was the difference? GEODES001 is slightly bigger than GEODES002 and GEODES003, but not by much and there are certainly much bigger metatranscriptomes. 

Job usage: 21GB disk, 17GB RAM. 20 GB disk, 20GB RAM (limit). 20 GB disk, 20GB RAM (limit)

I'll try upping to 30GB RAM and running more jobs to see if it's only the really big files that are failing. If it is, I'll split up the fastq files and either run them each as a job or run them sequentially in the same script.

Also dropping disk space to 25, since 20 seems to be all that is needed.
```{bash, eval = F}
ls /mnt/gluster/amlinz/GEODES_nonrRNA_concat/ > path2mappingfastqs.txt
head -10 path2mappingfastqs.txt > temp.txt; mv temp.txt path2mappingfastqs.txt
```

####2017-04-10

Success! Everything finished with no errors and the bam files look good. I did some poking around in the index and it looks like everything is there - same number of sequences as in the input fasta, so that's a good sign. I think I'm confident in saying the mapping worked. The last thing I want to do is write something to quickly check the .err files to make sure they worked,so that I don't have to go through them one by one.

```{bash, eval = F}
for file in 04mapping*.err; do test=$(grep "SAM header is present:" $file | wc -l); echo "$file $test"; done
```

I'm going to step away from mapping for a bit to get the HTSEQ count bit working with this output. I also need to figure out how to classify the contigs. Then I'll come back here, clean up the current workflow, and run the mapping on all files.

####2017-06-16

~ 10 YEARS LATER ~

Just kidding, it only feels that way.

HTSEQ takes forever, so I switched to featureCounts (a much faster program instead). It's pretty great so far and produce nearly identical results to HTSEQ when Josh tested it on some of our previous data. But I did need to re-run the mapping to use trd = T, which means it trims white space from names. Otherwise featureCounts truncates names. I'm still sticking with mapped = T to save space, featureCounts still records things with 0 reads mapped. Other than that, I changed some numbering.
Updating the workflow with the new scripts now.

####2017-06-28

Met with Frank A. about my workflow yesterday! He had some great ideas and corrections. Here's my notes regarding the mapping step:

- Don't use any mapping threshold lower than 90%, you get weird stuff otherwise. 
- If you have some divergent sequences that don't map, try LAST (using translational search or low threshold DNA search) to identify them. This runs pretty quickly.
- To deal with mulit-mapping reads, count only unique hits first, then assign the multi-mapping reads *based on the distribution of unique hits.* So if you have gene A and gene B, and gene A has 9 times more reads than gene B, a read mapping to gene A and B should be randomly assigned to A 9/10 times and B 1/10 times.
- If you are mapping to 95% ID, getting 30-40% of reads to map is sufficient.
- Trina would like to know the proportion of reads that map to MGs vs SAGs vs MAGs
- Redundant sequences are a huge issue! This might be why I have so many mapped features (Frank thinks my 8 million CDS hits is oddly high). To fix this, our favorite option was creating a non-redundant gene catalog using CDhit. You lose information about contigs, but Frank thinks we shouldn't put too much stock in contig assemblies either. Alternatively, I could use mapping to find unique contigs.
- Regarding the taxonomic classifications, Frank thinks binning and then classifying based on marker genes is the way to go. Contig classifications are iffy at best and you can't say contigs came from the same organism because they had the same classification. Metabat is the group favorite for binning. For unbinned contigs, BLAST/LAST the genes to refseq. 

####2017-07-17

Goal #1: use a test genome (2582580633) to extract only coding regions from the genome based on the gff file. I think I can use genometools gt extractfeat for this. The real question is can I make a gff file just for cds at the end to use for featurecounts.

Here's my submit file to try this out:
```{bash, eval = F}
#interactive1.sub
#
universe = vanilla
# Name the log file:
log = testgffextraction.log

# Name the files where standard output and error should be saved:
output = process.out
error = process.err

# If you wish to compile code, you'll need the below lines.
#  Otherwise, LEAVE THEM OUT if you just want to interactively test!
+IsBuildJob = true
requirements = (OpSysAndVer =?= "SL6") && ( IsBuildSlot == true )

# Indicate all files that need to go into the interactive job session,
#  including any tar files that you prepared:
transfer_input_files = 2582580633.fna,2582580633.gff,zipped/genometools-1.5.9.tar.gz

# It's still important to request enough computing resources. The below
#  values are a good starting point, but consider your file sizes for an
#  estimate of "disk" and use any other information you might have
#  for "memory" and/or "cpus".
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 8GB
request_disk = 4GB

queue

```

Before I can use the genome tools extraction feature, I need to format the gff files. borrowing code form 03processing lab notebook.

```{bash, eval = F}
#!/bin/bash
#Extract coding genes from a test genome

tar -xvzf genometools-1.5.9.tar.gz
cd genometools-1.5.9
make cairo=no
make prefix=$(pwd)/../genometools/ cairo=no install
cd ..
export PATH=$(pwd)/genometools/bin:$PATH

for file in 2*gff;do echo '##gff-version 3' | cat - $file > temp && mv temp $file;sed '/CRISPR/d' $file > temp && mv temp $file; sed -i 's|; |%3B |g' $file; gt gff3 -sort yes -tidy -retainids -o sorted_$file $file; done

gt extractfeat -type CDS -seqid yes -retainids yes -seqfile 2582580633.fna 

```

Not quite working right - won't accept both gff and seq file

It looks like I need a mapping file that says which contig is in which sequencing file. Which assumes that each contig is in a separate file. So I need to:
- split each contig (read?) into separate files (use bbtools?)
- make a mapping formatted file that says which contig name (from gff file) matches which sequencing file
- the gff file.

Presumably the program will read an entry in the gff file, look up which fna file contains the contig, and use the sequence position to extract the coding regions. My output will be a fna file of just coding regions for that genome - with any luck, they'll be a new gff file too, but I guess I can make that if I need to. I still don't know how to get the gff through CDHit anyway.

```{bash, eval = F}
tar -xvzf BBMap_36.99.tar.gz

carrots=$((`grep ">" 2582580633.fna | wc -l`))
startpoints=$(seq 0 1 $(($carrots-1)))

for num in $startpoints; do bbmap/getreads.sh in=2582580633.fna id=$num out=contig_$num.fna overwrite=T; done

tar -xvzf genometools-1.5.9.tar.gz
cd genometools-1.5.9
make cairo=no
make prefix=$(pwd)/../genometools/ cairo=no install
cd ..
export PATH=$(pwd)/genometools/bin:$PATH

for file in 2*gff;do echo '##gff-version 3' | cat - $file > temp && mv temp $file;sed '/CRISPR/d' $file > temp && mv temp $file; sed -i 's|; |%3B |g' $file; gt gff3 -sort yes -tidy -retainids -o sorted_$file $file; done

# That all seemed to work well - each contig is now in a separate file.
# I need the mapping in form:
# mapping = {
#  chr1  = "hs_ref_chr1.fa.gz",
#  chr2  = "hs_ref_chr2.fa.gz"
# }

# or in my case,
# mapping = {
#  TE3838DRAFT_TBepi_metabat_3838_1008378.3  = "contig_2.fna",
#  TE3838DRAFT_TBepi_metabat_3838_1019275.4  = "contig_3.fna"
# }
# contig name is the > to the first space. Let's make one column of that, one column of file names, paste together with stuff in between, and stick the first and last lines on there.

for file in contig_*.fna; do head -n1 $file | awk '{print $1;}'; done > contig_names.txt
#remove the carrot
sed -e 's/>//g' contig_names.txt > temp.txt && mv temp.txt contig_names.txt

#List the contig file names
for file in contig_*.fna; do echo " = "$file","; done > file_names.txt
#add quotes
sed -e 's/con/"con/g' file_names.txt > temp.txt && mv temp.txt file_names.txt
sed -e 's/fna/fna"/g' file_names.txt > temp.txt && mv temp.txt file_names.txt

# Remove the last comma in the file
sed '$ s/.$//' file_names.txt > temp.txt && mv temp.txt file_names.txt

#combine the two files
paste -d "", contig_names.txt file_names.txt > mapping_file.txt
# remvoe the comma separator
sed 's/, =/ =/g' mapping_file.txt > temp.txt && mv temp.txt mapping_file.txt

echo 'mapping = {' | cat - mapping_file.txt > temp.txt && mv temp.txt mapping_file.txt
echo '}' >> mapping_file.txt

gt extractfeat -type CDS -seqid yes -retainids yes -regionmapping mapping_file.txt 2582580633.gff  
# not working

#Well shoot. All I needed to do in the first place was add matchdescstart
 gt extractfeat -type CDS -seqid yes -retainids yes -seqfile 2582580633.fna -matchdescstart sorted_2582580633.gff >  CDS_2582580633.gff
 
 # Remove everything but the CDS.gff file and the genometools tarball and clear out
tar cvzf genometools.tar.gz genometools
```

Next I'll install CDHit and test it on my one fasta file. After that, I'll need to recreate a gff file to go with the CDhit output and add in all the information I can to the fasta headers. Or figure out how to make featurecounts add mroe information to its output.

```{bash, eval = F}
#change testinteractive.sub to have the new CDhit tarball

tar xvf cd-hit-v4.6.8-2017-0621-source.tar.gz --gunzip
cd cd-hit-v4.6.8-2017-0621
make
cd cd-hit-auxtools
make

cd ../..
tar czvf cd-hit.tar.gz cd-hit-v4.6.8-2017-0621
exit
mv cd-hit.tar.gz zipped/
```
Modify the testgff script to send out the cdhit tarball, the gff file, and the CDS fna file

```{bash, eval = F}
./cd-hit-v4.6.8-2017-0621/cd-hit-est -i CDS_2582580633.fna -o nr_2582580633.fna -c 0.95 

```

####2017-07-28
Test with multiple genomes!

```{bash, eval = F}
tar xvzf cd-hit.tar.gz
tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH

for file in 2*gff;do echo '##gff-version 3' | cat - $file > temp && mv temp $file;sed '/CRISPR/d' $file > temp && mv temp $file; sed -i 's|; |%3B |g' $file; gt gff3 -sort yes -tidy -retainids -o sorted_$file $file; done

for file in 2*fna; do name=`echo "$file" | cut -d'.' -f1`;gt extractfeat -type CDS -seqid yes -retainids yes -seqfile $name.fna -matchdescstart sorted_$name.gff >  CDS_$name.fna; done

cat CDS*fna > all_CDS.fna

./cd-hit-v4.6.8-2017-0621/cd-hit-est -i all_CDS.fna -o nr_test.fna -c 0.95

#Now for the complicated part - the dummy gff. Step 1, pull out the fasta headers

grep ">" nr_test.fna > fasta_headers.txt
#remove the carrot
sed -e 's/>//g' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
#keep only the first field
awk '{print $1}' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
#check for duplicate ids
sort fasta_headers.txt | uniq -cd
#nothing - good to go!

# Step 2, Link cluster number to that id

cat fasta_headers.txt | while read line; do hit=`grep -n $line nr_test.fna.clstr | awk -F: '{print $1}'`; head -n $hit nr_test.fna.clstr > splitfile.clstr; cluster=`grep "Cluster" splitfile.clstr | tail -1`; echo $cluster; done > clusters.txt
#remove the carrot and the space
#sed -e 's/>//g' clusters.txt > temp.txt && mv temp.txt clusters.txt
#sed -e 's/ //g' clusters.txt > temp.txt && mv temp.txt clusters.txt

cat fasta_headers.txt | while read line; do length=`grep $line nr_test.fna.clstr | awk '{print $2}' | sed 's/[^0-9]*//g'`; echo $length; done > endpoint.txt

# looking good! from here, I need phylogeny from the metagenome binning, so I'll hold off the on the rest of the dummy gff fro now. Hardest part is done, though.
```

####2017-08-08

Quick update on the metagenomic binning - it's been a bit of a saga. Gluster went down for updates so I had to move all my stuff off, make backup copies on the new lab storage drive (probably a good thing anyway), re-upload my files to Gluster after a week of updates, realize they have imposed new file number limits and I am way over, meet with CHTC to boost my limits, switch submit servers to get more space in my home folder and use Gluster less, re-upload the metagenome assemblies, talk to CHTC about how to install Metabat with all its many dependencies, they suggest a docker, available docker does not work, CHTC asks Metabat to make an official one, they do!, and now I can't get that to work.

I've asked Lauren what I can do about that. Meanwhile, I'll work on formatting the algae gffs.

testgffextraction.sub:
```{bash, eval = F}
tar xvzf refseq_algae.tar.gz
tar xvzf cd-hit.tar.gz
tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH

gzip -d refseq_algae/fastas/*
gzip -d refseq_algae/gffs/*

for file in refseq_algae/gffs/*;do name=$(basename $file);gt gff3 -sort yes -tidy -retainids -o sorted_$name $file; done

for file in refseq_algae/fastas/*; do name=$(basename $file .fna); gt extractfeat -type CDS -seqid yes -retainids yes -seqfile refseq_algae/fastas/$name.fna -matchdescstart sorted_$name.gff >  CDS_$name.fna; done

cat CDS*fna > all_CDS.fna

./cd-hit-v4.6.8-2017-0621/cd-hit-est -i all_CDS.fna -o nr_test.fna -c 0.95

# Make sure the gff stuff I've written so far works, too
grep ">" nr_test.fna > fasta_headers.txt
sed -e 's/>//g' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
#awk '{print $1}' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
#instead of removing 2nd field, just remove the space
sed -e 's/ /_/g' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
sort fasta_headers.txt | uniq -cd
#lots of duplicate headers - what gives?
#Keeping id helps, but still many dups. Looks like id tells chromosome. Are there multiple CDS entries for each cds#?
#Yup. that's weird. I think I'll ask around the lab on why this is the case.

numlines=$(wc -l sorted_GCF_000149405.2_ASM14940v2_genomic.gff | awk -F ' ' '{print $1}')
eachline=$(seq 0 $numlines)
testline=$(seq 1000 1010)

for num in $testline; do line=$(sed "${num}q;d" sorted_GCF_000149405.2_ASM14940v2_genomic.gff); iscds=$(echo "$line" | grep "CDS" | wc -l); if [ $iscds -gt 0 ]; then info=$(awk -F ' ' '{print9}'); repeats=$(head -$num sorted_GCF_000149405.2_ASM14940v2_genomic.gff | grep $info | wc -l); echo $repeats; fi; done

#never mind, I think it'd be easier to fix this in fasta headers than in the gff file.


```

####2017-08-09
Talking to Josh, it sounds like these duplicated CDS things - transpliced genes - shouldn't be too frequent, so I will try just removing them.

```{bash, eval = F}
tar xvzf refseq_algae.tar.gz
tar xvzf cd-hit.tar.gz
tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH

gzip -d refseq_algae/fastas/*
gzip -d refseq_algae/gffs/*

for file in refseq_algae/gffs/*;do name=$(basename $file);awk '!seen[$9]++' $file > nodups_$name; gt gff3 -sort yes -tidy -retainids -o sorted_$name nodups_$name; done

for file in refseq_algae/fastas/*; do name=$(basename $file .fna); gt extractfeat -type CDS -seqid yes -retainids yes -seqfile refseq_algae/fastas/$name.fna -matchdescstart sorted_$name.gff >  CDS_$name.fna; done

cat CDS*fna > all_CDS.fna

./cd-hit-v4.6.8-2017-0621/cd-hit-est -i all_CDS.fna -o nr_test.fna -c 0.95


```

Pre-transpliced gene removal line counts:
 94633 sorted_GCF_000149405.2_ASM14940v2_genomic.gff
    68845 sorted_GCF_000150955.2_ASM15095v2_genomic.gff
    92188 sorted_GCF_000186865.1_v_1.0_genomic.gff
    36150 sorted_GCF_000240725.1_ASM24072v1_genomic.gff
   395430 sorted_GCF_000315625.1_Guith1_genomic.gff
   419862 sorted_GCF_000372725.1_Emiliana_huxleyi_CCMP1516_main_genome_assembly_v1.0_genomic.gff
   
Post-transpliced gene removal line counts:
77439 sorted_GCF_000149405.2_ASM14940v2_genomic.gff
    61087 sorted_GCF_000150955.2_ASM15095v2_genomic.gff
    76305 sorted_GCF_000186865.1_v_1.0_genomic.gff
    29212 sorted_GCF_000240725.1_ASM24072v1_genomic.gff
   263340 sorted_GCF_000315625.1_Guith1_genomic.gff
   317935 sorted_GCF_000372725.1_Emiliana_huxleyi_CCMP1516_main_genome_assembly_v1.0_genomic.gff
   
I also get errors about CDS regions being in the wrong phase.

Still some duplicates... what gives? Actually what does it matter if I have duplicates from the same gene? I'll pull the same info down for each one anyway. I just have to watch out for getting more than one grep hit.

```{bash, eval = F}
tar xvzf refseq_algae.tar.gz
tar xvzf cd-hit.tar.gz
tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH

gzip -d refseq_algae/fastas/*
gzip -d refseq_algae/gffs/*

for file in refseq_algae/gffs/*;do name=$(basename $file);gt gff3 -sort yes -tidy -retainids -o sorted_$name $file; done

for file in refseq_algae/fastas/*; do name=$(basename $file .fna); gt extractfeat -type CDS -seqid yes -retainids yes -seqfile refseq_algae/fastas/$name.fna -matchdescstart sorted_$name.gff >  CDS_$name.fna; done

cat CDS*fna > all_CDS.fna

./cd-hit-v4.6.8-2017-0621/cd-hit-est -i all_CDS.fna -o nr_test.fna -c 0.95

# Make sure the gff stuff I've written so far works, too
grep ">" nr_test.fna > fasta_headers.txt
sed -e 's/>//g' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
#awk '{print $1}' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
#instead of removing 2nd field, just remove the space
sed -e 's/ /_/g' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
sort fasta_headers.txt | uniq -cd

cat fasta_headers.txt | while read line; do hit=`grep -n $line nr_test.fna.clstr | awk -F: '{print $1}'`; head -n $hit nr_test.fna.clstr > splitfile.clstr; cluster=`grep "Cluster" splitfile.clstr | tail -1`; echo $cluster; done > clusters.txt
#remove the carrot and the space
#sed -e 's/>//g' clusters.txt > temp.txt && mv temp.txt clusters.txt
#sed -e 's/ //g' clusters.txt > temp.txt && mv temp.txt clusters.txt

cat fasta_headers.txt | while read line; do length=`grep $line nr_test.fna.clstr | awk '{print $2}' | sed 's/[^0-9]*//g'`; echo $length; done > endpoint.txt

```

Problem #2. Not only do I have duplicate samples within a single genome... the genes in each genome are named the same thing. This is dumb. Maybe I will have to go back to renaming CDS regions, with the genome this time, too. Could I tag on the location and genome to each ID?

```{bash, eval = F}
#Strip off the hashtag lines and save for later
grep "##\|#!" refseq_algae/gffs/GCF_000149405.2_ASM14940v2_genomic.gff > top_of_file.txt
grep -v "##\|#!" refseq_algae/gffs/GCF_000149405.2_ASM14940v2_genomic.gff > bottom_of_file.txt

#Take bits and pieces of columns as needed
awk '{print $1,$2,$3,$4,$5,$6,$7,$8}' bottom_of_file.txt > part1.txt
awk '{print $9}' bottom_of_file.txt > part2.txt
awk -F ";" '{print $1}' part2.txt > IDs.txt
awk -F ";" '{print $2,$3,$4,$5,$6,$7,$8,$9,$10}' part2.txt > tags.txt
awk '{print $1}' bottom_of_file.txt > genome.txt
awk '{print $4}' bottom_of_file.txt > start.txt


#Put them back together - I want them in this order:
# part1, IDs.txt, ".", genome.txt, ".", start.txt, ";", tags.txt

paste part1.txt IDs.txt > cat1.txt
paste -d "." cat1.txt genome.txt > cat2.txt
paste -d "." cat2.txt start.txt > cat3.txt
paste -d ";" cat3.txt tags.txt > newbottom.txt

cat top_of_file.txt newbottom.txt > new.gff.txt

# Write a loop to do this for all the gffs: 
# Use - instead of . since there's a . in the genome names

tar xvzf refseq_algae.tar.gz
gzip -d refseq_algae/fastas/*
gzip -d refseq_algae/gffs/*

for file in refseq_algae/gffs/*;
  do name=$(basename $file);
  grep "##\|#!" $file > top_of_file.txt; grep -v "##\|#!" $file > bottom_of_file.txt;
  awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8}' bottom_of_file.txt > part1.txt;
  awk '{print $9}' bottom_of_file.txt > part2.txt;
  awk -F ";" '{print $1}' part2.txt > IDs.txt;
  awk -F "product=" '{print $2}' part2.txt > gene.txt;
  awk '{print $1}' bottom_of_file.txt > genome.txt;
  awk '{print $4}' bottom_of_file.txt > start.txt;
  paste -d "\t'" part1.txt IDs.txt > cat1.txt;
  paste -d "-" cat1.txt start.txt > cat2.txt;
  paste -d "-" cat2.txt genome.txt > cat3.txt;
  paste -d "-" cat3.txt gene.txt > newbottom.txt;
  cat top_of_file.txt newbottom.txt > new.gff.txt;
  gt gff3 -sort yes -tidy -retainids -o sorted_$name new.gff.txt;
done

#alright, I can get it working but only if I leave a lot of the crap behind - doesn't like not being able to find the parent gene for each cds
#let's see if the rest of the workflow works

for file in refseq_algae/fastas/*; do name=$(basename $file .fna); gt extractfeat -type CDS -seqid yes -retainids yes -seqfile refseq_algae/fastas/$name.fna -matchdescstart sorted_$name.gff >  CDS_$name.fna; done

cat CDS*fna > all_CDS.fna

./cd-hit-v4.6.8-2017-0621/cd-hit-est -i all_CDS.fna -o nr_test.fna -c 0.95

# Make sure the gff stuff I've written so far works, too
grep ">" nr_test.fna > fasta_headers.txt
sed -e 's/>//g' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
awk '{print $1}' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
#instead of removing 2nd field, just remove the space
#sed -e 's/ /_/g' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
sort fasta_headers.txt | uniq -cd

cat fasta_headers.txt | while read line; do var=$(echo $line | cut -c1-19); hit=`grep -n $var nr_test.fna.clstr | awk -F: '{print $1}'`; head -n $hit nr_test.fna.clstr > splitfile.clstr; cluster=`grep "Cluster" splitfile.clstr | tail -1`; echo $cluster; done > clusters.txt
#remove the carrot and the space
#sed -e 's/>//g' clusters.txt > temp.txt && mv temp.txt clusters.txt
#sed -e 's/ //g' clusters.txt > temp.txt && mv temp.txt clusters.txt

cat fasta_headers.txt | while read line; do length=`grep $line nr_test.fna.clstr | awk '{print $2}' | sed 's/[^0-9]*//g'`; echo $length; done > endpoint.txt

```

*shakes fist at gods of bioinformatics* so close! The problem is that cd-hit only outputs the first 19 characters of my IDs, which is not enough to be unique. Maybe I can switch the gene location to be ahead of the genome name to get around that?

That works!

####2017-08-10

With such success on the algal part, I'd like to modify the gene names of my MAGs and SAGs. Trina suggested this to make it easier to parse relevant information once I've counted mapped reads, rather than running grep every time I need a product name.

Well poop. My gff files are gone and I will need to re-upload them. which is fine, except I've still got 2 days left on my pre-rRNA sorting transfer and I'll need another 2 to transfer the metagenomes - which brings me to problem #2. So I've been busting my chops trying to get metabat running so that I can classify contigs based on their bin classifications. But we've always done that using phylosift, and I can't get phylosift working. So in the meantime, maybe I can work on finding a new classification method that works better in high throughput (and apply that to my other project, too!)

####2017-08-11

Here's the plan. I'm going to go into CHTC office hours next Tuesday and see if we can figure out the Metabat thing. I'm also going to considering discussing MAG building and classification for my lab meeting topic this semester - it seems like something other people in the lab have done and will need to start doing differently now that our lab server is gone, so that may be useful for me and everyone else. I haven't found a good alternative to Phylosift for classification yet.

Meanwhile, I'm going to focus my attention on the building the non-redundant database with the information I have currently. Later, I'll improve on the JGI classifications and add in the SAGs that are still in the pipeline. 1st up, add phylogeny to my ref MAGs and SAGs.

I'm also thinking about writing this up to work on one file at a time, then storing the CDS extracted fastas for a separate job of just the cd-hit step.

```{bash, eval = F}

tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH


for file in *gff;
  do name=$(basename $file);
  grep "##\|#!" $file > top_of_file.txt; grep -v "##\|#!" $file > bottom_of_file.txt; #take headers off the top of the file
  awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8}' bottom_of_file.txt > part1.txt; #split by first 8 columns
  awk '{print $9}' bottom_of_file.txt > part2.txt; #put the last of column of tags in its own file
  awk -F ";" '{print $1}' part2.txt > IDs.txt; #take out just the ID tag
  awk -F "product=" '{print $2}' part2.txt > gene.txt; #get the product name
  #awk '{print $1}' bottom_of_file.txt > genome.txt; #get the genome name
  #print a column of $name instead (without extension)
  rows=$(wc -l bottom_of_file.txt | awk '{print $1}')
  genome=$(echo $name | cut -d'.' -f1)
  yes $genome | head -n $rows > genome.txt
  #look up phylogeny and make that column, too
  assignment=$(grep -n $genome Readme.csv | awk -F "," '{print $3,$4,$5,$6,$7,$8}' OFS=",")
  yes $assignment | head -n $rows > classification.txt
  awk '{print $4}' bottom_of_file.txt > start.txt; #get the gene start location
  paste -d "\t'" part1.txt IDs.txt > cat1.txt; #paste first 8 columns to ID tag
  paste -d "_" cat1.txt start.txt > cat2.txt; #paste start location to ID
  paste -d "_" cat2.txt genome.txt > cat3.txt; #paste genome to ID
  paste -d "_" cat3.txt classification.txt > cat4.txt; #paste phylogeny to ID
  paste -d "_" cat4.txt gene.txt > newbottom.txt; #paste product name to ID
  cat top_of_file.txt newbottom.txt > new.gff; #put the top of the file back on
  gt gff3 -sort yes -tidy -retainids -o sorted_$name new.gff; #clean up the the gff sorter
done

#so far so good! Test cd-hit now. Don't keep the genome names this time.

tar xvzf cd-hit.tar.gz

for file in 2*fna; do name=`echo "$file" | cut -d'.' -f1`;gt extractfeat -type CDS -seqid no -retainids yes -seqfile $name.fna -matchdescstart sorted_$name.gff >  CDS_$name.fna; done

cat CDS*fna > all_CDS.fna

./cd-hit-v4.6.8-2017-0621/cd-hit-est -i all_CDS.fna -o nr_test.fna -c 0.95

#Now for the complicated part - the dummy gff. Step 1, pull out the fasta headers

grep ">" nr_test.fna > fasta_headers.txt
#remove the carrot
sed -e 's/>//g' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt
#check for duplicate ids
sort fasta_headers.txt | uniq -cd
#nothing - good to go!

# Step 2, Link cluster number to that id

cat fasta_headers.txt | while read line; do var=$(echo $line | cut -c1-19); hit=`grep -n $var nr_test.fna.clstr | awk -F: '{print $1}'`; head -n $hit nr_test.fna.clstr > splitfile.clstr; cluster=`grep "Cluster" splitfile.clstr | tail -1`; echo $cluster; done > clusters.txt
#remove the carrot and the space
#sed -e 's/>//g' clusters.txt > temp.txt && mv temp.txt clusters.txt
#sed -e 's/ //g' clusters.txt > temp.txt && mv temp.txt clusters.txt

cat fasta_headers.txt | while read line; do var=$(echo $line | cut -c1-19); length=`grep $var nr_test.fna.clstr | awk '{print $2}' | sed 's/[^0-9]*//g'`; echo $length; done > endpoint.txt

#already, now the good part. Make a couple of dummy columns and put the whole shebang together.
# I need:
# first line is version of gff
# genome name - something like "NR_database"
# source - cluster number
# type - CDS
# start - 1
# stop - my calculated endpoints
# strand and frame info - ., +, 0
# tags - use the fasta header as ID

rows=$(wc -l fasta_headers.txt | awk '{print $1}')
yes "NR_gene_database" | head -n $rows > genome.txt
yes "CDS" | head -n $rows > type.txt
yes "1" | head -n $rows > start.txt
yes "." | head -n $rows > info1.txt
yes "+" | head -n $rows > info2.txt
yes "0" | head -n $rows > info3.txt
yes "ID" | head -n $rows > ID.txt

paste -d "=" ID.txt fasta_headers.txt > tags.txt
paste genome.txt clusters.txt type.txt start.txt endpoint.txt info1.txt info2.txt info3.txt tags.txt > nrdb.gff

echo '##gff-version 3' | cat - nrdb.gff > temp && mv temp nrdb.gff
gt gff3 -sort yes -tidy -retainids -o sorted_nrdb.gff nrdb.gff
```

Next question - what delimiter should I use that is not in any of the genomes, seq ids, etc? I can't use space, tab, ;, -, Maybe _ or :?

Woohoo, I fooled the gff validator! Now - do I push more genomes through to this format? Or test mapping and feature counts first? I'm going to go with push more through, since I'm still uploading my reads to map. I guess first I should process the metagenome assemblies (JGI classifications and all) though.

algae classifications first:
```{bash, eval = F}
# I made a file called algae_phylogeny.txt in refseq_algae that has file names as col1 and comma separated taxonomic classifications in col2.

for file in refseq_algae/gffs/*;
  do name=$(basename $file);
  grep "##\|#!" $file > top_of_file.txt; grep -v "##\|#!" $file > bottom_of_file.txt;
  awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8}' bottom_of_file.txt > part1.txt;
  awk '{print $9}' bottom_of_file.txt > part2.txt;
  awk -F ";" '{print $1}' part2.txt > IDs.txt;
  awk -F "product=" '{print $2}' part2.txt > gene.txt;
  awk '{print $1}' bottom_of_file.txt > genome.txt;
  awk '{print $4}' bottom_of_file.txt > start.txt;
  #look up phylogeny and make that column, too
  rows=$(wc -l bottom_of_file.txt | awk '{print $1}')
  genome=$(echo $name | cut -d'.' -f1)
  assignment=$(grep -n $genome refseq_algae/algae_phylogeny.txt | awk '{print $2}')
  yes $assignment | head -n $rows > classification.txt
  paste -d "\t'" part1.txt IDs.txt > cat1.txt;
  paste -d "_" cat1.txt start.txt > cat2.txt;
  paste -d "_" cat2.txt genome.txt > cat3.txt;
  paste -d "_" cat3.txt classification.txt > cat4.txt; #paste phylogeny to ID
  paste -d "_" cat4.txt gene.txt > newbottom.txt;
  cat top_of_file.txt newbottom.txt > new.gff.txt;
  gt gff3 -sort yes -tidy -retainids -o sorted_$name new.gff.txt;
done
```

Ok, will test that in a bit. Let's tackle the real challenge - metagenome assemblies.
I'll get these files from gluster in an interactive session.

```{bash, eval = F}
tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH

gzip -d GEODES005.assembled.fna.gz

file="GEODES005.assembled.gff"
name=$(basename $file);

  awk -F "\t" -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8}' $file > part1.txt;
  awk -F "\t" '{print $9}' $file > part2.txt;
  awk -F ";" '{print $1}' part2.txt > IDs.txt;
#get product name from separate file
# I need locustags, not ids, to look up product names
  awk -F "locus_tag=" '{print $2}' part2.txt | awk -F ";" '{print $1}' > locust.txt;
  while read line; do find=$(grep -w $line GEODES005.assembled.product_names|awk '{print $2}');  echo $find; done < locust.txt > product_names.txt
  awk '{print $1}' $file > genome.txt;
  awk -F "\t" '{print $4}' $file > start.txt;
  #look up phylogeny from phylodist
  rows=$(wc -l $file | awk '{print $1}');
  while read line; do find=$(grep -w $line GEODES005.assembled.phylodist|awk '{print $5}');  echo $find; done < genome.txt > classifications.txt
  #######
  assignment=$(grep -n $genome refseq_algae/algae_phylogeny.txt | awk '{print $2}')
  yes $assignment | head -n $rows > classification.txt
  
  paste -d "\t'" part1.txt IDs.txt > cat1.txt;
  paste -d "_" cat1.txt start.txt > cat2.txt;
  paste -d "_" cat2.txt genome.txt > cat3.txt;
  paste -d "_" cat3.txt classification.txt > cat4.txt; #paste phylogeny to ID
  paste -d "_" cat4.txt gene.txt > newbottom.txt;
  cat top_of_file.txt newbottom.txt > new.gff.txt;
  gt gff3 -sort yes -tidy -retainids -o sorted_$name new.gff.txt

```

I stopped at the ##### after that step ran for 5 hours.... it finished 60,000 out of 4 million. Yikes. My options seem to be either split it up, or wait until after the NR bit and hope there are fewer genes to grep, or both. I'll sleep on this.

####2017-08-14

My two goals for the day: 
1. try the assembled contig gff's again. According to the internet, I can specify grep -F to match plain text instead of regex characters, which should speed it up significantly. I'm also considering a two step grep, where I make a smaller file of unique contigs and then look for each gene's assignment in that file instead of the full phylodist.
2. Get a mapping index built for one of the metagenome assemblies. I wrote a job for this but it keeps saying it can't find things.

Boot up an interactive job and run the code below until the #### (where I stopped last week)
```{bash, eval = F}
tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH
cp /mnt/gluster/amlinz/metagenome_assemblies/fastas/GEODES005.assembled.fna.gz .
cp /mnt/gluster/amlinz/metagenome_assemblies/gff/GEODES005.assembled.gff .
cp /mnt/gluster/amlinz/metagenome_assemblies/product_names/GEODES005.assembled.product_names .
cp /mnt/gluster/amlinz/metagenome_assemblies/phylogeny/GEODES005.assembled.phylodist .
gzip -d GEODES005.assembled.fna.gz

file="GEODES005.assembled.gff"
name=$(basename $file);

  awk -F "\t" -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8}' $file > part1.txt;
  awk -F "\t" '{print $9}' $file > part2.txt;
  awk -F ";" '{print $1}' part2.txt > IDs.txt;
#get product name from separate file
# I need locustags, not ids, to look up product names
  awk -F "locus_tag=" '{print $2}' part2.txt | awk -F ";" '{print $1}' > locust.txt;
  while read line; do find=$(grep -wF $line GEODES005.assembled.product_names|awk '{print $2}');  echo $find; done < locust.txt > product_names.txt
  awk '{print $1}' $file > genome.txt;
  awk -F "\t" '{print $4}' $file > start.txt;
  
  uniq genome.txt > uniq_genomes.txt
  #look up phylogeny from phylodist
  rows=$(wc -l $file | awk '{print $1}');
  while read line; do find=$(grep -wF $line GEODES005.assembled.phylodist|awk '{print $5}');  echo $find; done < genome.txt > classifications.txt
  #######
  assignment=$(grep -n $genome refseq_algae/algae_phylogeny.txt | awk '{print $2}')
  yes $assignment | head -n $rows > classification.txt
  
  paste -d "\t'" part1.txt IDs.txt > cat1.txt;
  paste -d "_" cat1.txt start.txt > cat2.txt;
  paste -d "_" cat2.txt genome.txt > cat3.txt;
  paste -d "_" cat3.txt classification.txt > cat4.txt; #paste phylogeny to ID
  paste -d "_" cat4.txt gene.txt > newbottom.txt;
  cat top_of_file.txt newbottom.txt > new.gff.txt;
  gt gff3 -sort yes -tidy -retainids -o sorted_$name new.gff.txt

```

Even with the -F and running all afternoon, still only 100,000 out of 4 million done. Can I split this up in parallel? My 10,000 jobs limit still gives me 400 lines per job, which would be quite maneageable.

Meanwhile, run an interactive job of the index building:
```{bash, eval = F}

#!/bin/bash
#Build a re-usable mapping index
cp /mnt/gluster/amlinz/metagenome_assemblies/fastas/GEODES005.assembled.fna.gz .
gzip -d GEODES005.assembled.fna.gz

#Unzip bbmap and build the index
tar -xvzf BBMap_36.99.tar.gz
bbmap/bbmap.sh ref=GEODES005.assembled.fna usemodulo=T -Xmx30g

# Make ref/ a tarball and move to gluster
tar czvf GEODES005_assembled_index.tar.gz ref/

cp GEODES005_assembled_index.tar.gz /mnt/gluster/amlinz/
rm *.tar.gz
rm -r bbmap/
rm -r ref/
rm *fna
```

That worked fine. I forgot to copy the output to gluster (head-keyboard). Now I can move on to the actual mapping.

mapping_metaGs.sh
```{bash, eval = F}
#!/bin/bash
#Map metatranscriptome reads to my pre-indexed database of reference genomes
#Transfer metaT from gluster
#Not splitting the metaTs anymore
cp /mnt/gluster/amlinz/metagenomes/GEODES005.metagenome.fastq.gz .
cp /mnt/gluster/amlinz/GEODES005_assembled_index.tar.gz .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar -xvf samtools.tar.gz
tar -xvzf GEODES005_assembled_index.tar.gz
gzip -d  GEODES005.metagenome.fastq.gz
mv GEODES005.metagenome.fastq GEODES005_metagenome.fastq
name=$(basename GEODES005_metagenome.fastq | cut -d'.' -f1)
sed -i '/^$/d' $name.fastq

#Run the mapping step
bbmap/bbmap.sh in=$name.fastq out=$name.mapped.sam minid=0.8 trd=T sam=1.3 threads=1 build=1 usemodulo=T mappedonly=T -Xmx30g

# I want to store the output as bam. Use samtools to convert.
samtools view -b -S -o $name.mapped.bam $name.mapped.sam

#Copy bam file back to gluster
cp $name.mapped.bam /mnt/gluster/amlinz/

#Clean up
rm -r bbmap
rm -r ref
rm *.bam
rm *.sam
rm *.fastq
rm *.gz
```

mapping_metaGs.sub
```{bash, eval = F}

# mapping_metaGs.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = mapping_metaGs_$(Cluster).log
error = mapping_metaGs_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/mapping_metaGs.sh
arguments = $(samplename)
output = mapping_metaGs_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz,zipped/samtools.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory =30GB
request_disk = 30GB
#
# 
queue 

```

#### 2017-08-17

Busy couple days! I had my committee meeting and I chatted with Sarah about mapping, binning, and classifying things. My committee strongly feels that trying to map and bin these genomes is not worth the time for the benefit to this project when I already have classifications. But Trina still feels that it would be beneficial to somewhat improve these classifications. Sarah has also run into this issue and her solution is to take the consensus taxonomy from the best BLAST hit of each gene to the IMG database, which JGI conveniently outputs. I'm hoping to use Sarah's Python script for this, so I first need to update my python tarball to include the pandas package.

```{bash, eval = F}
mkdir python
tar -xvf Python-2.7.13.tgz
cd Python-2.7.13
./configure --prefix=$(pwd)/../python
make
make install
cd ..
ls python
ls python/bin

export PATH=$(pwd)/python/bin:$PATH
wget https://bootstrap.pypa.io/get-pip.py
python get-pip.py
pip install numpy
pip install matplotlib
pip install htseq
pip install pysam
pip install pandas
tar -czvf python.tar.gz python/
exit

```

Looks good. Now to write a submit and executable to run Sarah's script. I'll try in interactive mode first.

```{bash, eval = F}
tar xvzf python.tar.gz
#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$(pwd)/bwa:$PATH
export HOME=$(pwd)/home
```

No dice. Sarah is working on modifying her script to work more generally, so I'll give that a try when it's ready.

####2017-08-18

Meanwhile, I'll start high-throughputting the other types of reference that need to be processed. MAGs_SAGs first.

03refMAGs_SAGs.sub
```{bash, eval=F}
# 03refMAGs_SAGs.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03refMAGs_SAGs_$(Cluster).log
error = 03refMAGs_SAGs_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = /home/amlinz/executables/03refMAGs_SAGs.sh
arguments = $(samplename)
output = 03refMAGs_SAGs_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/genometools.tar.gz,ref_MAGs_SAGs/fastas/$samplename.fna,ref_MAGs_SAGs/gffs/$samplename.gff,ref_MAGs_SAGs/Readme.csv
transfer_output_files = CDS_$1.fna
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
# Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 4GB
request_disk = 2GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from /home/amlinz/refMAGS_SAGs_list.txt
```

03refMAGs_SAGs.sh
```{bash, eval = F}
#!/bin/bash
#Rename genes to include relevant info from the gff file, then extract a fasta of only coding regions
tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH

grep "##\|#!" $1.gff > top_of_file.txt
grep -v "##\|#!" $1.gff > bottom_of_file.txt #take headers off the top of the file
awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8}' bottom_of_file.txt > part1.txt #split by first 8 columns
awk '{print $9}' bottom_of_file.txt > part2.txt #put the last of column of tags in its own file
awk -F ";" '{print $1}' part2.txt > IDs.txt #take out just the ID tag
awk -F "product=" '{print $2}' part2.txt > gene.txt #get the product name
rows=$(wc -l bottom_of_file.txt | awk '{print $1}')
yes $1 | head -n $rows > genome.txt
#look up phylogeny and make that column, too
assignment=$(grep -n $1 Readme.csv | awk -F "," '{print $3,$4,$5,$6,$7,$8}' OFS=",")
yes $assignment | head -n $rows > classification.txt
awk '{print $4}' bottom_of_file.txt > start.txt #get the gene start location
paste -d "\t'" part1.txt IDs.txt > cat1.txt #paste first 8 columns to ID tag
paste -d "_" cat1.txt start.txt > cat2.txt #paste start location to ID
paste -d "_" cat2.txt genome.txt > cat3.txt #paste genome to ID
paste -d "_" cat3.txt classification.txt > cat4.txt #paste phylogeny to ID
paste -d "_" cat4.txt gene.txt > newbottom.txt #paste product name to ID
cat top_of_file.txt newbottom.txt > new.gff #put the top of the file back on
gt gff3 -sort yes -tidy -retainids -o sorted_$1.gff new.gff #clean up the the gff sorter


gt extractfeat -type CDS -seqid no -retainids yes -seqfile $1.fna -matchdescstart sorted_$1.gff >  CDS_$1.fna

rm *tar.gz
rm -r genometools
rm *fna*
rm *txt
rm *gff

```


A script to make the list of files to run:
refMAGs_SAGs_list.sh
```{bash, eval = F}
#!/bin/bash

for file in ref_MAGs_SAGs/fastas/*;do name=$(basename $file | cut -d'.' -f1); echo $name; done > refMAGs_SAGs_list.txt

#For testing, uncomment the following lines:
head -2 refMAGs_SAGs_list.txt > temp.txt
mv temp.txt refMAGs_SAGs_list.txt
```

A script to move the output into its own folder:
move_CDS_output.sh
```{bash, eval = F}
#!/bin/bash

mv CDS*fna > CDS_fastas
```

And my first ever DAG!
02nonredundant_db.DAG:
```{bash, eval = F}
JOB 03refMAGs_SAGs /home/amlinz/submits/03refMAGs_SAGs.sub
SCRIPT PRE 03refMAGs_SAGs /home/amlinz/scripts/refMAGs_SAGs_list.sh
SCRIPT POST 03refMAGs_SAGs /home/amlinz/scripts/move_CDS_output.sh
```

Time to test! I'm going to rearrange my homefolder to archive old scripts and prepare to move over to DAGs. Submit with
```{bash, eval = F}
condor_submit_dag DAGs/02nonredundant_db.DAG
```

It's alive! A few things of note:
 - use absolute paths, otherwise I get horribly confused
 - all queued filenames in the submit file should be surrounded by $()
 - The command "yes" needs to be killed by the script in remote sessions, which produces an error that can be ignored.
 - make sure to run chmod +x on the pre and post scripts
 
Now add a submit and execute for the refseq algae.

04algae.sub:
```{bash, eval = F}
# 04algae.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 04algae_$(Cluster).log
error = 04algae_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = /home/amlinz/executables/04algae.sh
arguments = $(samplename)
output = 04algae_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/genometools.tar.gz,refseq_algae/fastas/$(samplename).fna.gz,ref_MAGs_SAGs/gffs/$(samplename).gff.gz,refseq_algae/algae_phylogeny.txt
transfer_output_files = CDS_$(samplename).fna
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
# Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 4GB
request_disk = 2GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from /home/amlinz/algae_list.txt
```

algae_list.sh
```{bash, eval = F}
#!/bin/bash

for file in refseq_algae/fastas/*;do name=$(basename $file .fna.gz); echo $name; done > algae_list.txt

#For testing, uncomment the following lines:
head -2 algae_list.txt > temp.txt
mv temp.txt algae_list.txt
```

04algae.sh
```{bash, eval = F}
#!/bin/bash
#Rename genes to include relevant info from the gff file, then extract a fasta of only coding regions
tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH

gzip -d *gz

grep "##\|#!" $1.gff > top_of_file.txt; grep -v "##\|#!" $1.gff > bottom_of_file.txt;
awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8}' bottom_of_file.txt > part1.txt;
awk '{print $9}' bottom_of_file.txt > part2.txt;
awk -F ";" '{print $1}' part2.txt > IDs.txt;
awk -F "product=" '{print $2}' part2.txt > gene.txt;
awk '{print $1}' bottom_of_file.txt > genome.txt;
awk '{print $4}' bottom_of_file.txt > start.txt;
#look up phylogeny and make that column, too
rows=$(wc -l bottom_of_file.txt | awk '{print $1}')
assignment=$(grep -n $1 algae_phylogeny.txt | awk '{print $2}')
yes $assignment | head -n $rows > classification.txt
paste -d "\t'" part1.txt IDs.txt > cat1.txt;
paste -d "_" cat1.txt start.txt > cat2.txt;
paste -d "_" cat2.txt genome.txt > cat3.txt;
paste -d "_" cat3.txt classification.txt > cat4.txt; #paste phylogeny to ID
paste -d "_" cat4.txt gene.txt > newbottom.txt;
cat top_of_file.txt newbottom.txt > new.gff;
gt gff3 -sort yes -tidy -retainids -o sorted_$1.gff new.gff;


gt extractfeat -type CDS -seqid no -retainids yes -seqfile $1.fna -matchdescstart sorted_$1.gff >  CDS_$1.fna

rm *tar
rm -r genometools
rm *fna*
rm *txt
rm *gff


```

Update the DAG:
```{bash, eval = F}

JOB 03refMAGs_SAGs /home/amlinz/submits/03refMAGs_SAGs.sub
JOB 04algae /home/amlinz/submits/04algae.sub
SCRIPT PRE 03refMAGs_SAGs /home/amlinz/scripts/refMAGs_SAGs_list.sh
SCRIPT PRE 04algae /home/amlinz/scripts/algae_list.sh
SCRIPT POST 03refMAGs_SAGs 04algae /home/amlinz/scripts/move_CDS_output.sh
```

And add a line to the post script to concatenate CDS_fasta into a single file on gluster.
```{bash, eval = F}

#!/bin/bash

mv /home/amlinz/CDS*.fna /home/amlinz/CDS_fastas
cat /home/amlinz/CDS_fastas/* > /mnt/gluster/amlinz/CDS_regions.fna
gzip /mnt/gluster/amlinz/CDS_regions.fna

```

Got the jobs to actually run, but the output fna files are empty. And the last script doesn't seem to be running. Looks like it can't be attached to two scripts, so I'll just run it manually after, or tell the algae to run after mags. Booting into interactive mode for the empty file issue.

Left a $file in the first grep line. Also realized I should add rm commands.

And then take out the rm *fna* one because it's deleting my output. But now it's working! Time to add CD-HIT into the equation.

05cd-hit.sh
```{bash, eval = F}
#!/bin/bash
#Cluster coding regions to get nonredundant genes and make a dummy gff file to go with it

tar xvzf cd-hit.tar.gz
tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH

cp /mnt/gluster/amlinz/CDS_regions.fna.gz .
gzip -d CDS_regions.fna.gz

./cd-hit-v4.6.8-2017-0621/cd-hit-est -i CDS_regions.fna -o nonredundant_database.fna -c 0.95

grep ">" nonredundant_database.fna > fasta_headers.txt
#remove the carrot
sed -e 's/>//g' fasta_headers.txt > temp.txt && mv temp.txt fasta_headers.txt

# Step 2, Link cluster number to that id
cat fasta_headers.txt | while read line; do var=$(echo $line | cut -c1-19); hit=`grep -n $var nonredundant_database.fna.clstr | awk -F: '{print $1}'`; head -n $hit nonredundant_database.fna.clstr > splitfile.clstr; cluster=`grep "Cluster" splitfile.clstr | tail -1`; echo $cluster; done > clusters.txt

cat fasta_headers.txt | while read line; do var=$(echo $line | cut -c1-19); length=`grep $var nonredundant_database.fna.clstr | awk '{print $2}' | sed 's/[^0-9]*//g'`; echo $length; done > endpoint.txt

#already, now the good part. Make a couple of dummy columns and put the whole shebang together.
# I need:
# first line is version of gff
# genome name - something like "NR_database"
# source - cluster number
# type - CDS
# start - 1
# stop - my calculated endpoints
# strand and frame info - ., +, 0
# tags - use the fasta header as ID

rows=$(wc -l fasta_headers.txt | awk '{print $1}')
yes "NR_gene_database" | head -n $rows > genome.txt
yes "CDS" | head -n $rows > type.txt
yes "1" | head -n $rows > start.txt
yes "." | head -n $rows > info1.txt
yes "+" | head -n $rows > info2.txt
yes "0" | head -n $rows > info3.txt
yes "ID" | head -n $rows > ID.txt

paste -d "=" ID.txt fasta_headers.txt > tags.txt
paste genome.txt clusters.txt type.txt start.txt endpoint.txt info1.txt info2.txt info3.txt tags.txt > nonredundant_database.gff

echo '##gff-version 3' | cat - nonredundant_database.gff > temp && mv temp nonredundant_database.gff
gt gff3 -sort yes -tidy -retainids -o sorted_nonredundant_database.gff nonredundant_database.gff

gzip sorted_nonredundant_database.gff
mv nonredundant_database.gff.gz /mnt/gluster/amlinz
gzip nonredundant_database.fna
mv nonredundant_database.fna.gz /mnt/gluster/amlinz

rm *tar.gz
rm *fna
rm *clstr
rm *gff
rm *txt
rm -r genometools
rm -r cd-hit-v4.6.8-2017-0621
```

05cd-hit.sub
```{bash, eval = F}
# 05cd-hit.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 05cd-hit_$(Cluster).log
error = 05cd-hit_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = /home/amlinz/executables/05cd-hit.sh
#arguments = $(samplename)
output = 05cd-hit_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/genometools.tar.gz,zipped/cd-hit.tar.gz
#transfer_output_files = CDS_$(samplename).fna
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
# Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 8GB
request_disk = 3GB
#
# run one instance
queue 
```

Works good on its own - add to DAG and run the whole thing

Forgot to set the path for genometools, so it couldn't find command gt. Try again.


####2017-08-19

```{bash, eval = F}
cp /mnt/gluster/amlinz/metagenome_assemblies/phylogeny/GEODES005.assembled.phylodist .
echo $'locus_tag\thomolog_gene_oid\thomolog_taxon_oid\tpercent_identity\tlineage' | cat - GEODES005.assembled.phylodist > temp.phylodist
mv temp.phylodist GEODES005.assembled.phylodist
python classifyWphylodist_contigs.py -pd GEODES005.assembled.phylodist -pc .70 -hm 2
 
```


####2017-08-21

Two problems - one, it's been 3 days and my cd-hit script has not yet finished. Which probably means I need to rethink the massive use of grep in that script. Two, I got Sarah's classification script working, but it only reports "NO CLASSIFICATION DUE TO LOW GENE NUMBER" or something along the lines of there's less than 2 genes on every contig, which is untrue. I'm thinking of making a mini test phylodist file to try and diagnose, so I can at least better tell Sarah what I need fixed.

In the same interactive job as before:
```{bash, eval = F}
#make a test file

grep "10000001" GEODES005.assembled.phylodist | head -5 > contig1.phylodist
grep "10000101" GEODES005.assembled.phylodist | head -5 > contig2.phylodist
grep "10001001" GEODES005.assembled.phylodist | head -5 > contig3.phylodist
echo $'locus_tag\thomolog_gene_oid\thomolog_taxon_oid\tpercent_identity\tlineage' | cat - contig1.phylodist contig2.phylodist contig3.phylodist > test.phylodist

#save to gluster, then add print statements to python script
tar xvzf python.tar.gz
#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$(pwd)/bwa:$PATH
export HOME=$(pwd)/home

#contig name getting cutoff by 1! fix and try again



```

####2017-08-22

Sort of have the phylodist script working - more on that later. Meanwhile, I'm trying to get my cd-hit script to work faster. I suspect the issue is my 4-stage grep, which in retrospect seems a bit ridiculous. I'm trying some new things in an interactive session:
```{bash, eval = F}
# to save time:
head -5000 fasta_headers.txt > test_fasta_headers.txt
# Old commands
cat test_fasta_headers.txt | while read line; do var=$(echo $line | cut -c1-19); hit=`grep -n $var nonredundant_database.fna.clstr | awk -F: '{print $1}'`; head -n $hit nonredundant_database.fna.clstr > splitfile.clstr; cluster=`grep "Cluster" splitfile.clstr | tail -1`; echo $cluster; done > clusters.txt

cat fasta_headers.txt | while read line; do var=$(echo $line | cut -c1-19); length=`grep $var nonredundant_database.fna.clstr | awk '{print $2}' | sed 's/[^0-9]*//g'`; echo $length; done > endpoint.txt

#New
#trying this to find out how many genes are in the largest cluster - then I can specify grep in a smaller range of the dataset
sort -nrk1,1 nonredundant_database.fna.clstr  | head -1
# It's 90 - let's even assume this gets riduclously large and grep 500 above
cat test_fasta_headers.txt | while read line; do var=$(echo $line | cut -c1-19); grep -B 500 $var nonredundant_database.fna.clstr > splitfile.clstr; cluster=`grep "Cluster" splitfile.clstr | tail -1`; echo $cluster; done > clusters2.txt
#Same clusters, much faster time. Hooray!
# the next step was extracting length of each gene. Can I get this from the first loop still?
touch endpoint.txt
cat test_fasta_headers.txt | while read line; do var=$(echo $line | cut -c1-19); grep -B 500 $var nonredundant_database.fna.clstr > splitfile.clstr; cluster=`grep "Cluster" splitfile.clstr | tail -1`; echo $cluster; length=`tail -1 splitfile.clstr | awk '{print $2}' | sed 's/[^0-9]*//g'`; echo $length >> endpoint.txt;done > clusters.txt

#Nice! Try with a larger file and time.
#At it's current rate (2382 fasta headers in 6 minutes), It should take 26 hours to finish the whole file. Even if I add more with the GEODES files, this should still finish under 3 days.
#Actually... can I set the grep chunk to only take the max of the cluster number?
maxsize=$(sort -nrk1,1 nonredundant_database.fna.clstr  | head -1 | cut -f1)
maxsize=$(($maxsize + 1))
touch endpoint.txt
cat fasta_headers.txt | while read line; do var=$(echo $line | cut -c1-19); grep -B $maxsize $var nonredundant_database.fna.clstr > splitfile.clstr; cluster=`grep "Cluster" splitfile.clstr | tail -1`; echo $cluster; length=`tail -1 splitfile.clstr | awk '{print $2}' | sed 's/[^0-9]*//g'`; echo $length >> endpoint.txt;done > clusters.txt

#3281 in 6 min! That's 19.3 hours. Woohoo! Even if it takes longer once the contigs are in, that's still an improvement. My work here is done (for now).
```


####2017-08-23

Alright, one last iteration of the script from Sarah. It should now read in arguments as numbers instead of as strings. I'm going to test it, make modifications as necessary, and send my changes to Sarah.

```{bash, eval = F}
#make a test file
cp /mnt/gluster/amlinz/metagenome_assemblies/phylogeny/GEODES005.assembled.phylodist .
grep "10000002" GEODES005.assembled.phylodist | head -10 > contig1.phylodist
grep "10000102" GEODES005.assembled.phylodist | head -10 > contig2.phylodist
grep "10001002" GEODES005.assembled.phylodist | head -10 > contig3.phylodist
echo $'locus_tag\thomolog_gene_oid\thomolog_taxon_oid\tpercent_identity\tlineage' | cat - contig1.phylodist contig2.phylodist contig3.phylodist > test.phylodist

#save to gluster, then add print statements to python script
tar xvzf python.tar.gz
#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$(pwd)/bwa:$PATH
export HOME=$(pwd)/home

chmod +x classifyWphylodist_contigs.py



```

####2017-09-05

Well, after much headbashing and discussion with CHTC, I think I've got my rRNA removal code running under the new 400 file gluster limit. It was a pain. It's nearly done. 

Meanwhile, I was also running the phylodist classification code, but it doesn't finish in 3 days for half the files (even if I give it gobs of RAM). Also, something's off about GEODES006 that results in no output. Looks like I'll need to split the phylodist files and classify them in bits - without splitting up any parts of contigs. To do this, I'm giving each job a list of 1500 contig names rather than a piece of the file. The executable will need to grep genes from those contigs out of the phylodist file, then classify. I'll turn off the header in output so I can simply concatenate the output.

I'm testing in GEODES006 since I suspect there's a line off somewhere that is ruining the whole file. Here's my start of splitting:
```{bash, eval = F}
# Get the contig names for each gene, which is the 1st 18 characters of the gene name
mkdir contig_lists
awk '{print substr($1,1,18)}' GEODES006.assembled.phylodist | sort | uniq > /home/amlinz/contig_lists/GEODES006-contigs.txt

# Split into bite-sized chunks
mkdir contig_lists/GEODES006
split -l 1500 -a 4 -d contig_lists/GEODES006-contigs.txt contig_lists/GEODES006/GEODES006-contigs

# Make a list of files to run - only doing a couple to test
#Modified later!
ls contig_lists | head -2 > metaG_contigs.txt

```

Modify the phylodist submit file to queue each contig name file
```{bash, eval = F}
# 02phylodist.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02phylodist_$(Cluster).log
error = 02phylodist_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = /home/amlinz/executables/02phylodist.sh
arguments = $(contigs)
output = 02phylodist_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/python.tar.gz,scripts/classifyWphylodist_contigs.py,$(contigs)
transfer_output_files = $(contigs).contig.classification.perc70.minhit3.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
# Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 40GB
request_disk = 1GB
#
# run from list
queue contigs from metaG_contigs.txt

```

Modify the executable
```{bash, eval = F}

#!/bin/bash
#Classify contigs based on their gene's USEARCH hits provided by JGI

name=$(basename $1)
mv $name $name.phylodist
#add header
echo $'locus_tag\thomolog_gene_oid\thomolog_taxon_oid\tpercent_identity\tlineage' | cat - $name.phylodist > temp.phylodist && mv temp.phylodist $name.phylodist

tar xvzf python.tar.gz
#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$PATH
export HOME=$(pwd)/home

chmod +x classifyWphylodist_contigs.py

python classifyWphylodist_contigs.py -pd $name.phylodist -pc .70 -hm 3 -conlen 18
mkdir contig_lists
mkdir contig_lists/GEODES006
mv *contig.classification.perc70.minhit3.txt $1.contig.classification.perc70.minhit3.txt

rm *phylodist
rm *py
rm -rf python/
rm -r home/
rm python.tar.gz


```

Try on just one file to start. My concern is the output file.

Nope. Tons of errors. Try an interactive file.


It's throwing errors because I tried to classify a list of contigs instead of a phylodist file from those contigs. Yikes. This is what I get for coding between talks on symposium day!


```{bash, eval = F}

#!/bin/bash
#Classify contigs based on their gene's USEARCH hits provided by JGI

name=$(basename $1)

cat $name | while read line
  do grep $line /mnt/gluster/amlinz/metagenome_assemblies/phylogeny/GEODES006.assembled.phylodist;
  done > $name.phylodist

#add header
echo $'locus_tag\thomolog_gene_oid\thomolog_taxon_oid\tpercent_identity\tlineage' | cat - $name.phylodist > temp.phylodist && mv temp.phylodist $name.phylodist

tar xvzf python.tar.gz
#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$PATH
export HOME=$(pwd)/home

chmod +x classifyWphylodist_contigs.py

python classifyWphylodist_contigs.py -pd $name.phylodist -pc .70 -hm 3 -conlen 18

rm *phylodist
rm *py
rm -rf python/
rm -r home/
rm python.tar.gz


```
 Well I think that works. Just need to rename my input file to make getting the output easier and change my variable scheme. And keep in mind that it seems as if a substantial number of contigs have no good classification.
 
 
New metaG_contig list

```{bash, eval = F}
mkdir contig_lists
cat metagenome.txt | while read line;
  do awk '{print substr($1,1,18)}' /mnt/gluster/amlinz/metagenome_assemblies/phylogeny/$line.assembled.phylodist | sort | uniq >   /home/amlinz/$line-contigs.txt;
  split -l 1500 -a 4 -d $line-contigs.txt contig_lists/$line-contigs;
done

# Make a list of files to run - only doing a couple to test
ls contig_lists > metaG_contigs.txt
```

5144 files off to the races! Here's the new submits and executables

02phylodist.sub
```{bash, eval = F}
# 02phylodist.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02phylodist_$(Cluster).log
error = 02phylodist_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = /home/amlinz/executables/02phylodist.sh
arguments = $(contigs)
output = 02phylodist_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/python.tar.gz,scripts/classifyWphylodist_contigs.py,contig_lists/$(contigs)
transfer_output_files = $(contigs).contig.classification.perc70.minhit3.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 4GB
request_disk = 1GB
#
# run from list
queue contigs from metaG_contigs.txt

```

02phylodist.sh
```{bash, eval = F}

#!/bin/bash
#Classify contigs based on their gene's USEARCH hits provided by JGI

cat $1 | while read line
  do grep $line /mnt/gluster/amlinz/metagenome_assemblies/phylogeny/GEODES006.assembled.phylodist;
  done > $1.phylodist

#add header
echo $'locus_tag\thomolog_gene_oid\thomolog_taxon_oid\tpercent_identity\tlineage' | cat - $1.phylodist > temp.phylodist && mv temp.phylodist $1.phylodist

tar xvzf python.tar.gz
#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$PATH
export HOME=$(pwd)/home

chmod +x classifyWphylodist_contigs.py

python classifyWphylodist_contigs.py -pd $1.phylodist -pc .70 -hm 3 -conlen 18

rm *phylodist
rm *py
rm -rf python/
rm -r home/
rm python.tar.gz
```

####2017-09-06

The phylodist jobs ran overnight and look good! But 1 job was held - GEODES006-contigs0873. When I was running the full files, GEODES006 kept failing and I suspected it was due to one line. Is this the issue? Will boot into interactive mode to try and get this one working.

```{bash, eval = F}
cat GEODES006-contigs0873 | while read line
  do grep $line /mnt/gluster/amlinz/metagenome_assemblies/phylogeny/GEODES006.assembled.phylodist;
  done > GEODES006-contigs0873.phylodist
  
echo $'locus_tag\thomolog_gene_oid\thomolog_taxon_oid\tpercent_identity\tlineage' | cat - GEODES006-contigs0873.phylodist > temp.phylodist && mv temp.phylodist GEODES006-contigs0873.phylodist

tar xvzf python.tar.gz
#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$PATH
export HOME=$(pwd)/home

chmod +x classifyWphylodist_contigs.py

python classifyWphylodist_contigs.py -pd GEODES006-contigs0873.phylodist -pc .70 -hm 3 -conlen 18

#Here's the error: File "classifyWphylodist_contigs.py", line 34, in <module>
#    "lineage column has too many or two few ';' seps"
# AssertionError: lineage column has too many or two few ';' seps
#Looks like the last line was cutoff - only has 6 ; fields. 
#I looked up the gene hit number - the IMG classification only has 6 fields. A strain but no species. Interesting.
#I'm going to make a dummy species field in nano of Curvibacter;sp;Curvibacter PAE-UM

```

It worked! I'm going to try copying these to gluster so I don't have to rerun. Should change this in the original file, or check that it didn't get truncated. 

Was not truncated, at least from my hard drive to CHTC. Could have happened in Globus but unlikely.

Got the new file into my home directory. Now concat by metagenome.

```{bash, eval = F}
mkdir phylodist_results
cat GEODES005-contigs*.contig.classification.perc70.minhit3.txt > phylodist_results/GEODES005.contig.classification.perc70.minhit3.txt

cat GEODES006-contigs*.contig.classification.perc70.minhit3.txt > phylodist_results/GEODES006.contig.classification.perc70.minhit3.txt

cat GEODES057-contigs*.contig.classification.perc70.minhit3.txt > phylodist_results/GEODES057.contig.classification.perc70.minhit3.txt

cat GEODES058-contigs*.contig.classification.perc70.minhit3.txt > phylodist_results/GEODES058.contig.classification.perc70.minhit3.txt

cat GEODES117-contigs*.contig.classification.perc70.minhit3.txt > phylodist_results/GEODES117.contig.classification.perc70.minhit3.txt

cat GEODES118-contigs*.contig.classification.perc70.minhit3.txt > phylodist_results/GEODES118.contig.classification.perc70.minhit3.txt


```

I'm on fire! Time to take a break to get my PCR stuff ready. Then I'll download these and run the R summary script so that I can update Trina and Sarah on the results.


####2017-09-07

Time to rename genes in the JGI files and add them to my DAG. I'll steal the code from 03ref_MAGs_SAGs and modify it, testing interactively.

05metagenomes.sh
```{bash, eval = F}
#!/bin/bash
#Rename genes to include relevant info from the gff file, then extract a fasta of only coding regions
tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH

#no top of file characters
grep -v "##\|#!" /mnt/gluster/amlinz/metagenome_assemblies/gff/$1.assembled.gff > bottom_of_file.txt #take headers off the top of the file

awk -F'\t' -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8}' bottom_of_file.txt > part1.txt #split by first 8 columns - metagenome assembly has issue with whitespace in the fields.
awk -F'\t' '{print $9}' bottom_of_file.txt > part2.txt #put the last of column of tags in its own file
awk -F ";" '{print $2}' part2.txt > locust.txt #These files have the relevant info in locustag instead of id
awk '{print substr($1,11)}' locust.txt > IDS.txt
# from product name file
while read line;
  do grep $line /mnt/gluster/amlinz/metagenome_assemblies/product_names/$1.assembled.product_names;
  done < IDS.txt > gene.txt
awk -F'\t' '{print $2}' gene.txt > product.txt

rows=$(wc -l bottom_of_file.txt | awk '{print $1}')
yes $1 | head -n $rows > genome.txt
#look up phylogeny and make that column, too

sed -i -e 's/^$/NA/' contigs.txt
while read line;
  do line2=`grep -wF "$line" GEODES005.contig.classification.perc70.minhit3.txt`;
  [ ! -z "$line2" ] && echo $line2 || echo "NA   No_classification"
  done < contigs.txt > class.txt
  
awk '{print $2}' class.txt > classification.txt

#add the ID tag back in
sed -i -e 's/^/ID=/' IDS.txt

awk -F'\t' '{print $4}' bottom_of_file.txt > start.txt #get the gene start location
paste -d "\t" part1.txt IDS.txt > cat1.txt #paste first 8 columns to ID tag
paste -d "_" cat1.txt start.txt > cat2.txt #paste start location to ID
paste -d "_" cat2.txt genome.txt > cat3.txt #paste genome to ID
paste -d "_" cat3.txt classification.txt > cat4.txt #paste phylogeny to ID
paste -d "_" cat4.txt product.txt > new.gff #paste product name to ID

#Fix the strand issue in the assemblies
awk -F'\t' -vOFS='\t' '{gsub("-1", "-", $7); gsub("1", "+", $7); print}' new.gff > f1.gff

#Add first comment line to the assemblies
echo '##gff-version 3' | cat - f1.gff > temp && mv temp f1.gff

awk -F'\t' -vOFS='\t' '{gsub(";", ":", $9); print}' f1.gff > temp && mv temp f1.gff

gt gff3 -sort yes -tidy -retainids -o sorted_$1.gff f1.gff #clean up the the gff sorter


gt extractfeat -type CDS -seqid no -retainids yes -seqfile /mnt/gluster/amlinz/metagenome_assemblies/fastas/$1.assembled.fna -matchdescstart sorted_$1.gff >  CDS_$1.fna

rm *tar.gz
rm -r genometools
rm *txt
rm *gff

```

Some notes: I also need to send over my product names and new classifications. The classifications are small enough to send with the job, the product name and gff (and for sure the fna) need to be sent from gluster.
Sarah mentioned not needing to copy things from gluster-just interact with them there-so I'll try that here.

####2017-09-10

Grepping the classifications out of my phylodist is going to take awhile, and this one I can easily split up and put back together. Will run a small chunk in the test interactive to make sure everything's working, then try splitting and concatenating. Will then need another script to format and sort the concatenate gff files.

The above script works! I do really need to change my seps in the gene names. This is really confusing. I was also thinking I can run the merge concurrent with all the other types of genomes - no reason to merge this to merge it again.

####2017-09-12

Today, I need to figure out how to split my gff files - by line number, probably - and then add the script to my DAG. I also need to check on the internal standard fasta. I think it's included in the refMAGsSAGs but I'm not 100% sure.

yep, the standard ran successfully from the refMAGSAGS folder.

Here's the script to split the metagenome gffs:
split_metagenome_gffs.sh
```{bash, eval = F}
#!/bin/bash
mkdir metaG_gffs
cat metagenome.txt | while read line;
  do ;split -l 5000 -a 4 -d /mnt/gluster/amlinz/metagenome_assemblies/gff/$line.assembled.gff metaG_gffs/;
done
#2500 should aim for about 1000 jobs per metagenome assembly, so 6000 total
# Make a list of files to run - only doing a couple to test
ls metaG_gffs > metaG_gffs.txt
```

05metagenome_assemblies.sh
```{bash, eval = F}
#!/bin/bash

tar xvzf genometools.tar.gz
export PATH=$(pwd)/genometools/bin:$PATH

#no top of file characters
grep "CDS" $1 > bottom_of_file.txt
metaG=$(echo $1 | cut -c1-9)

grep ";conf=" bottom_of_file.txt > bottom_of_file.1.txt
grep -v ";conf=" bottom_of_file.txt > bottom_of_file.2.txt
awk -F ";" '{print $4}' bottom_of_file.1.txt > locust.1.txt
awk -F ";" '{print $2}' bottom_of_file.2.txt > locust.2.txt
cat locust.1.txt locust.2.txt > locust.txt
cat bottom_of_file.1.txt bottom_of_file.2.txt > bottom_of_file.txt

awk -F'\t' -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8}' bottom_of_file.txt > part1.txt #split by first 8 columns - metagenome assembly has issue with whitespace in the fields.
awk -F'\t' '{print $9}' bottom_of_file.txt > part2.txt #put the last of column of tags in its own file
awk -F ";" '{print $2}' part2.txt > locust.txt #These files have the relevant info in locustag instead of id
awk '{print substr($1,11)}' locust.txt > IDS.txt
# from product name file
while read line;
  do grep -wF $line /mnt/gluster/amlinz/metagenome_assemblies/product_names/$metaG.assembled.product_names;
  done < IDS.txt > gene.txt
awk -F'\t' '{print $2}' gene.txt > product.txt

rows=$(wc -l bottom_of_file.txt | awk '{print $1}')
yes $metaG | head -n $rows > genome.txt
#look up phylogeny and make that column, too

while read line; do echo $line | cut -c1-18; done < IDS.txt > contigs.txt
sed -i -e 's/^$/NA/' contigs.txt
while read line;
  do line2=`grep -wF "$line" /mnt/gluster/amlinz/phylodist_results/$metaG.contig.classification.perc70.minhit3.txt`;
  [ ! -z "$line2" ] && echo $line2 || echo "NA   No_classification"
  done < contigs.txt > class.txt
  
awk '{print $2}' class.txt > classification.txt

#add the ID tag back in
sed -i -e 's/^/ID=/' IDS.txt

awk -F'\t' '{print $4}' bottom_of_file.txt > start.txt #get the gene start location
paste -d "\t" part1.txt IDS.txt > cat1.txt #paste first 8 columns to ID tag
paste -d "," cat1.txt start.txt > cat2.txt #paste start location to ID
paste -d "," cat2.txt genome.txt > cat3.txt #paste genome to ID
paste -d "," cat3.txt classification.txt > cat4.txt #paste phylogeny to ID
paste -d "," cat4.txt product.txt > new.gff #paste product name to ID

#Fix the strand issue in the assemblies
awk -F'\t' -vOFS='\t' '{gsub("-1", "-", $7); gsub("1", "+", $7); print}' new.gff > f1.gff

#Add first comment line to the assemblies
echo '##gff-version 3' | cat - f1.gff > temp && mv temp f1.gff

awk -F'\t' -vOFS='\t' '{gsub(";", ":", $9); print}' f1.gff > temp && mv temp f1.gff

gt gff3 -sort yes -tidy -retainids -o sorted_$1.gff f1.gff #clean up the the gff sorter


gt extractfeat -type CDS -seqid no -retainids yes -seqfile /mnt/gluster/amlinz/metagenome_assemblies/fastas/$metaG.assembled.fna -matchdescstart sorted_$1.gff >  CDS_$1.fna

rm *tar.gz
rm -r genometools
rm *txt
rm *gff
rm $1
```

05metagenome_assemblies.sub
```{bash, eval = F}
# 05metagenome_assemblies.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 05metagenome_assemblies_$(Cluster).log
error = 05metagenome_assemblies_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = /home/amlinz/executables/05metagenome_assemblies.sh
arguments = $(samplename)
output = 05metagenome_assemblies_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = /home/amlinz/zipped/genometools.tar.gz,/home/amlinz/metaG_gffs/$(samplename)
transfer_output_files = CDS_$(samplename).fna
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 4GB
request_disk = 2GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from /home/amlinz/testmetaG_gffs.txt


```

1st three test files worked great! One more sticking point before I start the whole DAG - the seperator values. I'd been using _ but realized there's one in the metagenome assembly names. ; is out, that's the gff sep. So is :, I switched that in for the phylogeny.

Nevermind, only 1/3 of the files finished. Why? 
One failed due to:
gt gff3: error: the multi-feature with ID "_1_GEODES0050001_No_classification_" on line 2815 in file "f1.gff" has a different sequence id than its counterpart on line 2643

and the other says:
gt gff3: error: token "ID" on line 3703 in file "f1.gff" does not contain exactly one '='

both of which resulted in sorted.gff being empty. The first problem could probably be solved by keeping only CDS regions - maybe that would solve the second as well. For reference, before grepping for CDS lines, the output of GEODES0050000 has 9854 lines and 4927 sequences.

that worked! Same gene count for 0000 as before, and the others all have genes too. Also sped it up quite nicely. Last challenge, I'm going to try commas for the sep. I think this should only potentially popup in the product names and I can deal with that. Would make loading as a csv simple. Testing on the three metagenome assembly files to see if genometools has any issue with that.

Looks good! only issue that I didn't notice before is that instead of "NO CLASSIFICATION" it just says "NO", which is actually kind of funny and gets the point across. I'll leave it!

I'm going to run this on the full metaG list just to make sure there are no more oopsies, then change _ to , in the other genome types and run the whole DAG.

Still some failing due to mismatches on lines. GEODEs0050005 is one of those - booting into interactive mode to diagnose. Looks like for some reason, some gene names are lost in the IDS tag, and when the start is the same by coincidence, gt interprets it as the same gene. So I need to fix blank lines in the IDS tag. 

There are some extra lines saying "conf=" and "gc=" in positions 2 and 3, so taht the locus tag is in position 4. These lines are CDS regions that look normal. How can I remove or skip these regions?

```{bash, eval = F}
#old awk -F ";" '{print $2}' part2.txt > locust.txt
awk -F ";" '$2 ~ /conf/ {print $4}' <part2.txt> || awk -F ";" '{print $2}' part2.txt > locust.txt

awk -F ";" '{
if ($2 == "conf"*) 
  print $4; 
else 
  print $2;}' testpart2.txt

cat testpart2.txt | while read line; do
  hit=$(echo $line | grep "conf");
  if [ -z "$hit" ]
    then
    awk -F ";" '{print $2}' $line
  else
    awk -F ";" '{print $4}' $line
  fi;
  done > testlocust.txt
  
grep ";conf=" bottom_of_file.txt > bottom_of_file.1.txt
grep -v ";conf=" bottom_of_file.txt > bottom_of_file.2.txt
awk -F ";" '{print $4}' bottom_of_file.1.txt > locust.1.txt
awk -F ";" '{print $2}' bottom_of_file.2.txt > locust.2.txt
cat locust.1.txt locust.2.txt > locust.txt
cat bottom_of_file.1.txt bottom_of_file.2.txt > bottom_of_file.txt

# re-run all following parsing to maintain order
```

New issue - too many product names. How did this happen? Some IDS must have duplicate entries in the product names file. 

Update: I forgot to include grep -w, which matches only whole words. Problem solved.