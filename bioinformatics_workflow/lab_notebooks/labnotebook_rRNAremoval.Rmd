# rRNA removal

####Goal of this analysis

To remove ribosomal reads from the metatranscriptomes. We used an rRNA removal kit before sequencing, but 50-70% of the reads in the sample are still rRNA. We're not interested in these biased reads with little information about gene expression. Luckily the samples are big enough that 30-50% of reads is still plenty to look at non-rRNA expression.

##### Approach

Starting with the quality filtered fastq files provided by the JGI through the Genome Portal, I will use Sortmerna to separate rRNA and non-rRNA reads into two separate files. I'll save both, but use the non-rRNA reads for downstream analyses.

#### A quick word on high-throughput computing

Because of the large amount of data from this project, I'm running my analyses in a high-throughput computing environment. I submit "jobs" (bits of analyses) from my "submit node" (computer where my home directory is located) and these jobs are sent off to open processors ("execute nodes") around campus. There are two files needed for this - ".sub" files state how many jobs to run, where to find the scripts, and what specs are needed. These are all stored in submits/ in my home folder. The executables, ".sh", are bash scripts that run the actual analysis, found in executables/. The more jobs I can split a task into, the more efficiently it will run - however, sometimes combining the results of many split jobs is prohibitively difficult. In some steps of this workflow, I've chosen to split each sample into many smaller files, while others I've submit one job per sample or even one job for all samples at once. This choice depends on the computational power required by each job and the ease of programming splitting and concatenation scripts.

Running my analyses as many separate jobs presents issues for installation - I need a portable, easy-to-use file I can open and immediately start running a bioinformatics program on a new computer. This means the program can't have any dependencies, and it can't go downloading something from the internet every time I want to run a job. To solve this, I've created tarballs containing a pre-installed version of a program that can be unzipped with executables and databases ready to go. This also means I can be sure that my jobs are using the same version of programs like Python!

UW-Madison Center for High-Throughput Computing (CHTC) runs on HTCondor, so all commands for submitting, checking, or removing jobs are from this program. If you want to run our workflow NOT in massive parallel, you can still use the bash (.sh) scripts. They accept one argument that is the input file name, path, or part of an input file name. So if you had one sample named "My_Favorite_Sample.fastq" you could run:

```{bash, eval = F}
./01rRNA_removal.sh /Path/To/My_Favorite_Sample.fastq
```

and receive files "My_Favorite_Sample_rRNA.fastq" and "My_Favorite_Sample_nonrRNA.fastq", as long as you had the program tarball sortmerna-2.1-linux-64.tar.gz in the same directory as 01rRNA_removal.sh. If you want to run several samples in sequence, you could use a for loop; if you have access to multiple threads, you could use a program to run a handful of samples in parallel on one computer.

Happy Coding!

## Most recent workflow. 
####Use this if you want to replicate our protocol. Updated 2017-04-25
First, a word on where all of our files are located:

- We run things through UW-Madison's Center for High Throughput Computing, specifically on submit-3.chtc.wisc.edu. I submit jobs and store short-term, small files on this submit node in /home/amlinz/
- The actual data files are still way too large to be stored on the submit node. Instead, I have a gluster account where I keep the data files - from submit-3.chtc.wisc.edu, it is located at /mnt/gluster/amlinz. This means that instead of transferring data files with my executable in the submit node, I instead wrote a line in the executable that says "go to gluster and download this file." Therefore, I can only run my programs on computers with access to gluster. Also, please note that gluster is not for long-term storage.
- Backup copies of data and results are stored on the GEODES external hard drive.

All the code below is run from my submit node, and frequently references my gluster account. Keep this in mind if you are trying to replicate the workflow! Also, if you copied the .sh and .sub files directly from this github repo, you'll need to run "dos2unix" on them to get rid of my silly Windows line endings before running.

1. The first thing we need to do is split the giant fastq files into many smaller pieces. I'm starting with the reads that went through quality control at the JGI (extension .filter-MTF.fastq), located in /mnt/gluster/amlinz/GEODES_metaT.

From the submit node:

```{bash, eval = F}
# Make a new directory for the split files
mkdir /mnt/gluster/amlinz/GEODES_metaT_split/
# Alternatively, if you've done this before, just make sure this directory is empty
# rm /mnt/gluster/amlinz/GEODES_metaT_split/*

# Make a list of samples to run
for file in /mnt/gluster/amlinz/GEODES_metaT/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > samplenames.txt

# Don't want to run all of your samples just yet? The line below will keep just the first 3 files in the list.
# head -3 samplenames.txt > test_samplenames.txt; mv test_samplenames.txt samplenames.txt

# Double check!
cat samplenames.txt

# You should have both 00split_fastq.sub and 00split_fastq.sh in your home folder. The .sub file references the .sh file.
# I've set my executable to split files into 1,000,000 reads each, but you can change that.

# Check that your submit and executable files are referencing the right places, then submit your jobs:
condor_submit submits/00split_fastq.sub

# Check status with this command:
condor_q
# For a more detailed report of computers available while your job is idle:
# condor_q -analyze
# Something stuck or not going well? Remove jobs with:
# condor_rm -all, condor_rm -amlinz or condor_rm -8222164 (job id)

# Check to see if there's anything in the error file
ls -ltr 00*.err

# Check to make sure the output is what you wanted:
ls -ltr /mnt/gluster/amlinz/GEODES_metaT_split/
find /mnt/gluster/amlinz/GEODES_metaT_split/ -type f
  
```

Here are the scripts used:
00splitfastqs.sub
```{bash, eval = F}
# 00split_fastq.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 00split_fastq_$(Cluster).log
error = 00split_fastq_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/00split_fastq.sh
arguments = $(sample)
output = 00split_fastq_$(Cluster).out
#
# No files to transfer, since it's going to interact with Gluster instead of the submit node
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2 GB
request_disk = 12 GB
#
# Submit jobs
queue sample from samplenames.txt

```

00splitfastqs.sh
```{bash, eval = F}
#!/bin/bash
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/GEODES_metaT/$1.filter-MTF.fastq.gz .
gzip -d $1.filter-MTF.fastq.gz

sed -i '/^$/d' $1.filter-MTF.fastq

maxreads=$((`wc -l < $1.filter-MTF.fastq` / 8 - 1))
startpoints=$(seq 0 500000 $maxreads)

for num in $startpoints;
  do endpoint=$(($num + 499999));
  bbmap/getreads.sh in=$1.filter-MTF.fastq id=$num-$endpoint out=$1_filter-MTF_$endpoint.fastq overwrite=T;
  done

rm $1.filter-MTF.fastq
gzip $1*
cp $1* /mnt/gluster/amlinz/GEODES_metaT_split
rm $1*
rm BBMap_36.99.tar.gz
rm -r bbmap
```


2. Sort that RNA! I'm using sortmerna-2.1-linux-64.tar.gz from http://bioinfo.lifl.fr/RNA/sortmerna/ . Make sure to have the program tarball in your home folder. I'm using all of the provided databases as my alignment references.

```{bash, eval = F}
# Make some directories to store the output, or alternatively, empty these directories as above:
mkdir /mnt/gluster/amlinz/GEODES_nonrRNA_split/
mkdir /mnt/gluster/amlinz/GEODES_rRNA_split/

# Make a file that contains the paths to all of the file parts:
find /mnt/gluster/amlinz/GEODES_metaT_split/ -type f > path2splitfastqs.txt
cat path2splitfastqs.txt

# Submit the rRNA sorting jobs
condor_submit submits/01rRNA_removal.sub

#Bonus: my jobs got stuck in limbo. Here's how CHTC Help recommend fixing.
# Check to see where the jobs are:
condor_q -run -nobatch
# My stuck files were all on the same server, so likely a problem with that execute node, not my script.
# Move stuck jobs to hold and then restart:
condor_hold amlinz
condor_release amlinz

#On my full run, I ran 7,987 jobs from 85 original fastq files. 7,986 finished in 4.5 hours. 1 got stuck and produced error to that effect.

# Check the output when all jobs have finished:
ls -ltr /mnt/gluster/amlinz/GEODES_nonrRNA/
ls -ltr /mnt/gluster/amlinz/GEODES_rRNA/
  
# Are the error files empty?
ls -ltr 01*.err
```

01rRNA_removal.sub
```{bash, eval = F}

# 01rRNA_removal.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 01rRNA_removal_$(Cluster).log
error = 01rRNA_removal_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/01rRNA_removal.sh
arguments = $(fastqfile)
output = 01rRNA_removal_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/sortmerna-2.1-linux-64.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue fastqfile from path2splitfastqs.txt


```

01rRNA_removal.sh
```{bash, eval = F}
#!/bin/bash
#Sort metatranscriptomic reads into rRNA and non-rRNA files

#Transfer the fasta file from gluster
cp $1 ./
name=$(basename $1 |cut -d'.' -f1)

#Unzip files
tar -xvf sortmerna-2.1-linux-64.tar.gz
gzip -d $name.fastq
cd sortmerna-2.1-linux-64

#Index the rRNA databases
./indexdb_rna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:\./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db

#Run the sorting program
./sortmerna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db --reads ../${name}.fastq  --fastx --aligned ${name}_rRNA --other ${name}_nonrRNA --log -v -m 1 -a 1

#Let's unpack the parameters I've chosen. --ref refers to the databases I've just indexed. --reads says use the fastq file provided. --fastx means I would like a fastq file as output. --aligned is the name for rRNA reads, --other is the name for non-rRNA reads. --log says compute statistics about the run. -v means verbose. I've left the alignment setting at the default, --best. There's a faster setting, but I'm hoping I won't need it. And finally, -m tells sortmerna it can use 3GB of RAM to hold each piece of the fastq file as it processes. Hopefully that will give it a fighting chance.
#Move the output files back to gluster
mv ${name}_rRNA.fastq /mnt/gluster/amlinz/GEODES_rRNA_split/
mv ${name}_nonrRNA.fastq /mnt/gluster/amlinz/GEODES_nonrRNA_split/

#Remove files
cd ..
rm ${name}.fastq
rm sortmerna-2.1-linux-64.tar.gz
rm -r sortmerna-2.1-linux-64

```


3. Put everything back together. This is a pretty simple program - all it does it copy files generated from the same sample, concatenate them into a single file, count the number of lines, zip it up, and send it back to gluster.

```{bash, eval = F}
# We'll be using samplenames.txt again as our reference file.

# Submit the jobs:
condor_submit submits/02cat_files.sub

# You'll get a file back in your home folder that has the number of lines in both the rRNA and nonrRNA files. Concatenate these into a single file:
cat *_rRNA_results.txt > GEODES_rRNA_ratios.txt
cat GEODES_rRNA_ratios.txt

# Check the output!
ls -ltr /mnt/gluster/amlinz/GEODES_nonrRNA_concat/
ls -ltr /mnt/gluster/amlinz/GEODES_rRNA_concat/
  
# Are the error files empty?
ls -ltr 02*.err
```

02cat_files.sub
```{bash, eval = F}

# 02cat_files.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02cat_files_$(Cluster).log
error = 02cat_files_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/02cat_files.sh
arguments = $(sample)
output = 02cat_files_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files =
#transfer_output_files = $(sample)_rRNA_results.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 10GB
request_disk = 10GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue sample from samplenames.txt

```


02cat_files.sh
```{bash, eval = F}
#!/bin/bash
#Concatenate sortmerna output

cp /mnt/gluster/amlinz/GEODES_nonrRNA_split/$1*_nonrRNA.fastq ./
cat $1*_nonrRNA.fastq > $1_nonrRNA.fastq
nonrRNAcount=$(wc -l $1_nonrRNA.fastq)
gzip $1_nonrRNA.fastq
mv $1_nonrRNA.fastq.gz /mnt/gluster/amlinz/GEODES_nonrRNA_concat/
rm *_nonrRNA.fastq

cp /mnt/gluster/amlinz/GEODES_rRNA_split/$1*_rRNA.fastq ./
cat $1*_rRNA.fastq > $1_rRNA.fastq
rRNAcount=$(wc -l $1_rRNA.fastq)
gzip $1_rRNA.fastq
mv $1_rRNA.fastq.gz /mnt/gluster/amlinz/GEODES_rRNA_concat/
rm *_rRNA.fastq

echo ${nonrRNAcount},${rRNAcount} > $1_rRNA_results.txt
```


4. Clean up after yourself. Gluster is not meant for long term storage of files! Download these somewhere else and delete the copies on gluster once you're confident in the analysis.

On my computer:
Open up WinSCP and log into submit-3.chtc.wisc.edu. Download the GEODES_rRNA_ratios.txt file to my github repo, geodes/analyses/01rRNA_removal/. Download the most recent versions of the scripts used here while you're at it. I like to take a quick look at the results in R using the following code:

```{r, echo = T, fig.width = 4, fig.height = 10}
library(ggplot2)
rRNA_ratio <- read.table("C:/Users/Alex/Desktop/geodes/analyses/01rRNA_removal/GEODES_rRNA_ratios.txt", header = F, sep = ",", colClasses = c("character"))
split1 <- strsplit(rRNA_ratio$V1, " ")
split2 <- strsplit(rRNA_ratio$V2, " ")
nonrRNA_count <- c()
rRNA_count <- c()
samplenames <- c()
for(i in 1:length(split1)){
  nonrRNA_count[i] <- split1[[i]][1]
  rRNA_count[i] <- split2[[i]][1]
  samplenames[i] <- substr(split2[[i]][2], start = 1, stop = 9)
}
nonrRNA_count <- as.numeric(nonrRNA_count)/4
rRNA_count <- as.numeric(rRNA_count)/4
percent_rRNA <- rRNA_count/(rRNA_count + nonrRNA_count) * 100
rRNA_ratios <- data.frame(samplenames, percent_rRNA)
ggplot(rRNA_ratios, aes(x = samplenames, y = percent_rRNA)) + theme_bw() + geom_bar(stat = "identity") + coord_flip()

```

On submit-3.chtc.wisc.edu:

```{bash, eval = F}
# Delete all the .log, .out, and .err files in your home directory
rm *.err
rm *.log
rm *.out

# Remove the rRNA ratios report and the sample name files
rm *.txt

```

Congratulations! You now have files of just nonrRNA reads from your metatranscriptomes, and are ready to run the next step.

### Lazy Run

You've already run the code above and worked out any errors that may arise on a few test files. Or maybe you got everything processed and then received a couple more samples to analyze. Either way, AS LONG AS YOU ARE CONFIDENT THAT THE WORKFLOW WORKS FOR YOUR SAMPLES AND PRODUCEs ABSOLUTELY ZERO ERRORS OR WARNINGS, go ahead and use the lazy run code. 

```{bash, eval = F}
rm /mnt/gluster/amlinz/GEODES_metaT_split/*
for file in /mnt/gluster/amlinz/GEODES_metaT/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > samplenames.txt
condor_submit submits/00split_fastq.sub
#Wait for all jobs to finish

rm /mnt/gluster/amlinz/GEODES_nonrRNA_split/*
rm /mnt/gluster/amlinz/GEODES_rRNA_split/*
find /mnt/gluster/amlinz/GEODES_metaT_split/ -type f > path2splitfastqs.txt
condor_submit submits/01rRNA_removal.sub
#Wait

condor_submit submits/02cat_files.sub
#Wait

cat *_rRNA_results.txt > GEODES_rRNA_ratios.txt
#Download results
rm *.err
rm *.log
rm *.out
```

# Lab Notebook

#### 2017-02-02

I've downloaded the latest precompiled Linux binaries from here: http://bioinfo.lifl.fr/RNA/sortmerna/

Now I want to know where the databases are kept and which ones come preloaded with the install. On my home folder in submit-3.chtc.wisc.edu, I entered:

```{bash, eval = F}
tar -xvf sortmerna-2.1-linux-64.tar.gz
```

They are located in sortmerna/rRNA_databases/ , and looks everything I need.

The steps my executable will need to perform are:

1. Get a fastq file from gluster (the place to hold giant files on our high throughput computing system)
2. Unzip the fastq file
3. Unzip the sortmerna program file
4. Index the databases
5. Run sortmerna
6. Transfer output files back to gluster
7. Remove all files transferred from gluster

Here's my first stab at the executable. Just running on one fastq file for now.
```{bash, eval = F}
#!/bin/bash
#Sort metatranscriptomic reads into rRNA and non-rRNA files
#Starting with one fastq file for now

#Transfer the fasta file from gluster
cp /mnt/gluster/amlinz/GEODES_metaT/GEODES001.filter-MTF.fastq.gz ./

#Unzip files
gzip -d GEODES001.filter-MTF.fastq.gz
tar -xvf sortmerna-2.1-linux-64.tar.gz

cd sortmerna-2.1-linux-64

#Index the rRNA databases
./indexdb_rna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac$
./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:\
./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:\
./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:\
./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:\
./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:\
./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:\
./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db

#Run the sorting program
./sortmerna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-1$
./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:\
./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:\
./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:\
./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:\
./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:\
./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:\
./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db\
--reads ./GEODES001.filter-MTF.fastq  --fastx --aligned GEODES001_rRNA\
--other GEODES002_nonrRNA --log -v -m 3

#Let's unpack the parameters I've chosen. --ref refers to the databases I've just indexed. --reads says use the fastq file provided. --fastx means I would like a fastq file as output. --aligned is the name for rRNA reads, --other is the name for non-rRNA reads. --log says compute statistics about the run. -v means verbose. I've left the alignment setting at the default, --best. There's a faster setting, but I'm hoping I won't need it. And finally, -m tells sortmerna it can use 3GB of RAM to hold each piece of the fastq file as it processes. Hopefully that will give it a fighting chance

#Move the output files back to gluster
mv GEODES001_rRNA.fastq /mnt/gluster/amlinz/GEODES_rRNA
mv GEODES001_nonrRNA.fastq /mnt/gluster/amlinz/GEODES_nonrRNA

#Remove files
cd ..
rm GEODES001.filter-MTF.fastq
rm GEODES001.filter-MTF.fastq.gz
rm sortmerna-2.1-linux-64.tar.gz
rm -f sortmerna-2.1-linux-64

```

Saved as 01rRNA_removal.sh
Here's its associated submit file, 01rRNA_removal.sub:

```{bash, eval = F}
# 01rRNA_removal.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 01rRNA_removal_$(Cluster).log
error = 01rRNA_removal_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 01rRNA_removal.sh
#arguments = $(fastqfile)
output = 01rRNA_removal_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
#when_to_transfer_output = ON_EXIT
transfer_input_files = sortmerna-2.1-linux-64.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 4GB
request_disk = 10GB
#
# Tell HTCondor to run every fasta file in the provided list:
#queue fastafile from metagenome_list.txt
queue 1

```
I've commented out the options I'll need later (arguments, queue)

Now the moment of truth. By the way, I'm running things on HTCondor using

```{bash, eval = F}
condor_submit 01rRNA_removal.sub
#check status
condor_q
```

Error thrown: I got the help page for sortmerna and the output files weren't generated, which the flags for that command are wrong. I also got that I can't remove a directory.

I couldn't find anything wrong with sortmerna, so I move the commands for ./indexdb and ./sortmerna onto one line, instead of / at the end of each line (like what I copied off the manual). Maybe that doesn't work in bash scripts. I also changed the removal of the directory to "rm -rf".

Error again: No help file from sortmerna, but no results either. I realized I was missing a dot in calling the reads file - it should be "--reads ../GEODESetc" instead lf "--reads ./GEODESetc". I also realized that gzip -d removes the .gz file, which is why I got the error that it didn't exist when I tried to remove it.

Next run produced nothing - no errors, no writing to the screen, no output files. I think I need to run this on Zissou to do more testing. I need to get all my files up there anyway. Will come back to this later!

#### 2017-02-03

Well, the Zissou test worked great and ran for awhile (2 hours) before I killed it. It was 1/5 done. But everything seems to be in order in the bash script. I suspect it might have died because it was too greedy for RAM - on Zissou it used nearly 40GB, despite the "-m 3" setting, while I'd only requested 4. I'll also need double the hard drive space - 10GB is enough for the input file, but not the input file + output. I'll change my request settings and try again. I'm also going to add 10 threads in sortmerna using "-a 10"
Request: 10 threads, 25GB RAM, 20GB drive space. No idea how many computers in the network meet that requirement.

#### 2017-02-06

Sucess! (ish) The first run I started last Friday was stuck in the queue for an hour, so I restarted with 5 threads, 10GB RAM, and 18GB drive space. That started right away and finished in about 11 hours. No errors were produced, and the right output ended up on Gluster.

Fun fact: check how many computers can run your job with:

```{bash, eval = F}
condor_q -analyze
```

HOWEVER. I'm going to need to run this on 108 metatranscriptomes, and there are not 108 servers that meet my requirements, and 11 hours is a long time. SortmeRNA broke up the files into nearly 1900 pieces, then ran them 5 at a time. In order to increase the number of computers meeting my requirements (and therefore increase speed), I need to split my fastq files into many smaller files. I'll do this on CHTC as well, and use Grace's script from https://github.com/dshrade1/Metagenomic-Time-Series-CHTC/blob/master/TE/preprocess/preprocess_fasta.sh
as a model. Just doing one file to test for right now.

```{bash, eval = F}
#!/bin/bash

cp /mnt/gluster/amlinz/GEODES_metaT/GEODES001.filter-MTF.fastq.gz ./
gzip -d GEODES001.filter-MTF.fastq.gz

# make a folder for each metagenome
code=`echo "$1" | cut -d'.' -f1`
mkdir ${code}

# Split by number of lines
# Each takes up four lines - @HISEQ header, forward read, "+", reverse read. So make sure my line cutoff is a multiple of 4
split -l 1000000 $1 ${code}/${code}

# Grace includes a read file in the folder with the split files, but I don't think that makes sense for using with SortmeRNA
# I'm also not going to zip up the file, because I'd just need to unzip back on gluster
cp ${code} /mnt/amlinz/gluster/GEODES_metaT_split/${code}
rm -r ${code}
rm GEODES001.filter-MTF.fastq


```

And the submit file:

```{bash, eval = F}
# 00split_fastq.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 00split_fastq_$(Cluster).log
error = 00split_fastq_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 00split_fastq.sh
#arguments = $(fastqfile)
output = 00split_fastq_$(Cluster).out
#
# No files to transfer, since it's going to interact with Gluster instead of the submit node
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files = 
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
# This gives me ~800 target computers
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2GB
request_disk = 12GB
#
# Just one job for testing
queue 1
```

I did a little tweaking of naming the output files. Here's the final version of the executable. I'm debating adding a line to add .fastq to the end of the split files, but will see if sortmerna can figure it out first.

```{bash, eval = F}

#!/bin/bash

cp /mnt/gluster/amlinz/GEODES_metaT/GEODES001.filter-MTF.fastq.gz ./
gzip -d GEODES001.filter-MTF.fastq.gz

# make a folder for each metagenome
code=`echo "GEODES001.filter-MTF.fastq" | cut -d'.' -f1`
mkdir ${code}

# Split by number of lines
split -l 1000000 GEODES001.filter-MTF.fastq ${code}/${code}

# Grace includes a read file in the folder with the split files, but I don't think that makes sense for using with SortmeRNA
# I'm also not going to zip up the file, because I'd just need to unzip back on gluster
mv ${code} /mnt/gluster/amlinz/GEODES_metaT_split/
rm GEODES001.filter-MTF.fastq
```


Now to test sortmerna on one of the split files!

New executable:
```{bash, eval = F}

#!/bin/bash
#Sort metatranscriptomic reads into rRNA and non-rRNA files
#Starting with one fastq file for now

#Transfer the fasta file from gluster
cp /mnt/gluster/amlinz/GEODES_metaT_split/GEODES001/GEODES001aa ./

#Unzip files
#gzip -d GEODES001.filter-MTF.fastq.gz
tar -xvf sortmerna-2.1-linux-64.tar.gz

cd sortmerna-2.1-linux-64

#Index the rRNA databases
./indexdb_rna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:\./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db

#Run the sorting program
./sortmerna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db --reads ../GEODES001aa  --fastx --aligned GEODES001_rRNA --other GEODES001_nonrRNA --log -v -m 3 -a 10

#Let's unpack the parameters I've chosen. --ref refers to the databases I've just indexed. --reads says use the fastq file provided. --fastx means I would like a fastq file as output. --aligned is the name for rRNA reads, --other is the name for non-rRNA reads. --log says compute statistics about the run. -v means verbose. I've left the alignment setting at the default, --best. There's a faster setting, but I'm hoping I won't need it. And finally, -m tells sortmerna it can use 3GB of RAM to hold each piece of the fastq file as it processes. Hopefully that will give it a fighting chance.
#Move the output files back to gluster
mv GEODES001aa_rRNA.fastq /mnt/gluster/amlinz/GEODES_rRNA/
mv GEODES001aa_nonrRNA.fastq /mnt/gluster/amlinz/GEODES_nonrRNA/

#Remove files
cd ..
rm GEODES001aa
rm sortmerna-2.1-linux-64.tar.gz
rm -r sortmerna-2.1-linux-64


```

Submit file is the same except changed requirements - 10 threads, 3GB RAM, 5GB drive space

Sortmerna has no problem running on the file without the fastq extension, but I'm having trouble moving the output to gluster. Moving to Zissou to find out what the output file names should be.

Turns out the error was because there was no .fastq extension.

I added:
```{bash, eval = F}
# Rename files to include .fastq extension
for file in ${code}/*;do mv "$file" "$file.fastq";done
```

Note: code will break if there's already a folder named GEODES001 in the sample place. Delete old run outputs before rerunning.

Looks like everything worked! My next goal is to run the rRNA sorting script on all the pieces of GEODES001.

#### 2017-02-07

First things first, let's make a list of the files to run:
```{bash, eval = F}
find /mnt/gluster/amlinz/GEODES_metaT_split/ -type f > path2splitfastqs.txt
```

Now modify the submit file to read from that list:
```{bash, eval = F}

# 01rRNA_removal.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 01rRNA_removal_$(Cluster).log
error = 01rRNA_removal_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 01rRNA_removal.sh
arguments = $(fastqfile)
output = 01rRNA_removal_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = sortmerna-2.1-linux-64.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue fastqfile from path2splitfastqs.txt
```

And add the $1 argument variable to the executable:

```{bash, eval = F}

#!/bin/bash
#Sort metatranscriptomic reads into rRNA and non-rRNA files

#Transfer the fasta file from gluster
cp $1 ./
name=$(basename '$1' |cut -d'.' -f1)

#Unzip files
tar -xvf sortmerna-2.1-linux-64.tar.gz

cd sortmerna-2.1-linux-64

#Index the rRNA databases
./indexdb_rna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:\./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db

#Run the sorting program
./sortmerna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db --reads ../${name}.fastq  --fastx --aligned ${name}_rRNA --other ${name}_nonrRNA --log -v -m 1 -a 1

#Let's unpack the parameters I've chosen. --ref refers to the databases I've just indexed. --reads says use the fastq file provided. --fastx means I would like a fastq file as output. --aligned is the name for rRNA reads, --other is the name for non-rRNA reads. --log says compute statistics about the run. -v means verbose. I've left the alignment setting at the default, --best. There's a faster setting, but I'm hoping I won't need it. And finally, -m tells sortmerna it can use 3GB of RAM to hold each piece of the fastq file as it processes. Hopefully that will give it a fighting chance.
#Move the output files back to gluster
mv ${name}_rRNA.fastq /mnt/gluster/amlinz/GEODES_rRNA/
mv ${name}_nonrRNA.fastq /mnt/gluster/amlinz/GEODES_nonrRNA/

#Remove files
cd ..
rm ${name}.fastq
rm sortmerna-2.1-linux-64.tar.gz
rm -r sortmerna-2.1-linux-64

```

I'm going to try running this on just three files to start:

```{bash, eval = F}
head -3 path2splitfastqs.txt > test_path2splitfastqs.txt
```

Looking good! In fact, I can drop my memory requirements down to 1GB RAM and 2GB drive space.

Now one last script - putting the files back together. I'll need to copy all of the files from each GEODES number to a server, concatenate them, and send it back to gluster.

This line makes a file of all of the sample nams:

```{bash, eval = F}
for file in /mnt/gluster/amlinz/GEODES_metaT/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > samplenames.txt

head -1 samplenames.txt > test_samplenames.txt
```

First stab at the executable:
```{bash, eval = F}
#!/bin/bash
#Concatenate sortmerna output

cp /mnt/gluster/amlinz/GEODES_nonrRNA/$1??_nonrRNA.fastq ./
cat $1??_nonrRNA.fastq > $1_nonrRNA.fastq
mv $1_nonrRNA.fastq /mnt/gluster/amlinz/GEODES_nonrRNA_concat/
rm *_nonrRNA.fastq

cp /mnt/gluster/amlinz/GEODES_rRNA/$1*??_rRNA.fastq ./
cat $1??_rRNA.fastq > $1_rRNA.fastq
mv $1_rRNA.fastq /mnt/gluster/amlinz/GEODES_rRNA_concat/
rm *_rRNA.fastq

```

And the submit file:
```{bash, eval = F}
# 02cat_files.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02cat_files_$(Cluster).log
error = 02cat_files_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 02cat_files.sh
arguments = $(sample)
output = 02cat_files_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files = 
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 10GB
request_disk = 10GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue sample from test_samplenames.txt
```

I got an error from cat saying "input file is output file". I added ?? characters to the selection of file names to specify that I want GEODES001ae_nonrRNA.fastq, not GEODES001_nonrRNA.fastq.

That worked! Now I know I said that was the last process, but there's something else I want. I want to know the ratio of nonrRNA to rRNA reads for each sample. I can add that to the above script, and also zip the files up when I'm done. I've add the output file $1_rRNA_results.txt to transfer_output in .sub.

New executable:
```{bash, eval = F}
#!/bin/bash
#Concatenate sortmerna output

cp /mnt/gluster/amlinz/GEODES_nonrRNA/$1??_nonrRNA.fastq ./
cat $1??_nonrRNA.fastq > $1_nonrRNA.fastq
nonrRNAcount=$(wc -l $1_nonrRNA.fastq)
gzip $1_nonrRNA.fastq
mv $1_nonrRNA.fastq.gz /mnt/gluster/amlinz/GEODES_nonrRNA_concat/
rm *_nonrRNA.fastq

cp /mnt/gluster/amlinz/GEODES_rRNA/$1*??_rRNA.fastq ./
cat $1??_rRNA.fastq > $1_rRNA.fastq
rRNAcount=$(wc -l $1_rRNA.fastq)
gzip $1_rRNA.fastq
mv $1_rRNA.fastq.gz /mnt/gluster/amlinz/GEODES_rRNA_concat/
rm *_rRNA.fastq

echo ${nonrRNAcount},${rRNAcount} > $1_rRNA_results.txt
```

Alright! I'm going to clean up my directories, then rerun all scripts on 3 samples. I'll document everything at the top "most recent workflow" section so that I'll be ready to go when I have all of the data downloaded (hopefully tomorrow!)

####2017-02-08

Everything great on the three samples, except for 3 samples getting stuck in infinite run time. I asked Christina at CHTC about this and she said that just happens sometimes. In that case, I can stop and restart the runs with:

```{bash, eval = F}
condor_q -run -nobatch # Check to see whate server they're on
condor_hold amlinz # Move all my jobs to holding
condor_release amlinz # Start all jobs that are in holding
```

After that it was all peachy, right up until I wiped my home folder clean with a bad rm command *sigh*. Luckily I'd downloaded all my scripts to my github repo, but I lost a couple minor changes. For posterity, here's the FINAL FINAL versions of the scripts. I'm testing these on one more sample while I'm downloading the remaining fastq files from the JGI portal.

New fun fact: Since I saved my files on a Windows machine, they got new line endings and won't run. Fix using:
```{bash, eval = F}
dos2unix *.sh
dos2unix *.sub
```
I love Windows.

00split_fastq.sh:
```{bash, eval = F}

#!/bin/bash

cp /mnt/gluster/amlinz/GEODES_metaT/$1.filter-MTF.fastq.gz ./
gzip -d $1.filter-MTF.fastq.gz

# make a folder for each metagenome
mkdir $1

# Split by number of lines
split -l 1000000 $1.filter-MTF.fastq $1/$1

# Rename files to include .fastq extension
for file in $1/*;do mv "$file" "$file.fastq";done

# Grace includes a read file in the folder with the split files, but I don't think that makes sense for using with SortmeRNA
# I'm also not going to zip up the file, because I'd just need to unzip back on gluster
mv $1 /mnt/gluster/amlinz/GEODES_metaT_split/
rm $1.filter-MTF.fastq

```

00split_fastq.sub:
```{bash,eval = F}

# 00split_fastq.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 00split_fastq_$(Cluster).log
error = 00split_fastq_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 00split_fastq.sh
arguments = $(sample)
output = 00split_fastq_$(Cluster).out
#
# No files to transfer, since it's going to interact with Gluster instead of the submit node
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files =
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2 GB
request_disk = 12 GB
#
# Just one job for testing
queue sample from samplenames.txt
```

01rRNA_removal.sh:
```{bash, eval = F}

#!/bin/bash
#Sort metatranscriptomic reads into rRNA and non-rRNA files

#Transfer the fasta file from gluster
cp $1 ./
name=$(basename $1 |cut -d'.' -f1)

#Unzip files
tar -xvf sortmerna-2.1-linux-64.tar.gz

cd sortmerna-2.1-linux-64

#Index the rRNA databases
./indexdb_rna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:\./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db

#Run the sorting program
./sortmerna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db --reads ../${name}.fastq  --fastx --aligned ${name}_rRNA --other ${name}_nonrRNA --log -v -m 1 -a 1

#Let's unpack the parameters I've chosen. --ref refers to the databases I've just indexed. --reads says use the fastq file provided. --fastx means I would like a fastq file as output. --aligned is the name for rRNA reads, --other is the name for non-rRNA reads. --log says compute statistics about the run. -v means verbose. I've left the alignment setting at the default, --best. There's a faster setting, but I'm hoping I won't need it. And finally, -m tells sortmerna it can use 3GB of RAM to hold each piece of the fastq file as it processes. Hopefully that will give it a fighting chance.
#Move the output files back to gluster
mv ${name}_rRNA.fastq /mnt/gluster/amlinz/GEODES_rRNA/
mv ${name}_nonrRNA.fastq /mnt/gluster/amlinz/GEODES_nonrRNA/

#Remove files
cd ..
rm ${name}.fastq
rm sortmerna-2.1-linux-64.tar.gz
rm -r sortmerna-2.1-linux-64

```

01rRNA_removal.sub
```{bash, eval = F}

# 01rRNA_removal.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 01rRNA_removal_$(Cluster).log
error = 01rRNA_removal_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 01rRNA_removal.sh
arguments = $(fastqfile)
output = 01rRNA_removal_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = sortmerna-2.1-linux-64.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue fastqfile from path2splitfastqs.txt
```

02cat_files.sh:
```{bash, eval = F}

#!/bin/bash
#Concatenate sortmerna output

cp /mnt/gluster/amlinz/GEODES_nonrRNA/$1??_nonrRNA.fastq ./
cat $1??_nonrRNA.fastq > $1_nonrRNA.fastq
nonrRNAcount=$(wc -l $1_nonrRNA.fastq)
gzip $1_nonrRNA.fastq
mv $1_nonrRNA.fastq.gz /mnt/gluster/amlinz/GEODES_nonrRNA_concat/
rm *_nonrRNA.fastq

cp /mnt/gluster/amlinz/GEODES_rRNA/$1*??_rRNA.fastq ./
cat $1??_rRNA.fastq > $1_rRNA.fastq
rRNAcount=$(wc -l $1_rRNA.fastq)
gzip $1_rRNA.fastq
mv $1_rRNA.fastq.gz /mnt/gluster/amlinz/GEODES_rRNA_concat/
rm *_rRNA.fastq

echo ${nonrRNAcount},${rRNAcount} > $1_rRNA_results.txt
```

02cat_files.sub
```{bash, eval = F}

# 02cat_files.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 02cat_files_$(Cluster).log
error = 02cat_files_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 02cat_files.sh
arguments = $(sample)
output = 02cat_files_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files =
#transfer_output_files = $(sample)_rRNA_results.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 10GB
request_disk = 10GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue sample from samplenames.txt
```

Everything looks ready for the full run now! I'm just waiting on the file upload to gluster. It'll take at least the rest of the night, so I'll plan on starting in the morning.

####2017-02-09

Off we go! Everything finished downloading around 10 this morning. Got the first step started at 10:35. I'm currently running on 85 samples. (The remaining 23 are still being sequenced or processed)

2 got stuck, and produced errors that there was their output already existed in gluster, but everything looks fine so I'll carry on. For reference, those two errors were from GEODES050 and GEODES154. Total time: 3 hours.

Moving on. 7897 jobs submitted - holy smokes!

####2017-02-10

7,986 finished in 4.5 hours. 1 got stuck and produced error to that effect. I restarted it this morning. I'm a bit concerned about the errors from the last step about output already existing, and that happening during the rRNA run as well. For a preliminary analysis, this doesn't matter too much, but I'll look into preventing clashing of jobs that needed a restart in the future. Is there a way to tell the "mv" command to overwrite an existing file of the same name?

A quick Google search suggests either a cp and rm combo, or using rsync. I'll keep this in mind in the future.

That last job from 01rRNA_removal finally finished! (9:30AM) I have 3 errors - one segmentation (likely the fault of a computer running the job) and two that say the fastq file has uneven lines. I'll need to look into this, too - maybe I can use a program for splitting fastq files specifically? Like the fastx-toolkit or something? Might be a question for lab meeting.

No errors on the 02cat_files script, hooray! Copying rRNA and nonrRNA zipped files over to Zissou now, and then over to my hard drive.

####2017-02-13

Happy Monday! I got everything back up on Zissou and my hard drive over the weekend. The last thing I want to do before closing out this analysis is check that my rRNA/mRNA ratios are similar to those measured by the JGI. I saved the number of lines in each file and will calculate that here.

NOTE: removed this plot after I reran the rRNA removal on 3/15. Check that entry in the lab notebook.

```{r, echo = F, fig.width = 4, fig.height = 10, eval = F}
library(ggplot2)
rRNA_ratio <- read.table("C:/Users/Alex/Desktop/geodes/analyses/01rRNA_removal/GEODES_rRNA_ratios.txt", header = F, sep = ",", colClasses = c("character"))
split1 <- strsplit(rRNA_ratio$V1, " ")
split2 <- strsplit(rRNA_ratio$V2, " ")
nonrRNA_count <- c()
rRNA_count <- c()
samplenames <- c()
for(i in 1:length(split1)){
  nonrRNA_count[i] <- split1[[i]][1]
  rRNA_count[i] <- split2[[i]][1]
  samplenames[i] <- substr(split2[[i]][2], start = 1, stop = 9)
}
nonrRNA_count <- as.numeric(nonrRNA_count)/4
rRNA_count <- as.numeric(rRNA_count)/4
percent_rRNA <- rRNA_count/(rRNA_count + nonrRNA_count) * 100
rRNA_ratios <- data.frame(samplenames, percent_rRNA)
ggplot(rRNA_ratios, aes(x = samplenames, y = percent_rRNA)) + theme_bw() + geom_bar(stat = "identity") + coord_flip()

```

####2017-03-10

I found that some of my fastq files had blank lines in the mapping step, and wrote a new splitting script using getreads.sh instead of splitting by line number. See the mapping lab notebook for details. Here's the new submit and executable files.

```{bash, eval = F}
# 00split_fastq.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 00split_fastq_$(Cluster).log
error = 00split_fastq_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/00split_fastq.sh
arguments = $(sample)
output = 00split_fastq_$(Cluster).out
#
# No files to transfer, since it's going to interact with Gluster instead of th$
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2 GB
request_disk = 12 GB
#
# Just one job for testing
queue sample from samplenames.txt


```

```{bash, eval = F}
#!/bin/bash
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/GEODES_metaT/$1.filter-MTF.fastq.gz .
gzip -d $1.filter-MTF.fastq.gz

sed -i '/^$/d' $1.filter-MTF.fastq

maxreads=$((`wc -l < $1.filter-MTF.fastq` / 8 - 1))
startpoints=$(seq 0 500000 $maxreads)

for num in $startpoints;
  do endpoint=$(($num + 499999));
  bbmap/getreads.sh in=$1.filter-MTF.fastq id=$num-$endpoint out=$1_filter-MTF_$endpoint.fastq overwrite=T;
  done

rm $1.filter-MTF.fastq 
gzip $1*
cp $1* /mnt/gluster/amlinz/GEODES_metaT_split
rm $1*
rm BBMap_36.99.tar.gz
rm -r bbmap

```

####2017-03-13

The scripts above seemed to be actually running, but not saving the output in the right place. I'm going to try it in interactive mode to work out the issue.

Got that worked out, plus a directory error in 02catfiles.sh. Going to start the full run now.

####2017-03-15

All done! The fastq splitting ran overnight on the 13th, the rRNA sorting ran during the day yesterday, and the file concatenation ran overnight last night.  Everything looks good - full files, right number of files, and no errors! I'm confident in this analysis and will move on to the mapping now.

But first, I should double check the rRNA/nonrRNA ratios.

```{r, echo = T, fig.width = 4, fig.height = 10}
library(ggplot2)
rRNA_ratio <- read.table("C:/Users/Alex/Desktop/geodes/analyses/01rRNA_removal/GEODES_rRNA_ratios.txt", header = F, sep = ",", colClasses = c("character"))
split1 <- strsplit(rRNA_ratio$V1, " ")
split2 <- strsplit(rRNA_ratio$V2, " ")
nonrRNA_count <- c()
rRNA_count <- c()
samplenames <- c()
for(i in 1:length(split1)){
  nonrRNA_count[i] <- split1[[i]][1]
  rRNA_count[i] <- split2[[i]][1]
  samplenames[i] <- substr(split2[[i]][2], start = 1, stop = 9)
}
nonrRNA_count <- as.numeric(nonrRNA_count)/4
rRNA_count <- as.numeric(rRNA_count)/4
percent_rRNA <- rRNA_count/(rRNA_count + nonrRNA_count) * 100
rRNA_ratios <- data.frame(samplenames, percent_rRNA)
ggplot(rRNA_ratios, aes(x = samplenames, y = percent_rRNA)) + theme_bw() + geom_bar(stat = "identity") + coord_flip()

```

####2017-08-17

Nothing wrong here, but I do have to re-implement my rRNA sorting to depend less on gluster, so I'm going to modify my scripts slightly and right a DAG. Already did this for the non-redundant gene database!

samplenames.sh
```{bash, eval = F}
for file in /mnt/gluster/amlinz/filtered/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > samplenames.txt

# Don't want to run all of your samples just yet? The line below will keep just the first 3 files in the list.
# head -3 samplenames.txt > test_samplenames.txt; mv test_samplenames.txt samplenames.txt

```

Here are the scripts used:
00splitfastqs.sub
```{bash, eval = F}
# 00split_fastq.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 00split_fastq_$(Cluster).log
error = 00split_fastq_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/00split_fastq.sh
arguments = $(sample)
output = 00split_fastq_$(Cluster).out
#
# No files to transfer, since it's going to interact with Gluster instead of the submit node
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz
transfer_output_files = $(sample)*.gz

#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 2 GB
request_disk = 12 GB
#
# Submit jobs
queue sample from samplenames.txt

```

00splitfastqs.sh
```{bash, eval = F}
#!/bin/bash
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/filtered/$1.filter-MTF.fastq.gz .
gzip -d $1.filter-MTF.fastq.gz

sed -i '/^$/d' $1.filter-MTF.fastq

maxreads=$((`wc -l < $1.filter-MTF.fastq` / 8 - 1))
startpoints=$(seq 0 500000 $maxreads)

for num in $startpoints;
  do endpoint=$(($num + 499999));
  bbmap/getreads.sh in=$1.filter-MTF.fastq id=$num-$endpoint out=$1-$endpoint.fastq overwrite=T;
  done

rm $1.filter-MTF.fastq
mkdir $1-splitfiles
mv *fastq $1-splitfiles
tar cvzf $1-splitfiles.tar.gz $1-splitfiles
rm BBMap_36.99.tar.gz
rm -r bbmap
```


path2splitfastqs.sh
```{bash, eval = F}
#!/bin/bash
mv GEODES*gz /home/amlinz/GEODES_metaT_split/
cd /home/amlinz/GEODES_metaT_split
ls *.tar.gz |xargs -n1 tar -xvzf
rm *.tar.gz
cd /home/amlinz/
gzip /home/amlinz/GEODES_metaT_split/*/*fastq
mv /home/amlinz/GEODES_metaT_split/*/*fastq.gz /home/amlinz/GEODES_metaT_split/
rmdir /home/amlinz/GEODES_metaT_split/*
ls /home/amlinz/GEODES_metaT_split/ > path2splitfastqs.txt

for file in /home/amlinz/GEODES_metaT_split/*gz; do name=$(basename $file |cut -d'.' -f1);echo $name; done > path2splitfastqs.txt

```

01rRNA_removal.sub
```{bash, eval = F}

# 01rRNA_removal.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 01rRNA_removal_$(Cluster).log
error = 01rRNA_removal_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/01rRNA_removal.sh
arguments = $(fastqfile)
output = 01rRNA_removal_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/sortmerna-2.1-linux-64.tar.gz,$(fastqfile)
transfer_output_files = $(fastqfile)-rRNA,$(fastqfile)-nonrRNA
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
request_cpus = 1
request_memory = 2GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue fastqfile from path2splitfastqs.txt


```

01rRNA_removal.sh
```{bash, eval = F}
#!/bin/bash
#Sort metatranscriptomic reads into rRNA and non-rRNA files

name=$(basename $1 |cut -d'.' -f1)

#Unzip files
tar -xvf sortmerna-2.1-linux-64.tar.gz
gzip -d $name.fastq
cd sortmerna-2.1-linux-64

#Index the rRNA databases
./indexdb_rna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:\./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db

#Run the sorting program
./sortmerna --ref ./rRNA_databases/silva-bac-16s-id90.fasta,./index/silva-bac-16s-db:./rRNA_databases/silva-bac-23s-id98.fasta,./index/silva-bac-23s-db:./rRNA_databases/silva-arc-16s-id95.fasta,./index/silva-arc-16s-db:./rRNA_databases/silva-arc-23s-id98.fasta,./index/silva-arc-23s-db:./rRNA_databases/silva-euk-18s-id95.fasta,./index/silva-euk-18s-db:./rRNA_databases/silva-euk-28s-id98.fasta,./index/silva-euk-28s:./rRNA_databases/rfam-5s-database-id98.fasta,./index/rfam-5s-db:./rRNA_databases/rfam-5.8s-database-id98.fasta,./index/rfam-5.8s-db --reads ../${name}.fastq  --fastx --aligned ${name}_rRNA --other ${name}_nonrRNA --log -v -m 1 -a 1


gzip *RNA.fastq
mkdir ../$1-rRNA
mkdir ../$1-nonrRNA
mv *_rRNA.fastq.gz ../$1-rRNA
mv *nonrRNA.fastq.gz ../$1-nonrRNA

#Remove files
cd ..
rm sortmerna-2.1-linux-64.tar.gz
rm -r sortmerna-2.1-linux-64

```


cat_files.sh
```{bash, eval = F}
#!/bin/bash
#Concatenate sortmerna output
mv /home/amlinz/*fastq.gz-rRNA/* /home/amlinz/GEODES_rRNA_split
mv /home/amlinz/*fastq.gz-nonrRNA/* /home/amlinz/GEODES_nonrRNA_split
rmdir *fastq.gz-*

gzip -d /home/amlinz/GEODES_nonrRNA_split/*
gzip -d /home/amlinz/GEODES_rRNA_split/*

cat /home/amlinz/samplenames.txt | while read line;
  do cat /home/amlinz/GEODES_nonrRNA_split/$line*nonrRNA.fastq > /mnt/gluster/amlinz/GEODES_nonrRNA/$line-nonrRNA.fastq;
  cat /home/amlinz/GEODES_rRNA_split/$line*rRNA.fastq > /mnt/gluster/amlinz/GEODES_rRNA/$line-rRNA.fastq;
  done
  
gzip /mnt/gluster/amlinz/GEODES_nonrRNA/*
gzip /mnt/gluster/amlinz/GEODES_rRNA/*

```

01rRNA_sorting.DAG
```{bash, eval = F}
JOB 00split_fastq /home/amlinz/submits/00split_fastq.sub
JOB 01rRNA_removal /home/amlinz/submits/01rRNA_removal.sub
SCRIPT PRE 00split_fastq /home/amlinz/scripts/samplenames.sh
SCRIPT POST 00split_fastq /home/amlinz/scripts/path2splitfastqs.sh
SCRIPT POST 01rRNA_removal /home/amlinz/scripts/cat_files.sh
```

####2017-09-06

Finally I finished! The issue wasn't the code, it was the new file limit on Gluster. Long story short, 200 files does not cut if my input is 110, my output is 110, and I'd like to have 20 files for figuring out the phylodist classifications, and directories count against my file limit. I now have 400 files. Re-ran the 2nd portion of my files that got killed last time and backed those up on both my hard drive and the lakes drive. Before I delete unnecessary files, I'll run checksums to make sure everything transferred:

```{bash, eval = F}

```

I'm checking the downloaded files from my PC using 7-Zip. Just checking the lakes drive ones, I assume that if they made it there uncorrupted via my hard drive, the ones on my hard drive are just fine.

