# Process mapped reads

####Goal of this analysis

The mapping in the last step produced a .sam file where each line is a read in the metatranscriptome, and information is included about whether or not it mapped, and where it mapped. Technically, this is the data I need to address my hypotheses, but each file is way to large and clunky to analyze. I'd like to analyze these using the DESEQ R package, which requires a table where rows are genes and columns are samples. 

##### Approach

I'm using a program called FeatureCounts to count the number of reads mapped, then compiling these into a table. I'm also making metadata files to tell me which contig each gene locus_id belongs to and what it is annotated as.

## Most recent workflow. 
####Use this if you want to replicate our protocol. Updated 2017-06-16

1. Set up folders in gluster for the outputs, and a list of files to run.
```{r, eval = F}
mkdir /mnt/gluster/amlinz/GEODES_mapping_summaries/

for file in /mnt/gluster/amlinz/GEODES_mapping_results/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > bamfiles.txt
cat bamfiles.txt
```

Build the gff file in an interactive session. Note that every gff format is different, and I'll likely need to modify this for every type of reference genome I (you) want to use. I'm using the program genometools to fix up gff files - better than wrangling around in awk - but it can't fix everything. In my case, the gff files for the metagenome assemblies produced by the JGI don't have an initial comment line and use a different strand annotation (-1, 1, instead of -, +), both of which crash genometools' gff tidy function.

build_gff.sub:
```{bash, eval = F}

#build_gff.sub
#
universe = vanilla
# Name the log file:
log = interactive.log

# Name the files where standard output and error should be saved:
output = process.out
error = process.err

# If you wish to compile code, you'll need the below lines.
#  Otherwise, LEAVE THEM OUT if you just want to interactively test!
+IsBuildJob = true
requirements = (OpSysAndVer =?= "SL6") && ( IsBuildSlot == true )

# Indicate all files that need to go into the interactive job session,
#  including any tar files that you prepared:
transfer_input_files = zipped/genometools-1.5.9.tar.gz

# It's still important to request enough computing resources. The below
#  values are a good starting point, but consider your file sizes for an
#  estimate of "disk" and use any other information you might have
#  for "memory" and/or "cpus".
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 20GB
request_disk = 25GB

queue
```

Running the interactive session:
```{bash, eval = F}
condor_submit -i submits/build_gff.sub
# Wait

tar -xvzf genometools-1.5.9.tar.gz
cd genometools-1.5.9
make cairo=no
make prefix=$(pwd)/../genometools/ cairo=no install
cd ..
export PATH=$(pwd)/genometools/bin:$PATH

#Copy the gff files to this directory
cp /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff .
cp /mnt/gluster/amlinz/ref_genomes/gff_files/*.gff .

#Make and save a list of which contigs are in which file
for file in *.gff;do awk '{print $1}' $file > temp.txt; sort -u temp.txt > $file.txt;awk '{print FILENAME $0}' $file.txt > temp2.txt;mv temp2.txt $file.txt;done
cat *.gff.txt > contig_metadata.txt
cp contig_metadata.txt /mnt/gluster/amlinz/

#Attach the standard to one of the ref genomes files
sed '2q;d' pFN18A_DNA_transcript.gff > line2.gff
cat 2619619040.gff line2.gff > temp && mv temp 2619619040.gff

#Fix the strand issue in the assemblies
for file in GEODES*.gff; do awk -F'\t' -vOFS='\t' '{gsub("-1", "-", $7); gsub("1", "+", $7); print}' $file > f1_$file; done

#Add first comment line to the assemblies
for file in f1*gff; do echo '##gff-version 3' | cat - $file > temp && mv temp $file; done

#Sort and tidy the assemblies
for file in f1*gff; do gt gff3 -sort yes -tidy -retainids -o sorted_$file $file; done

#Fix up the ref genomes - add comment line, remove CRISPR lines, change semicolons to escaped semicolons, and sort.
for file in 2*gff;do echo '##gff-version 3' | cat - $file > temp && mv temp $file;sed '/CRISPR/d' $file > temp && mv temp $file; sed -i 's|; |%3B |g' $file; gt gff3 -sort yes -tidy -retainids -o sorted_$file $file; done

#Run the merge
gt merge -o mapping_database.gff sorted*gff

gzip mapping_database.gff
cp mapping_database.gff.gz /mnt/gluster/amlinz/

rm GEODES*gff
rm sorted*gff
rm f1*gff

rm *.gz
rm *.gff
rm -r genome*
exit
```

Last thing before running the scripts is to build an install for the FeatureCounts program. I installed it from source code and am using the Linux instructions here: http://bioinf.wehi.edu.au/subread-package/

install_featurecounts.sub
```{bash, eval = F}

#install_featurecounts.sub
#
universe = vanilla
# Name the log file:
log = interactive.log

# Name the files where standard output and error should be saved:
output = process.out
error = process.err

# If you wish to compile code, you'll need the below lines.
#  Otherwise, LEAVE THEM OUT if you just want to interactively test!
+IsBuildJob = true
requirements = (OpSysAndVer =?= "SL6") && ( IsBuildSlot == true )

# Indicate all files that need to go into the interactive job session,
#  including any tar files that you prepared:
transfer_input_files = zipped/subread-1.5.2-source.tar.gz

# It's still important to request enough computing resources. The below
#  values are a good starting point, but consider your file sizes for an
#  estimate of "disk" and use any other information you might have
#  for "memory" and/or "cpus".
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 8GB
request_disk = 4GB

queue

```

In the interactive session:
```{bash, eval = F}
condor_submit -i install_featurecounts.sub
# Wait

tar zxvf subread-1.5.2-source.tar.gz
cd subread-1.5.2-source/src
make -f Makefile.Linux
tar czvf subread.tar.gz subread-1.5.2-source

# Move tarball back to gluster
cp subread.tar.gz /mnt/gluster/amlinz

# Clean up
rm subread.tar.gz
rm -r subread-1.5.2.source
exit

# Move tarball into zipped folder
mv /mnt/gluster/amlinz/subread.tar.gz zipped/subread.tar.gz

```

2. Peform the counting. Like Jimmy Johns, FeatureCounts is freaky fast (and seems to be freaky good as well, at least it gives the same results as HTSeq-count) so this should only take a day or so. I'm using minOverlap and fracOverlap == 1, equivalent to HTSeq's mode intersection_strict (read must be contained in a gene to be mapped to that gene). 

FeatureCounts produces two files. The txt.summary has mapped and unmapped read proportions, while the mapped.txt has counts per feature. I remove unmapped reads from the file to save on space, otherwise I can't open the files.

06featurecounts.sub:
```{bash, eval = F}
# 06featurecounts.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 06featurecounts_$(Cluster).log
error = 06featurecounts_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/06featurecounts.sh
arguments = $(samplename)
output = 06featurecounts_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/subreads.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 20GB
request_disk = 12GB
#
# Tell HTCondor to run every file in the provided list:
queue samplename from bamfiles.txt


```

06featurecounts.sh:
```{bash, eval = F}


#!/bin/bash
#Make a count table of mapped reads

#Transfer data from gluster
cp /mnt/gluster/amlinz/GEODES_mapping_results/$1.mapped.bam .
cp /mnt/gluster/amlinz/mapping_database.gff.gz .

#Unzip programs
tar xvzf subreads.tar.gz
gzip -d mapping_database.gff.gz

#Count reads
./subread-1.5.2-source/bin/featureCounts -t CDS -g locus_tag --minOverlap 1 --fracOverlap 1 -M --fraction -a mapping_database.gff -o $1.CDS.txt $1.mapped.bam

# Keep only mapped reads
#awk '(NR==1) || ($7 > 0 ) ' $1.CDS.txt > $1.CDS.mapped.txt

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
#cp $1.CDS.mapped.txt /mnt/gluster/amlinz/GEODES_mapping_summaries/
cp $1.CDS.txt /mnt/gluster/amlinz/GEODES_mapping_summaries/
cp $1.CDS.txt.summary /mnt/gluster/amlinz/GEODES_mapping_summaries/
rm $1.CDS*

#Clean up
rm *.bam
rm mapping_database*
rm *.tar.gz
rm -r subread-1.5.2-source


```

Run and check the results

```{r, eval = F}
condor_submit submits/06feature_counts.sub

ls -ltr 06*err
ls -ltr /mnt/gluster/amlinz/GEODES_mapping_summaries/
```

This will also dump a bunch of files with stats on the counting in your home folder.

3. Compile the result into a single table using the scripts 07maketable. Change the date in the .sh file so that you have a record of which result this is (for when you inevitably have to re-run something).

07maketable.sub:
```{bash, eval = F}
# 03maketable.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 07maketable_$(Cluster).log
error = 07maketable_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/07maketable.sh
#arguments =
output = 07maketable_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files =
#transfer_output_files = GEODES_genes_2017-05-19.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 1GB
request_disk = 2GB
#
# Only need one instance of this job
queue

```

07maketable.sh
```{r, eval = F}

#!/bin/bash
cp /mnt/gluster/amlinz/GEODES_mapping_summaries/*.CDS.txt .

awk '{print $1}' GEODES001_nonrRNA.CDS.txt > rownames.txt
for file in *.CDS.txt;do awk '{print $7}' $file > temp.txt;sample=$(basename $f$
echo "placeholder" | cat - rownames.txt > temp3.txt && mv temp3.txt rownames.txt
files=$(echo *CDS.txt)
paste rownames.txt $files > GEODES_genes_2017-05-23.txt
sed -i -e '2,3d' GEODES_genes_2017-05-23.txt
gzip GEODES_genes_2017-05-23.txt
cp GEODES_genes_2017-05-23.txt.gz /mnt/gluster/amlinz

rm *gz
rm *.CDS.txt
rm rownames.txt
rm temp*.txt

```

```{bash, eval = F}
condor_submit submits/07maketable.sub

ls -ltr 07*.err
```

4. Cleanup

The only thing I want to save is the table itself, which I downloaded onto my computer with WinSCP. Once you've checked the table to make sure it works, delete all /*.err, /*.out, and /*.log files. Move onto the 04R_analyses directory, where you'll link those locus tags to genomes and gene products.

#Lazy Run

As in the previous step's lazy run workflow, THIS IS FOR AFTER YOU'VE DONE THE FULL WORKFLOW ONCE AND KNOW EVERYTHING WORKS. NOT FOR ACTUALLY BEING LAZY.

```{bash, eval = F}
mkdir /mnt/gluster/amlinz/GEODES_mapping_summaries/

for file in /mnt/gluster/amlinz/GEODES_mapping_results/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > bamfiles.txt
cat bamfiles.txt

condor_submit -i submits/build_gff.sub
# Wait

tar -xvzf genometools-1.5.9.tar.gz
cd genometools-1.5.9
make cairo=no
make prefix=$(pwd)/../genometools/ cairo=no install
cd ..
export PATH=$(pwd)/genometools/bin:$PATH

#Copy the gff files to this directory
cp /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff .
cp /mnt/gluster/amlinz/ref_genomes/gff_files/*.gff .

#Make and save a list of which contigs are in which file
for file in *.gff;do awk '{print $1}' $file > temp.txt; sort -u temp.txt > $file.txt;awk '{print FILENAME $0}' $file.txt > temp2.txt;mv temp2.txt $file.txt;done
cat *.gff.txt > contig_metadata.txt
cp contig_metadata.txt /mnt/gluster/amlinz/

#Attach the standard to one of the ref genomes files
sed '2q;d' pFN18A_DNA_transcript.gff > line2.gff
cat 2619619040.gff line2.gff > temp && mv temp 2619619040.gff

#Fix the strand issue in the assemblies
for file in GEODES*.gff; do awk -F'\t' -vOFS='\t' '{gsub("-1", "-", $7); gsub("1", "+", $7); print}' $file > f1_$file; done

#Add first comment line to the assemblies
for file in f1*gff; do echo '##gff-version 3' | cat - $file > temp && mv temp $file; done

#Sort and tidy the assemblies
for file in f1*gff; do gt gff3 -sort yes -tidy -retainids -o sorted_$file $file; done

#Fix up the ref genomes - add comment line, remove CRISPR lines, change semicolons to escaped semicolons, and sort.
for file in 2*gff;do echo '##gff-version 3' | cat - $file > temp && mv temp $file;sed '/CRISPR/d' $file > temp && mv temp $file; sed -i 's|; |%3B |g' $file; gt gff3 -sort yes -tidy -retainids -o sorted_$file $file; done

#Run the merge
gt merge -o mapping_database.gff sorted*gff

gzip mapping_database.gff
cp mapping_database.gff.gz /mnt/gluster/amlinz/

rm GEODES*gff
rm sorted*gff
rm f1*gff

rm *.gz
rm *.gff
rm -r genome*
exit

condor_submit -i install_featurecounts.sub
# Wait

tar zxvf subread-1.5.2-source.tar.gz
cd subread-1.5.2-source/src
make -f Makefile.Linux
tar czvf subread.tar.gz subread-1.5.2-source

# Move tarball back to gluster
cp subread.tar.gz /mnt/gluster/amlinz

# Clean up
rm subread.tar.gz
rm -r subread-1.5.2.source
exit

# Move tarball into zipped folder
mv /mnt/gluster/amlinz/subread.tar.gz zipped/subread.tar.gz

condor_submit submits/06feature_counts.sub

ls -ltr 06*err
ls -ltr /mnt/gluster/amlinz/GEODES_mapping_summaries/

condor_submit submits/07maketable.sub
ls
ls -ltr 07*.err
```

#Lab Notebook

####2017-02-21

Some of this is carried over from the 02mapping lab notebook, since I was originally planning to do that in one step. However, the script was large, slow and error prone, so here we are. I'll need my samtools install from the last script, and the python install with the htseq module.

Make a list of files to run:
```{r, eval = F}
ls /mnt/gluster/amlinz/GEODES_mapping_concat/ > samfiles.txt
cat samfiles.txt

```

Copy the python file over to gluster then write submit script - the only things it's sending over is the samtools install and the gff file.
03processing.sub:
```{r, eval = F}
# 03processing.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03processing_$(Cluster).log
error = 03processing_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 03processing.sh
arguments = $(samplename)
output = 03processing_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = samtools.tar.gz,mapping_database.gff.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 3GB
request_disk = 5GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from samfiles.txt
```

And the executable, based on my early mapping testing with htseq:
```{r, eval = F}
#!/bin/bash
#Make a count table of mapped reads

#Transfer data from gluster
cp /mnt/gluster/amlinz/GEODES_mapping_concat/$1.all.sam .
cp /mnt/gluster/amlinz/python.tar.gz .


#Unzip programs
tar xzf python.tar.gz
tar xvf samtools.tar.gz
gzip -d mapping_database.gff.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$(pwd)/samtools/bin:$PATH
export HOME=$(pwd)/home

#Manipulate the output
samtools view -b -S $1.all.sam > $1.bam 
samtools sort $1.bam $1.sorted 
samtools index $1.sorted.bam

#Count reads
python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t CDS -i locus_tag -m intersection-strict -o $1.CDS.sam $1.sorted.bam mapping_database.gff > $1.CDS.out

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.CDS.out /mnt/gluster/amlinz/GEODES_mapping_summaries/
rm $1.CDS.out

#Clean up
rm -rf python
rm -rf samtools
rm *.sam
rm *.bam
rm *.bai
rm mapping_database*
rm *.tar.gz
  
```

Something's wrong with the header in the all.sam file? That would be an issue with the merging script. Going into interactive mode to check. samtools is a pain and their documentation seems out of date.

Fixed the issue - left out the -h flag in samtools view that leaves the header in output. Rerun 02merge script and 03processing, and got the following error:

Error occured when processing SAM input (record #285895 in file GEODES002_nonrRNA.all.sam):
  object of type 'NoneType' has no len()
  [Exception type: TypeError, raised in _HTSeq.pyx:771]

It happened in all three files, although in different lines. The only thing I can find online is here? https://sourceforge.net/p/htseq/bugs/12/ Doesn't seem helpful because many of my lines have *. Will sleep on this and come back tomorrow.

####2017-02-22

Back to the googling. In regular old python, you can get this error when measuring the length of "None" because that is a protected variable. Does "None" appear in my sam files?

```{r, eval = F}
grep -n None /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam
```

Nothing. Replaced search term with \* and nearly every line has that symbol. I didn't get this error before I split and merged files - what is different now?
Not sure, but I realized I wasn't including header in the line count. So record #341922 is actually line 382861, which does have an asterix in a weird place - after the sequence. Is this my error? let's change that * to "None" and see what happens.

```{r, eval = F}
awk '{gsub(/\*/,"None",$11)}1' /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam > /mnt/gluster/amlinz/GEODES_mapping_concat/test.sam
```

Now I get an error that says "truncated file" right at that same line. There are only 18 lines with no Phred score out of 500,000+ , so I'll just remove these and address it later.

```{r, eval = F}
#Count phred scores with "*"
 awk '{print $11}' /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam | grep \* |wc -l
#Remove these lines
awk '{ if ( $11 != "\*" ) { print $0; } }' /mnt/gluster/amlinz/GEODES_mapping_concat/GEODES001_nonrRNA.all.sam > /mnt/gluster/amlinz/GEODES_mapping_concat/test.all.sam
```

Looks like that removed the right lines! I get an warning about the escape character, but I think that's what I want. Testing 03processing again.

It made a feature.out file! I'm just going to look up a couple "features" in the gff file to make sure I can use that as metadata.
```{r, eval = F}
gzip -d mapping_database.gff.gz
grep scaffold_01090 mapping_database.gff
grep YUCDRAFT_00363 mapping_database.gff
gzip mapping_database.gff
```

Looking good! Those both produce unique lines. I'm going to add the awk lines (including printing how many * lines were removed) and run both the merging and processing on all three files. (make sure to clean up dud files first)

I'm still getting this error occasionally in the merge step:
[sam_header_line_parse] expected '@XY', got [@BFFGGGGGGGGGGGDGGGGD>FGGCGGGGGGGGGGGGGGGGEGGGGGBGFGGGDGGGGGGGEAGGGGGGGGGGEGCGGGFGGGGCGGGGGGBGGGGG:CGGGGGGGGGEGBEG<BGGEG/DDEGGGGGDD=EGEGD>GGGGGGGGG     4       *       0       0       *       *       0   0*       AS:i:0  XS:i:0]
Hint: The header tags must be tab-separated.
[samopen] no @SQ lines in the header.
[sam_read1] missing header? Abort!

Ran the merging and processing, and everything looks great! Now I need a script to combine all of the output files into a single table. I'm going to start an interactive session for testing this.

```{r, eval = F}
condor_submit -i testinteractive.sub

cp /mnt/gluster/amlinz/GEODES_mapping_summaries/* .

#Are all files the same length?
for file in *.out;do wc -l $file;done

#Yes - presumably the rownames are the same.
#Add a header of sample name to each of the files
for file in *.out;do sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - $file > temp && mv temp $file;done

#That worked!
#Now try join to combine

join GEODES002_nonrRNA.CDS.out GEODES001_nonrRNA.CDS.out 
# Worked, but gave me an warning about sorting and didn't keep the headers.

#Let's try paste instead:
paste -d' ' GEODES001_nonrRNA.CDS.out GEODES002_nonrRNA.CDS.out

#Keeps the headers but prints every rowname (with the header on the rowname)
#Can I 1. make a separeate file of rownames 2. remove rownames from the out files 3. add the header 4. paste everything together?

awk '{print $1}' GEODES001_nonrRNA.CDS.out > rownames.txt
for file in *.out;do awk '{print $2}' $file > temp.txt;sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - temp.txt > temp2.txt && mv temp2.txt $file;done
echo "/t" | cat - rownames.txt > temp3.txt && mv temp3.txt rownames.txt
files=$(echo *.out)
paste rownames.txt $files > GEODES_genes_all.txt

#Looking good! exit and download to file to make sure I can open it in Rstudio
exit
```

Write this up as a script and test again:
03maketable.sub:
```{r, eval = F}
# 03maketable.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03maketable_$(Cluster).log
error = 03maketable_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 03maketable.sh
#arguments = 
output = 03maketable_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
#transfer_input_files = 
transfer_output_files = GEODES_genes_2017-02-22.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 1GB
request_disk = 2GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue
```

03maketable.sh
```{r, eval = F}
#!/bin/bash
#Combine htseq-count results into one table
cp /mnt/gluster/amlinz/GEODES_mapping_summaries/* .

awk '{print $1}' GEODES001_nonrRNA.CDS.out > rownames.txt
for file in *.out;do awk '{print $2}' $file > temp.txt;sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - temp.txt > temp2.txt && mv temp2.txt $file;done
echo "\t" | cat - rownames.txt > temp3.txt && mv temp3.txt rownames.txt
files=$(echo *.out)
paste rownames.txt $files > GEODES_genes_2017-02-22.txt

rm *.out
rm rownames.txt
rm temp*.txt

```

Beautiful! Everything seems to be in working order. Time to put this aside for now and move over to the kraken classification workflow.

####2017-02-23

I got some weird errors about truncated files when trying to get a fastq file out of samtools, so I'm going back to all scripts, testing samtools in interactive mode, and then running. I'll do the same here, while also updating the "current workflow" section.

####2017-02-27

Ran the processing and didn't get any errors but about 10 samples produced empty output files. I'll come back and fix this if I have time before lab meeting. Moving on to making the metadata file now. 

####2017-04-11

It's been awhile for this step! I've got 10 files mapped accurately and am ready to revisit read counting. My issues last time were:
- need a better way to parse the gff file
- got errors because some SAM files had * as a Phred score
- some files were just empty

I'll start with the gff file. Can I find a tool for merging gff files? The tools at genometools.org/pub look good. I downloaded verstion 1.5.9. Uploading to CHTC.

WHY JGI WHY??? GFF formats are the bane of my existance right now. After getting genometools installed (not easy) I finally run merge and get the error: strand '-1' not one character long on line 1 in GEODES005.assembled.gff

Running head on my gff file piece indeed confirms that strand is indicated with -1 or 1. The standard gff format is - or +, hence the "one character" error. I'll need to change -1 to - and 1 to +, but only for that specific column. A job for awk maybe? Will also need to be conservative so as not to screw up my MAG gff files.

First I should right down how I ultimately installed genometools. This is in an interactive session.
```{bash, eval = F}
tar -xvzf genometools-1.5.9.tar.gz
cd genometools-1.5.9
make
make prefix=$(pwd)/../genometools/ install
cd ..
export PATH=$(pwd)/genometools/bin:$PATH

#Now I can run genometools
gt merge -retainids -tidy -o mapping_database.gff /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff

#This is what gave me that error

#Copy the gff files to this directory
cp /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff .

#Fix the strand issue
for file in *.gff; do awk -F'\t' -vOFS='\t' '{gsub("-1", "-", $7); gsub("1", "+", $7); print}' $file > f1_$file; done

#Try again
gt merge -retainids -tidy -o mapping_database.gff -force f1*.gff


```

error:gff file not sorted

####2017-04-12

Busy day of meetings but I'll be working on the gff issue in between. According to the genometools manual, the command gt gff3 will sort gff files if that's added as a parameter. It will also tidy them up here instead of at the merge step. Starting a new interactive session to test:

```{bash, eval = F}
tar -xvzf genometools-1.5.9.tar.gz
cd genometools-1.5.9
make cairo=no
make prefix=$(pwd)/../genometools/ cairo=no install
cd ..
export PATH=$(pwd)/genometools/bin:$PATH

#Copy the gff files to this directory
cp /mnt/gluster/amlinz/metagenome_assemblies/gff/*.gff .

#Fix the strand issue
for file in *.gff; do awk -F'\t' -vOFS='\t' '{gsub("-1", "-", $7); gsub("1", "+", $7); print}' $file > f1_$file; done

#Add first comment line
for file in f1*gff; do echo '##gff-version 3' | cat - $file > temp && mv temp $file; done
echo '##gff-version 3' | cat - mapping_database.gff > temp && mv temp mapping_database.gff

#Sort and tidy
for file in f1*gff; do gt gff3 -sort yes -tidy -force -retainids -o tidy_$file $file; done

#Run the merge
gt merge -o mapping_database.gff tidy*gff

gzip mapping_database.gff

cp mapping_database.gff.gz /mnt/gluster/amlinz/

rm *.gz
rm *.gff
rm -r genome*
rm temp
exit

```

I think that's working! Needed cairo=no, apparently not all nodes have cairo. Also needed to add a first comment line into the gff files. The tidy function added a gff header and corrected items to lowercase capitalization. After that the merge ran with no errors!
This will work for now, but unfortunately I think I will need a separate script for each "type" of gff file I want to include in my database due to formatting issues... yuck.

####2017-04-13

Next up, edit the htseq-couns script:
```{bash, eval = F}
#!/bin/bash
#Make a count table of mapped reads

#Transfer data from gluster
cp /mnt/gluster/amlinz/GEODES_mapping_results/$1.mapped.bam .
cp /mnt/gluster/amlinz/python.tar.gz .
cp /mnt/gluster/amlinz/mapping_database.gff.gz .

#Unzip programs
tar xzf python.tar.gz
tar xvf samtools.tar.gz
gzip -d mapping_database.gff.gz

#Update the path variable
mkdir home
export PATH=$(pwd)/python/bin:$PATH
export HOME=$(pwd)/home

#Manipulate the output
samtools sort -o $1.sorted $1.mapped.bam
samtools index $1.sorted.bam

#Count reads
python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t CDS -i locus_tag -m intersection-strict -o $1.CDS.sam $1.sorted.bam mapping_database.gff > $1.CDS.out

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.CDS.out /mnt/gluster/amlinz/GEODES_mapping_summaries/
rm $1.CDS.out

#Clean up
rm -r python
rm -r samtools
rm *.bam
rm *.bai
rm mapping_database*
rm *.tar.gz
```

Make a list of files to run:
```{bash, eval = F}
for file in /mnt/gluster/amlinz/GEODES_mapping_results/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > bamfiles.txt
cat bamfiles.txt

```

And the submit file:
```{bash, eval = F}

# 03processing.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03processing_$(Cluster).log
error = 03processing_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = 03processing.sh
arguments = $(samplename)
output = 03processing_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/samtools.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 3GB
request_disk = 12GB
#
# Tell HTCondor to run every file in the provided list:
queue samplename from bamfiles.txt

```

Looks like CHTC is still down for repairs from last night. Will try again later today.

####2017-04-17

I did get the script started, and it ran for a few hours before running out of RAM. So I bumped the RAM up to 12GB, and it ran for 2.5 days, and then ran out of RAM. Clearly that's not feasible. I think I need to split up the input BAM files, run them individually, and then compile the results of HTseq count afterwards.

Here's my first attempt at that script. After looking online, splitting by line should be sufficient.

05splitbams.sh
```{bash, eval = F}
#!/bin/bash
#Split bam files into smaller pieces based on line count
tar xvf samtools.tar.gz

cp /mnt/gluster/amlinz/GEODES_mapping_results/$1.mapped.bam .

samtools view $1.mapped.bam > $1.sam
split -l 1000000 $1.sam part_$1

# Rename files to include .sam extension
for file in part_$1*;do mv $file $file.sam;done

# Convert to bam and add the header back in
for file in part_$1*.sam; do name=$(basename $file | cut -d'.' -f1);  samtools reheader $1.sam $file > $name.sam; samtools view -b $name.sam > $name.bam; done

# Send files back to gluster
cp part_$1*.bam /mnt/gluster/amlinz/GEODES_split_bams/

# Clean up
rm -r $1
rm *sam
rm *bam
rm -r samtools
rm samtools.tar.gz

```

Run this in interactive mode to test

SO MANY PROBLEMS. The internet lies. I can't reheader because I have no EOF lines and I can't get the EOF lines by converting to BAM because I have no header. At this point, I think it would be easier to install another program.  I'm looking at NGSutils bamutils.

Downloaded to my submit node via git, then intalling in interactive mode here: http://ngsutils.org/installation/

####2017-04-18

Couldn't install ngsutils yesterday - it needs Python, and I don't think my custom Python install had what it need. It kept giving an error about not finding a bin file during make and when I tried to run bamutils, I got an error that pysam was not installed even though I can call pysam just fine in a python session.

Josh didn't have any more ideas for splitting bam files, so I'm going to try featureCounts instead of HTSeq. This one is supposed to be much faster, and I'm not seeing anything saying it's more error prone. Some say that it is more liberal in calling reads than HTSeq but that HTSeq may be too conservative overall.

I'm going to build a custom install of featureCounts in an interactive session. I installed it from source code and am using the Linux instructions here: http://bioinf.wehi.edu.au/subread-package/

```{bash, eval = F}
tar zxvf subread-1.5.2-source.tar.gz
cd subread-1.5.2-source/src
make -f Makefile.Linux
tar czvf subreads.tar.gz subread-1.5.2-source
cp subreads.tar.gz /mnt/gluster/amlinz
#Move to zipped/ back in home directory
```

New counting script - lines just commented out for now
03featurecounts.sh
```{bash, eval = F}

#!/bin/bash
#Make a count table of mapped reads

#Transfer data from gluster
cp /mnt/gluster/amlinz/GEODES_mapping_results/$1.mapped.bam .
#cp /mnt/gluster/amlinz/python.tar.gz .
cp /mnt/gluster/amlinz/mapping_database.gff.gz .

#Unzip programs
#tar xzf python.tar.gz
#tar xvf samtools.tar.gz
tar xvzf subreads.tar.gz
gzip -d mapping_database.gff.gz

#Update the path variable
#mkdir home
#export PATH=$(pwd)/python/bin:$PATH
#export HOME=$(pwd)/home

#Manipulate the output
#samtools sort -O bam $1.mapped.bam > $1.sorted.bam
#samtools index $1.sorted.bam

#Count reads
#python ./python/bin/htseq-count -f bam -r pos -s no -a 0 -t CDS -i locus_tag -m intersection-strict -o $1.CDS.sam $1.sorted.bam mapping_database.gff > $1.CDS.out
./subread-1.5.2-source/bin/featureCounts -t CDS -g locus_tag -a mapping_database.gff -o $1.CDS.txt $1.mapped.bam


#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
cp $1.CDS.txt /mnt/gluster/amlinz/GEODES_mapping_summaries/
cp $1.CDS.txt.summary /mnt/gluster/amlinz/GEODES_mapping_summaries/
rm $1.CDS.txt


#Clean up
#rm -r python
#rm -r samtools
rm *.bam
#rm *.bai
rm mapping_database.gff
rm *.tar.gz
rm -r subread-1.5.2-source
```

03featurecounts.sub
```{bash, eval = F}
# 03featurecounts.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 03featurecounts_$(Cluster).log
error = 03featurecounts_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (OpSysMajorVer == 6)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/03featurecounts.sh
arguments = $(samplename)
output = 03featurecounts_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/subreads.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 20GB
request_disk = 12GB
#
# Tell HTCondor to run every file in the provided list:
queue samplename from bamfiles.txt

```

Test on one first

#### 2017-05-04

Everything seems to be working well! featureCounts works great and Josh did some comparisons to HTSeq-count. It works the same as long as we set the intersection/fraction parameters to match HTSeq's intersection-strict mode. I've also uploaded all the metaTs! We have 109 - one extra? Probably one of the borderline low samples worked after we replaced it. My next task is to combine the ref MAGs SAGs database with the metagenome assemblies. I tried running my current script and no go - some of the refMS files were empty after sorting. Internet suggests that there may be empty lines in the files. I'm going into interactive mode to test with a limited number of files and troubleshoot.

Think I've got it - the key is not to tidy the ref mags but to tidy the assemblies. My test samples didn't have blank lines but I'll add this to the script just in case for other samples.

They will just not play nice. Current plan is to run the counting twice since it's so fast.


#### 2017-05-15

```{bash, eval = F}

#!/bin/bash
#Combine htseq-count results into one table
cp /mnt/gluster/amlinz/GEODES_mapping_summaries/* .

awk '{print $1}' GEODES001_nonrRNA.CDS.txt > rownames.txt
for file in *.CDS.txt;do awk '{print $2}' $file > temp.txt;sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - temp.txt > temp2.txt && mv temp2.txt $file;done
echo "\t" | cat - rownames.txt > temp3.txt && mv temp3.txt rownames.txt
files=$(echo *CDS.txt)
paste rownames.txt $files > GEODES_genes_2017-05-15.txt

rm *.CDS.txt
rm rownames.txt
rm temp*.txt

```

```{bash, eval = F}

#!/bin/bash
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/GEODES_metaT/$1.filter-MTF.fastq.gz .
gzip -d $1.filter-MTF.fastq.gz

sed -i '/^$/d' $1.filter-MTF.fastq

maxreads=$((`wc -l < $1.filter-MTF.fastq` / 8 - 1))
startpoints=$(seq 0 500000 $maxreads)

for num in $startpoints;
  do endpoint=$(($num + 499999));
  bbmap/getreads.sh in=mapping_database.fna id=$num-$endpoint out=mapping_database.$num.fna overwrite=T;
  done

rm $1.filter-MTF.fastq
gzip $1*
cp $1* /mnt/gluster/amlinz/GEODES_metaT_split
rm $1*
rm BBMap_36.99.tar.gz
rm -r bbmap

```

####2017-05-22

03maketable script produces no errors but then can't find the output file to transfer. Running in interactive mode to test.

```{bash, eval = F}

cp /mnt/gluster/amlinz/GEODES_mapping_summaries/*.CDS.txt .

awk '{print $1}' GEODES001_nonrRNA.CDS.txt > rownames.txt
for file in *.CDS.txt;do awk '{print $7}' $file > temp.txt;sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - temp.txt > temp2.txt && mv temp2.txt $file;done
echo " " | cat - rownames.txt > temp3.txt && mv temp3.txt rownames.txt
files=$(echo *CDS.txt)
paste rownames.txt $files > GEODES_genes_2017-05-23.txt
sed -e '2,3d' GEODES_genes_2017-05-23.txt
gzip GEODES_genes_2017-05-23.txt
cp GEODES_genes_2017-05-23.txt.gz /mnt/gluster/amlinz

rm *gz
rm *.CDS.txt
rm rownames.txt
rm temp*.txt

```

#### 2017-06-16

Long time, no see. I've been working on the downstream steps, but realized one issue with the read mapping - multi-mapping reads. By default, featureCounts does not count reads mapping to more than one location in the reference database. But since we have so much in our reference, there's a high chance that locations are replicated, particularly in replicate metagenomes. I switched this to fractional instead. Here's the new script:

```{bash, eval = F}

#!/bin/bash
#Make a count table of mapped reads

#Transfer data from gluster
cp /mnt/gluster/amlinz/GEODES_mapping_results/$1.mapped.bam .
cp /mnt/gluster/amlinz/mapping_database.gff.gz .

#Unzip programs
tar xvzf subreads.tar.gz
gzip -d mapping_database.gff.gz

#Count reads
./subread-1.5.2-source/bin/featureCounts -t CDS -g locus_tag --minOverlap 1 --fracOverlap 1 -M --fraction -a mapping_database.gff -o $1.CDS.txt $1.mapped.bam

# Keep only mapped reads
#awk '(NR==1) || ($7 > 0 ) ' $1.CDS.txt > $1.CDS.mapped.txt

#Save output to gluster. Using cp/rm instead of mv so it will overwrite old output in gluster.
#cp $1.CDS.mapped.txt /mnt/gluster/amlinz/GEODES_mapping_summaries/
cp $1.CDS.txt /mnt/gluster/amlinz/GEODES_mapping_summaries/
cp $1.CDS.txt.summary /mnt/gluster/amlinz/GEODES_mapping_summaries/
rm $1.CDS*

#Clean up
rm *.bam
rm mapping_database*
rm *.tar.gz
rm -r subread-1.5.2-source

```

I will update the workflows to reflect this change.


####2017-10-16

Running the test case of python dictionary approach to storing info. The featurecounts script worked with only minor naming modificatons:

10featurecounts.sub:
```{bash, eval = F}

# 10featurecounts.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 10featurecounts_$(Cluster).log
error = 010featurecounts_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX")
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/10featurecounts.sh
arguments = $(samplename)
output = 10featurecounts_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/subreads.tar.gz
transfer_output_files = $(samplename).MAGsSAGs.CDS.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 8GB
request_disk = 4GB
#
# Tell HTCondor to run every file in the provided list:
queue samplename from bamfiles.txt

```

10featurecounts.sh:
```{bash, eval = F}

#!/bin/bash
#Count mapped reads

#Unzip programs
tar -xvzf subreads.tar.gz
#Pair reads

#Count reads
./subread-1.5.2-source/bin/featureCounts -t CDS -g ID --minOverlap 1 --fracOverlap 1 -M --fraction -a /mnt/gluster/amlinz/ref_MAGs_SAGs_database.gff -o $1.MAGsSAGs.CDS.txt /mnt/gluster/amlinz/GEODES_mapping_results/$1.MAGsSAGs.mapped.bam

#Clean up - don't delete the text file, that is being sent back to the submit node
rm *.tar.gz
rm -r subread-1.5.2-source

```

All the output files are in my home dir. Sidenote, I didn't save the summary files, so I'm going to eventually need to calculate how many reads are in my samples to get the proportion mapped/unmapped.

Now see if my maketable script still works

maketable.sh
```{r, eval = F}

#!/bin/bash
#cp /mnt/gluster/amlinz/GEODES_mapping_summaries/*.CDS.txt .

awk '{print $1}' GEODES001-nonrRNA.MAGsSAGs.CDS.txt | tail -n +2 > rownames.txt
for file in *.CDS.txt;do awk '{print $7}' $file | tail -n +3 > temp.txt;sample=$(basename $file |cut -d'.' -f1);echo $sample | cat - temp.txt > temp2.txt && mv temp2.txt $file;done
                                                                    
files=$(echo *.MAGsSAGs.CDS.txt)
paste rownames.txt $files > pasted_table.txt
# remove duplicate whitespaces
 tr -s "\t" < pasted_table.txt > GEODES_genes_2017-10-16.txt
#sed -i -e '2,3d' GEODES_genes_2017-10-16.txt
#gzip GEODES_genes_2017-05-23.txt

```


I made some minimal test cases. Now to make a python script to process that table. I can reduce the size of both the table and the associated dictionary by removing genes with no mapped reads.

```{python, eval = F}

#%%#############################################################################
### Import packages
################################################################################

import pandas
import os
import sys

#%%#############################################################################
### Define input files
################################################################################

readcounts = 'endtable_testcase.txt'
genekey  = 'endproduct_testcase.txt'
outputcounts = 'slim_endtable.txt'
outputkey = 'slim_key.txt'

#%%#############################################################################
### Remove rows of all 0s from the table of read counts and sort rows alphabetically
### Remove the same genes from the gene key and sort alphabetically
################################################################################

# Open files

df = pandas.read_table(readcounts)
inKey = pandas.read_table(genekey, header=None)
outTable = open(outputcounts, 'w')
outKey = open(outputkey, 'w')

# Remove rows that sum to 0
df = df.loc[(df.sum(axis=1) != 0),]

# Sort rows by gene name
df = df.sort_values(by = 'Geneid')

df.to_csv(outputcounts, index=False)

# Remove genes in the key file that are no longer in the table
# Or maybe keep only genes that are still in the table
inKey = inKey[inKey[0].isin(df['Geneid'])]
inKey = inKey.sort_values(by = 0)
inKey.to_csv(outKey, index=False)
```


####2017-10-17

One problem I'd encountered was that there are extra tabs between the rownames and the 1st column of data. 2 tabs, to be specific (checking with cat -A). Fixed with tr -s (squeeze option), added to maketable.sh.  Now to finish my test case files, I need to grep for the first 10 genes or so in the key file.

```{bash, eval = F}
awk {'print $1'} endtable_testcase.txt | tail -n +2 > testgenes.txt

while read line; do grep $line /mnt/gluster/amlinz/ref_MAGs_SAGs_key.txt; done < testgenes.txt > endproduct_testcase.txt
```

Manually added some non-zero values in case there aren't any in these genes. Now try the python interactive again.

tar xvzf python.tar.gz
mkdir home
export PATH=$(pwd)/python/bin:$PATH
export HOME=$(pwd)/home

Last column (GEODES164) is all NaNs? That's not right. Maybe removing the extra whitespace shifted the columns.
For the sake of the script, I'm deleting that column for now and will fix next. It does say there are the right number of columns without that last one (109 samples + rownames). Where was that last one coming from?

Script works! BAck in my home folder, I had an extra blank CDS.txt file. Specifying MAGsSAGS.CDS.txt instead, which also solved the whitespace. Shall we try a full run here?

11tableprocessing.sh:
```{bash, eval = F}
#!/bin/bash
tar xvzf python.tar.gz
mkdir home
export PATH=$(pwd)/python/bin:$PATH
export HOME=$(pwd)/home

cp /mnt/gluster/amlinz/ref_MAGs_SAGs_key.txt .
chmod +x genekey.py

python genekey.py

rm GEODES_genes_2017-10-16.txt
rm ref_MAGs_SAGs_key.txt
rm genekey.py
rm -r home
rm -rf python
rm python.tar.gz

```


11tableprocessing.sub:
```{bash, eval = F}
# 11tableprocessing.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 11tableprocessing_$(Cluster).log
error = 11tableprocessing_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX")
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/11tableprocessing.sh
output = 11tableprocessing_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/python.tar.gz,GEODES_genes_2017-10-16.txt,scripts/genekey.py
transfer_output_files = GEODES2refMAGsSAGs_readcounts.txt,GEODES2refMAGsSAGS_geneinfo.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 4GB
request_disk = 2GB
#
# Tell HTCondor to run every file in the provided list:
queue

```

readcounts = 'GEODES_genes_2017-10-16.txt'
genekey  = 'ref_MAGs_SAGs_key.txt'

outputcounts = 'GEODES2refMAGsSAGs_readcounts.txt'
outputkey = 'GEODES2refMAGsSAGS_geneinfo.txt'


Bigger issue: there are no genes with mapped reads. Do an interactive test on a Mendota sample where I reduce the overlap thresholds. If not, I'll have to try mapping.

Removed product qualifiers, hopefually that fixes featurecounts issue

Nope. Remove everything after the last semicolon.

```{bash, eval = F}
awk -F';' {'print $1'} /mnt/gluster/amlinz/ref_MAGs_SAGs_database.gff > test.gff
```