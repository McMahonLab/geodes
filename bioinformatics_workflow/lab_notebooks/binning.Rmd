---
title: "GEODES MAGs"
author: "Alex Linz"
date: "January 11, 2018"
output: html_document
---

### 2018-01-11
We've decided that I need to bin the GEODES metaG assemblies to make some MAGs. I wouldn't be analyzing these, just counting up the reads mapped. But first I need to install a binning program. I've selected MaxBin because there are 2 papers showing it works well for high-complexity data (the other 2 good ones being MetaBAT and  MetaWatt) and it looks easiest to install. My goal for today is to make an install package for CHTC. I've downloaded version 2.2.4 from https://downloads.jbei.org/data/microbial_communities/MaxBin/MaxBin.html

I'm following the installation instructions here: https://downloads.jbei.org/data/microbial_communities/MaxBin/README.txt

In an interactive session:
```{bash, eval = F}
tar xvzf MaxBin-2.2.4.tar.gz
cd MaxBin-2.2.4
cd src
make
cd ..
./autobuild_auxiliary

```

Looks good!

### 2018-01-14

This is a long shot, but I'm going to try running a single metagenome to start. This way, I can also test if MaxBin runs properly, and maybe if I'm lucky it will finish in 3 days.

13map_metaGs.sub
```{bash, eval = F}
# 13map_metaGs.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 13map_metaGs_$(Cluster).log
error = 13map_metaGs_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX")
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/13map_metaGs.sh
arguments = $(metaG)
output = 13map_metaGs_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz,zipped/samtools.tar.gz
#transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory =72GB
request_disk = 48GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue metaG from path2metaGfastqs.txt

```

13map_metaGs.sh
```{bash, eval = F}
#!/bin/bash
#Map metagenomic reads to my pre-indexed database of reference genomes
#Transfer metaG from gluster
#Not splitting the metaTs anymore
cp /mnt/gluster/amlinz/GEODES_metagenomes/$1 .
cp /mnt/gluster/amlinz/ref.tar.gz .

#Unzip program and database
tar -xvzf BBMap_36.99.tar.gz
tar -xvf samtools.tar.gz
tar -xvzf ref.tar.gz
gzip -d $1
name=$(basename $1 | cut -d'.' -f1)
sed -i '/^$/d' $name.fastq

#Run the mapping step
bbmap/bbmap.sh in=$name.fastq out=$name.90.mapped.sam minid=0.90 trd=T sam=1.3 threads=1 build=1 mappedonly=T -Xmx64g

# I want to store the output as bam. Use samtools to convert.
./samtools/bin/samtools view -b -S -o $name.90.mapped.bam $name.90.mapped.sam

#Copy bam file back to gluster
cp $name.90.mapped.bam /mnt/gluster/amlinz/GEODES_metaG_mapping/

#Clean up
rm -r bbmap
rm -r ref
rm *.bam
rm *.sam
rm *.fastq
rm *.gz

```

### 2018-01-17

THREE DAYS LATER

Unless the 13map_metaG job finishes in the next 30 minutes, I'll need to split up my metagenomic reads. Talked to Sarah and this should still count as a competitive mapping, because I'm not splitting up the database. I'll need to use featurecounts (which means I'll need a gff file), and then sum each metagenomic read part by sample. Then I can use this input for a) binning (need featurecounts by contig) and b) comparison to metaTs (need featurecounts by CDS). I'll look for a gff file on IMG. Meanwhile, I'll start writing a series of scripts to split the mapping. None of it should be new concepts!

1. Split fastq files into smaller (1GBish) bits (borrow from 00split_fastq)
2. Count up mapped reads in each split part (borrow from 10featurecounts)
3. Sum up counted reads (borrow from make_table - zero rows removal?)

For all scripts, I'm testing using GEODES005 for now.

get_metaGs.sh
```{bash, eval = F}
#!/bin/bash
mkdir /mnt/gluster/amlinz/split_metagenomes/
for file in /mnt/gluster/amlinz/GEODES_metagenomes/*; do sample=$(basename $file |cut -d'.' -f1); echo $sample;done > metaG_samples.txt
head -1 metaG_samples.txt > test_metaG_samples.txt; mv test_metaG_samples.txt metaG_samples.txt

```


13split_metagenomes.sub
```{bash, eval = F}
# 13split_metagenomes.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 13split_metagenomes_$(Cluster).log
error = 13split_metagenomes_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/13split_metagenomes.sh
arguments = $(sample)
output = 13split_metagenomes_$(Cluster).out
#
# No files to transfer, since it's going to interact with Gluster instead of the submit node
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz
transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 12 GB
request_disk = 40 GB
#
# Submit jobs
queue sample from metaG_samples.txt

```

13split_metagenomes.sh
```{bash, eval = F}
#!/bin/bash
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/GEODES_metagenomes/$1.fastq.gz .
gzip -d $1.fastq.gz

sed -i '/^$/d' $1.fastq

maxreads=$((`wc -l < $1.fastq` / 8 - 1))
startpoints=$(seq 0 1000000 $maxreads)

for num in $startpoints;
  do endpoint=$(($num + 999999));
  bbmap/getreads.sh in=$1.fastq id=$num-$endpoint out=$1-$endpoint.fastq overwrite=T;
  done

rm $1.fastq
mkdir $1-splitfiles
mv $1*.fastq $1-splitfiles
tar cvzf $1-splitfiles.tar.gz $1-splitfiles
mv $1-splitfiles /mnt/gluster/amlinz/split_metagenomes/
rm BBMap_36.99.tar.gz
rm -r bbmap

```

Testing in interactive mode first - need to figure out things like how many reads per filepart.

### 2017-01-18

I'm officially declaring the metagenome files unworkable as is. It took an hour to copy to the interactive session, another 2 hours to unzip it, and then I couldn't get any of the following, fairly simple commands (wc to get max reads and sed to remove blank lines) to run, despite leaving it going all afternoon. So as a last ditch effort, I tried bbmap's reformat.sh to downsample to 10% of the metagenomic reads. This ran in 1-2 hours and I now have a much smaller version of the metagenome! I'm hoping I can feed it directly into MaxBin instead of mapping, counting reads, and summing parts.

Side note, I also want to use BBmap's reformat.sh to remove small contigs before binning.

So I need to rework my scripts. My new tasks are:
1. Downsample metagenomes
2. Remove small contigs from the assembly fasta
3. Bin

13downsample.sub:
```{bash, eval = F}
# 13downsample.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 13downsample_$(Cluster).log
error = 13downsample_$(Cluster)_$(Process).err
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/13downsample.sh
arguments = $(sample)
output = 13downsample_$(Cluster).out
#
# No files to transfer, since it's going to interact with Gluster instead of the submit node
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz
transfer_output_files =
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 64 GB
request_disk = 100 GB
#
# Submit jobs
queue sample from metaG_samples.txt
```

13downsample.sh:
```{bash, eval = F}
#!/bin/bash
tar -xvzf BBMap_36.99.tar.gz

cp /mnt/gluster/amlinz/GEODES_metagenomes/$1.fastq.gz .
gzip -d $1.fastq.gz

./bbmap/reformat.sh in=$1.fastq out=$1-sampled.fastq samplerate=0.1

gzip $1-sampled.fastq
rm $1.fastq
mv $1-sampled.fastq.gz /mnt/gluster/amlinz/downsampled_metagenomes/
rm BBMap_36.99.tar.gz
rm -r bbmap

```

14contig_filter.sub:
```{bash, eval = F}
# 14contig_filter.sub
#
#
# Specify the HTCondor Universe
universe = vanilla
log = 14contig_filter_$(Cluster).log
error = 14contig_filter_$(Cluster)_$(Process).err
requirements = (OpSys == "LINUX") && (Target.HasGluster == true)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = /home/amlinz/executables/14contig_filter.sh
arguments = $(samplename)
output = 14contig_filter_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = zipped/BBMap_36.99.tar.gz,http://proxy.chtc.wisc.edu/SQUID/amlinz/GEODES005.datafiles2.tar.gz

#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
# Requirements = (Target.HasGluster == true)
request_cpus = 1
request_memory = 16GB
request_disk = 8GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue samplename from contigs.txt

```

14contig_filter.sh:
```{bash, eval = F}
#!/bin/bash
tar -xvzf BBMap_36.99.tar.gz

tar -xvzf $1.datafiles2.tar.gz
gzip -d $1.assembled.fna.gz

./bbmap/reformat.sh in=$1.assembled.fna out=$1-filtered.assembled.fna minlength=1000

gzip $1-filtered.assembled.fna

cp $1-filtered.assembled.fna.gz /mnt/gluster/amlinz/filtered_assemblies/

rm *gz
rm *fna
rm *txt
rm *product_names
rm -r bbmap

```

Now modify the mapping to use the smaller assembly and metagenome files.

15binning.sh:
```{bash, eval = F}
#!/bin/bash

tar xvzf MaxBin.tar.gz
export PATH=$(pwd)/MaxBin-2.2.4/auxiliary/FragGeneScan1.30:$PATH
export PATH=$(pwd)/MaxBin-2.2.4/auxiliary/bowtie2-2.2.3:$PATH
export PATH=$(pwd)/MaxBin-2.2.4/auxiliary/hmmer-3.1b1/src:$PATH
export PATH=$(pwd)/MaxBin-2.2.4/auxiliary/idba-1.1.1/bin:$PATH


cp /mnt/gluster/amlinz/filtered_assemblies/GEODES005-filtered.assembled.fna.gz .
cp /mnt/gluster/amlinz/downsampled_metagenomes/GEODES005-sampled.fastq.gz .

gzip -d GEODES005-filtered.assembled.fna.gz
gzip -d GEODES005-sampled.fastq.gz

./MaxBin-2.2.4/run_MaxBin.pl -contig GEODES005-filtered.assembled.fna -out GEODES005-binned -reads GEODES005-filtered.assembled.fna

mkdir GEODES005-binning
mv GEODES005-binned* GEODES005-binning/
tar cvzf GEODES005-binning.tar.gz GEODES005-binning/

rm *assembled.fna
rm *fastq
rm MaxBin-2.2.4.tar.gz
rm -rf MaxBin-2.2.4
```

####2018-01-29

That worked! All 10  metagenome bins in gluster. I opened up GEODES005-binned and it looks like there are about 200 fna files, a summary file, an abundance file, a log file, and a couple on marker genes. I still need to run this in CheckM to get classifications - Sarah was super awesome and made a docker image for me! So fingers crossed, all I have to do is throw an fna file at the docker image and it'll return classification and completeness estimates. While I'm at it, I'll want to extract the names of contigs from the fna files, since that's what I actually want the classifications for.

1st, put all the fna files from all the metagenomes in a single folder. If there are about 200/metagenome, I'll end up with 2000 jobs. Solid.

####2018-01-31

Good news everyone! After some troubleshooting, Sarah's docker works great! Things I need to keep in mind are to specify that I want to use the most recent docker image using ":latest", and to change my file extensions from .fasta to .fna. 

Checkm produces a lot output, but I don't necessarily need all of it. I need a file that says what contigs are in what bin (and maybe the classification of the bin, so I can easily assign that to each contig), and a file that contains quality and taxonomy info about all the bins generated. I can get the contig lists from my bin input fasta file, and the checkm result summaries appear to be in a file at output/storage/bin_stats_ext.tsv. This file appears to be a python dictionary - can I read it as such?

```{python, eval = F}
def readDict(filename, sep):
    with open(filename, "r") as f:
        dict = {}
        for line in f:
            values = line.split(sep)
            dict[values[0]] = {values[1:len(values)]}
        return(dict)
        
        
```

Apparently reading python dictionaries from files is not as straightforward as I thought it should be. Trying awk instead.

```{bash, eval = F}
tax=$(awk -F', ' '{for(i=1;i<=NF;i++){if ($i ~ /marker lineage/){print $i}}}' output/storage/bin_stats_ext.tsv | awk -F': ' '{print $2}')
length=$(awk -F', ' '{for(i=1;i<=NF;i++){if ($i ~ /Genome size/){print $i}}}' output/storage/bin_stats_ext.tsv | awk -F': ' '{print $2}')
complete=$(awk -F', ' '{for(i=1;i<=NF;i++){if ($i ~ /Completeness/){print $i}}}' output/storage/bin_stats_ext.tsv | awk -F': ' '{print $2}')
contamination=$(awk -F', ' '{for(i=1;i<=NF;i++){if ($i ~ /Contamination/){print $i}}}' output/storage/bin_stats_ext.tsv | awk -F': ' '{print $2}')

echo $tax $length $complete $contamination > results.txt

```

Looks good! I'll add this to my checkm scripts. Modify the submit script to send back less, too.

16checkm.sh:
```{bash, eval = F}

#!/bin/bash

mkdir input
mv $1.fna input/

checkm lineage_wf input output

grep ">" input/$1.fna > contigs.txt
sed -i 's/>//g' contigs.txt

tax=$(awk -F', ' '{for(i=1;i<=NF;i++){if ($i ~ /marker lineage/){print $i}}}' output/storage/bin_stats_ext.tsv | awk -F': ' '{print $2}')
length=$(awk -F', ' '{for(i=1;i<=NF;i++){if ($i ~ /Genome size/){print $i}}}' output/storage/bin_stats_ext.tsv | awk -F': ' '{print $2}')
complete=$(awk -F', ' '{for(i=1;i<=NF;i++){if ($i ~ /Completeness/){print $i}}}' output/storage/bin_stats_ext.tsv | awk -F': ' '{print $2}')
contamination=$(awk -F', ' '{for(i=1;i<=NF;i++){if ($i ~ /Contamination/){print $i}}}' output/storage/bin_stats_ext.tsv | awk -F': ' '{print $2}')

echo $1 $tax $length $complete $contamination > $1-checkm.txt
numcontigs=$(wc -l $1-contigs.txt | awk '{print $1}')
yes $tax | head -n $numcontigs > taxcolumn.txt
yes $1 | head -n $numcontigs > binid.txt
paste -d "\t" contigs.txt binid.txt > cat1.txt
paste -d "\t" cat1.txt taxcolumn.txt > $1-contigs.txt


rm -r input/
rm -r output/
rm taxcolumn.txt
rm contigs.txt
rm binid.txt
rm cat1.txt

```

16checkm.sub:
```{bash, eval = F}
# 16checkm.sub
#
#
# Specify the HTCondor Universe
universe = docker
docker_image = sstevens/checkm:latest

log = 16checkm_$(Cluster).log
error = 16checkm_$(Cluster)_$(Process).err
requirements = (OpSysMajorVer == 7)
#
# Specify your executable, arguments, and a file for HTCondor to store standard
#  output.
executable = executables/16checkm.sh
arguments = $(bin)
output = 16checkm_$(Cluster).out
#
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs.
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = metaG_bins/$(bin).fna
transfer_output_files = $(bin)-contigs.txt,$(bin)-checkm.txt
#
# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.

request_cpus = 1
request_memory = 40GB
request_disk = 12GB
#
# Tell HTCondor to run every fastq file in the provided list:
queue bin from testbins_to_classify.txt

```

Redo the command to build bins_to_classify.txt to not include file extensions

move_bins.sh
```{bash, eval = F}
#!/bin/bash

while read line; do tar xvzf /mnt/gluster/amlinz/$line-binning.tar.gz;
       for file in $line-binning/*fasta; do
               name=$(basename "$file" .fasta);
               mv $file metaG_bins/$name.fna;
               done;
       rm -r $line-binning;
       done < metaG_samples.txt

for file in metaG_bins/*; do
        name=$(basename "$file" .fna);
        echo $name;
        done > bins_to_classify.txt

head -3 bins_to_classify.txt > testbins_to_classify.txt

```